var documenterSearchIndex = {"docs":
[{"location":"internal_library/initialisation/#Initialisation-Internal","page":"Initialisation Internal","title":"Initialisation Internal","text":"","category":"section"},{"location":"internal_library/initialisation/","page":"Initialisation Internal","title":"Initialisation Internal","text":"Pages = [\"initialisation.md\"]","category":"page"},{"location":"internal_library/initialisation/","page":"Initialisation Internal","title":"Initialisation Internal","text":"LikelihoodBasedProfileWiseAnalysis.init_uni_profiles_df\nLikelihoodBasedProfileWiseAnalysis.init_biv_profiles_df\nLikelihoodBasedProfileWiseAnalysis.init_dim_samples_df\nLikelihoodBasedProfileWiseAnalysis.init_uni_profile_row_exists!\nLikelihoodBasedProfileWiseAnalysis.init_biv_profile_row_exists!\nLikelihoodBasedProfileWiseAnalysis.init_dim_samples_row_exists!","category":"page"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_uni_profiles_df","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_uni_profiles_df","text":"init_uni_profiles_df(num_rows::Int; existing_largest_row::Int=0)\n\nInitialises the DataFrame of model.uni_profiles_df with num_rows initial rows. In the event that the DataFrame already exists and more rows are being added, keyword argument, existing_largest_row, will be the number of rows in the existing dataframe, so that values of row_ind when concatenating the DataFrames will increase in steps of 1.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_biv_profiles_df","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_biv_profiles_df","text":"init_biv_profiles_df(num_rows::Int; existing_largest_row::Int=0)\n\nInitialises the DataFrame of model.biv_profiles_df with num_rows initial rows. In the event that the DataFrame already exists and more rows are being added, keyword argument, existing_largest_row, will be the number of rows in the existing dataframe, so that values of row_ind when concatenating the DataFrames will increase in steps of 1.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_dim_samples_df","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_dim_samples_df","text":"init_dim_samples_df(num_rows::Int; existing_largest_row::Int=0)\n\nInitialises the DataFrame of model.dim_samples_df with num_rows initial rows. In the event that the DataFrame already exists and more rows are being added, keyword argument, existing_largest_row, will be the number of rows in the existing dataframe, so that values of row_ind when concatenating the DataFrames will increase in steps of 1.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_uni_profile_row_exists!","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_uni_profile_row_exists!","text":"init_uni_profile_row_exists!(model::LikelihoodModel, \n    θs_to_profile::Vector{<:Int}, \n    dof::Int,\n    profile_type::AbstractProfileType)\n\nInitialises the dictionary entry in model.uni_profile_row_exists for the key (θi, dof, profile_type), where θi is an element of θs_to_profile and dof is the degrees of freedom used to define the asymptotic threshold, with a DefaultDict with key of type Float64 (a confidence level) and default value of 0.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_biv_profile_row_exists!","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_biv_profile_row_exists!","text":"init_biv_profile_row_exists!(model::LikelihoodModel, \n    θcombinations::Vector{Vector{Int}}, \n    dof::Int,\n    profile_type::AbstractProfileType, \n    method::AbstractBivariateMethod)\n\nInitialises the dictionary entry in model.biv_profile_row_exists for the key ((ind1, ind2), dof, profile_type, method), where (ind1, ind2) is a combination in θcombinations and dof is the degrees of freedom used to define the asymptotic threshold, with a DefaultDict with key of type Float64 (a confidence level) and default value of 0.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.init_dim_samples_row_exists!","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.init_dim_samples_row_exists!","text":"init_dim_samples_row_exists!(model::LikelihoodModel, \n    sample_type::AbstractSampleType)\n\nInitialises the dictionary entry in model.dim_samples_row_exists for the key (sample_type) with a DefaultDict with key of type Float64 (a confidence level) and default value of 0. For a full likelihood sample (dimension equal to the number of model parameters).\n\n\n\n\n\ninit_dim_samples_row_exists!(model::LikelihoodModel, \n    θindices::Vector{Vector{Int}},\n    sample_type::AbstractSampleType)\n\nInitialises the dictionary entry in model.dim_samples_row_exists for the key (θvec, dof, sample_type), where θvec is a vector in θindices and dof=length(θvec) is the degrees of freedom used to define the asymptotic threshold, with a DefaultDict with key of type Float64 (a confidence level) and default value of 0. For a non-full likelihood sample (dimension less than the number of model parameters).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#Estimating-Parameter-Magnitudes-and-Bounds","page":"Initialisation Internal","title":"Estimating Parameter Magnitudes and Bounds","text":"","category":"section"},{"location":"internal_library/initialisation/","page":"Initialisation Internal","title":"Initialisation Internal","text":"LikelihoodBasedProfileWiseAnalysis.calculate_θmagnitudes","category":"page"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.calculate_θmagnitudes","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.calculate_θmagnitudes","text":"calculate_θmagnitudes(θlb::Vector{<:Float64}, θub::Vector{<:Float64})\n\nEstimates the magnitude for each parameter using the difference between parameter bounds. If a bound is an Inf, the value is set to NaN. Values are divided by the minimum estimated magnitude such that the returned magnitudes have a lowest value of 1.0.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/initialisation/#Parameter-Transformations","page":"Initialisation Internal","title":"Parameter Transformations","text":"","category":"section"},{"location":"internal_library/initialisation/","page":"Initialisation Internal","title":"Initialisation Internal","text":"LikelihoodBasedProfileWiseAnalysis.checkforInf","category":"page"},{"location":"internal_library/initialisation/#LikelihoodBasedProfileWiseAnalysis.checkforInf","page":"Initialisation Internal","title":"LikelihoodBasedProfileWiseAnalysis.checkforInf","text":"checkforInf(x::AbstractVector{<:Real})\n\nWarns via a message if any of the bounds returned given the provided forward transformation are +/-Inf.\n\nArguments\n\nx: vector of transformed bounds. \n\n\n\n\n\n","category":"function"},{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"M. J. Simpson and O. J. Maclaren. Profile-Wise Analysis: A profile likelihood-based workflow for identifiability analysis, estimation, and prediction with mechanistic mathematical models. PLOS Computational Biology 19, 1–31 (2023).\n\n\n\nY. Pawitan. In All Likelihood: Statistical Modelling and Inference Using Likelihood (Oxford University Press, 2001).\n\n\n\nR. J. Murphy, O. J. Maclaren and M. J. Simpson. Implementing measurement error models in a likelihood-based framework for estimation, identifiability analysis, and prediction in the life sciences, arXiv:2307.01539 (2023).\n\n\n\nM. J. Simpson, R. E. Baker, P. R. Buenzli, R. Nicholson and O. J. Maclaren. Reliable and efficient parameter estimation using approximate continuum limit descriptions of stochastic models. Journal of Theoretical Biology 549, 111201 (2022).\n\n\n\nD. A. Sprott. Statistical Inference in Science (Springer Science & Business Media, 2008).\n\n\n\nA. Raue, C. Kreutz, T. Maiwald, J. Bachmann, M. Schilling, U. Klingmüller and J. Timmer. Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood. Bioinformatics 25, 1923–1929 (2009).\n\n\n\nE. M. Wright and P. Royston. Calculating reference intervals for laboratory measurements. Statistical Methods in Medical Research 8, 93–112 (1999).\n\n\n\nH. A. Katki, E. A. Engels and P. S. Rosenberg. Assessing uncertainty in reference intervals via tolerance intervals: application to a mixed model describing HIV infection. Statistics in Medicine, 3185–3198 (2005).\n\n\n\nR. J. Hyndman. Computing and Graphing Highest Density Regions. The American Statistician 50, 120–126 (1996). Publisher: [American Statistical Association, Taylor & Francis, Ltd.].\n\n\n\nG. A. Satten. Upper and Lower Bound Distributions that Give Simultaneous Confidence Intervals for Quantiles. Journal of the American Statistical Association 90, 747–752 (1995). Publisher: [American Statistical Association, Taylor & Francis, Ltd.].\n\n\n\nM. J. Simpson, S. A. Walker, E. N. Studerus, S. W. McCue, R. J. Murphy and O. J. Maclaren. Profile likelihood-based parameter and predictive interval analysis guides model choice for ecological population dynamics. Mathematical Biosciences 355 (2023).\n\n\n\nD. Cole. Parameter Redundancy and Identifiability (Chapman and Hall/CRC, 2020).\n\n\n\nA. Viallefont, J.-D. Lebreton, A.-M. Reboulet and G. Gory. Parameter Identifiability and Model Selection in Capture-Recapture Models: A Numerical Approach. Biometrical Journal 40, 313–325 (1998).\n\n\n\n","category":"page"},{"location":"internal_library/plots/#Plotting-Functions","page":"Plotting Functions","title":"Plotting Functions","text":"","category":"section"},{"location":"internal_library/plots/","page":"Plotting Functions","title":"Plotting Functions","text":"Pages = [\"plots.md\"]","category":"page"},{"location":"internal_library/plots/#Helper-Functions","page":"Plotting Functions","title":"Helper Functions","text":"","category":"section"},{"location":"internal_library/plots/","page":"Plotting Functions","title":"Plotting Functions","text":"LikelihoodBasedProfileWiseAnalysis.profilecolor\nLikelihoodBasedProfileWiseAnalysis.profile1Dlinestyle\nLikelihoodBasedProfileWiseAnalysis.profile2Dmarkershape\nLikelihoodBasedProfileWiseAnalysis.θs_to_plot_typeconversion\nLikelihoodBasedProfileWiseAnalysis.θcombinations_to_plot_typeconversion","category":"page"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.profilecolor","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.profilecolor","text":"profilecolor(profile_type::Union{AbstractProfileType, AbstractSampleType})\n\nColors of profile types and sample types as integers between 1 and 6.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.profile1Dlinestyle","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.profile1Dlinestyle","text":"profile1Dlinestyle(profile_type::AbstractProfileType)\n\nLinestyle of each profile_type. EllipseApproxAnalytical - :dash, EllipseApprox - :dashdot, LogLikelihood - :solid, \n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.profile2Dmarkershape","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.profile2Dmarkershape","text":"profile2Dmarkershape(profile_type::Union{AbstractProfileType, AbstractSampleType}, on_boundary::Bool)\n\nMarker shapes of bivariate profiles depending on profile type or sample type and whether or not the point is on the profile's boundary.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.θs_to_plot_typeconversion","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.θs_to_plot_typeconversion","text":"θs_to_plot_typeconversion(model::LikelihoodModel, θs_to_plot::Union{Vector{<:Symbol}, Vector{<:Int64}})\n\nConverts θs_to_plot to integer index values with LikelihoodBasedProfileWiseAnalysis.convertθnames_toindices if they are supplied as a vector of symbols.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.θcombinations_to_plot_typeconversion","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.θcombinations_to_plot_typeconversion","text":"θcombinations_to_plot_typeconversion(model::LikelihoodModel,\n    θcombinations_to_plot::Union{Vector{Vector{Symbol}}, Vector{Tuple{Symbol, Symbol}}, Vector{Vector{Int}}, Vector{Tuple{Int,Int}}})\n\nConverts θcombinations_to_plot to a vector of tuples of integer index values with LikelihoodBasedProfileWiseAnalysis.convertθnames_toindices if they are supplied as a vector of vectors/tuples containing symbols. The index elements in each vector/tuple will be sorted in ascending order. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#Plots","page":"Plotting Functions","title":"Plots","text":"","category":"section"},{"location":"internal_library/plots/","page":"Plotting Functions","title":"Plotting Functions","text":"LikelihoodBasedProfileWiseAnalysis.plot1Dprofile!\nLikelihoodBasedProfileWiseAnalysis.addMLEandLLstar!\nLikelihoodBasedProfileWiseAnalysis.plot2Dboundary!\nLikelihoodBasedProfileWiseAnalysis.addMLE!\nLikelihoodBasedProfileWiseAnalysis.plotprediction!\nLikelihoodBasedProfileWiseAnalysis.plotrealisation!\nLikelihoodBasedProfileWiseAnalysis.add_yMLE!\nLikelihoodBasedProfileWiseAnalysis.add_extrema!","category":"page"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.plot1Dprofile!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.plot1Dprofile!","text":"plot1Dprofile!(plt, parRange, parProfile, label=\"profile\"; kwargs...)\n\nPlots a univariate profile at ψ locations parRange, with normalised profile log-likelihood function values parProfile.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.addMLEandLLstar!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.addMLEandLLstar!","text":"addMLEandLLstar!(plt, llstar, parMLE, MLE_color, llstar_color; kwargs...)\n\nOn a univariate profile, adds the MLE location as a vertical line and the asymptotic confidence threshold as a horizontal line. The intersection between the horizontal asymptotic confidence threshold and the univariate profile gives the location of the corresponding confidence interval for that parameter. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.plot2Dboundary!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.plot2Dboundary!","text":"plot2Dboundary!(plt, parBoundarySamples, label=\"boundary\"; use_lines=false, kwargs...)\n\nPlots the boundary of a bivariate profile at 2D locations parBoundarySamples. If use_lines=false then it will use a scatter plot, otherwise it will connect the boundary in the order of the parBoundarySamples.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.addMLE!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.addMLE!","text":"addMLE!(plt, parMLEs; kwargs...)\n\nOn a bivariate profile, adds the MLE location as a scatter point.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.plotprediction!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.plotprediction!","text":"plotprediction!(plt, t, predictions, extrema, linealpha, layout; extremacolor=:red, kwargs...)\n\nPlots the profile-wise confidence set for the model trajectory using the same format as in the profile-wise analysis workflow paper [1]. The extrema of the profile-wise trajectory confidence set is labelled as approximate profile-wise simultaneous confidence bands (SCBs).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.plotrealisation!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.plotrealisation!","text":"plotrealisation!(plt, t, extrema, linealpha, layout; extremacolor=:red, kwargs...)\n\nPlots the extrema of the profile-wise reference tolerance set for the (1-δ) population reference set. The (1-δ) population reference set refers to a set containing the (1-δ) reference region of the population at each time point; i.e. the smallest region at each time point which contains (1-δ) of possible realisations. The extrema of the profile-wise reference tolerance set is labelled as approximate profile-wise simultaneous reference tolerance bands (SRTBs).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.add_yMLE!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.add_yMLE!","text":"add_yMLE!(plt, t, yMLE, layout; kwargs...)\n\nAdds the model trajectory obtained from simulating the model at times t using the maximum likelihood estimate for parameters. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/plots/#LikelihoodBasedProfileWiseAnalysis.add_extrema!","page":"Plotting Functions","title":"LikelihoodBasedProfileWiseAnalysis.add_extrema!","text":"add_extrema!(plt, t, extrema, layout; extremacolor=:gold, label=[\"Sampled SCBs (≈)\" \"\"])\n\nAdds additional extrema to plots for the model trajectory and population reference sets. Typically for comparing the extrema of profile-wise predictions sets from profiles with sets from dimensional samples (often the full parameter confidence set).\n\n\n\n\n\n","category":"function"},{"location":"examples/logistic_timing_estimates/#Function-Evaluation-Timing-Logistic-Model","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"Recording the number of function evaluations (likelihood and optimisation functions) that a particular method requires to evaluate a profile is implemented only in a singly threaded environment as we use an external package (TimerOutputs) for recording; it's not inherently built in to our package. It is likely possible to implement this in a distributed environment, but given limited time this has not been implemented. When TimerOutputs is not being used, no timing overhead is introduced. ","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"To demonstrate how to use this, we will use the Logistic Model example that we previously discussed.","category":"page"},{"location":"examples/logistic_timing_estimates/#Initial-Setup,-Data-and-Parameter-Definition","page":"Function Evaluation Timing - Logistic Model","title":"Initial Setup, Data and Parameter Definition","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"using Random, Distributions\nusing DataFrames\nusing Combinatorics\nusing TimerOutputs\nusing LikelihoodBasedProfileWiseAnalysis\nusing LikelihoodBasedProfileWiseAnalysis.TimerOutputs: TimerOutputs as TO\n\n@everywhere function solvedmodel(t, θ)\n    return (θ[2]*θ[3]) ./ ((θ[2]-θ[3]) .* (exp.(-θ[1] .* t)) .+ θ[3])\nend\n\n@everywhere function loglhood(θ, data)\n    y=solvedmodel(data.t, θ)\n    e=sum(loglikelihood(data.dist, data.y_obs .- y))\n    return e\nend\n\n# DATA GENERATION FUNCTION \n@everywhere function data_generator(θ_true, generator_args::NamedTuple)\n    y_obs = generator_args.y_true .+ rand(generator_args.dist, length(generator_args.t))\n    if generator_args.is_test_set; return y_obs end\n\n    data = (y_obs=y_obs, generator_args...)\n    return data\nend\n\n# true parameters\nλ_true=0.01; K_true=100.0; C0_true=10.0; t=0:100:1000; \n@everywhere global σ=10.0;\nθ_true=[λ_true, K_true, C0_true]\ny_true = solvedmodel(t, θ_true)\ny_obs = [19.27, 20.14, 37.23, 74.87, 88.51, 82.91, 123.88, 103.25, 78.89, 87.87, 113.0]\n\n# Named tuple of all data required within the log-likelihood function\ndata = (y_obs=y_obs, t=t, dist=Normal(0, σ))\n\n# Bounds on model parameters \nλ_min, λ_max = (0.00, 0.05)\nK_min, K_max = (50., 150.)\nC0_min, C0_max = (0.0, 50.)\nlb = [λ_min, K_min, C0_min]\nub = [λ_max, K_max, C0_max]\n\nθnames = [:λ, :K, :C0]\nθG = θ_true\npar_magnitudes = [0.005, 10, 10]\n\ntraining_gen_args = (y_true=y_true, t=t, dist=Normal(0, σ), is_test_set=false)","category":"page"},{"location":"examples/logistic_timing_estimates/#LikelihoodModel-Initialisation","page":"Function Evaluation Timing - Logistic Model","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5))\nmodel = initialise_LikelihoodModel(loglhood, data, θnames, θG, lb, ub, par_magnitudes, optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic_timing_estimates/#Profiling","page":"Function Evaluation Timing - Logistic Model","title":"Profiling","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/#Univariate-Profile-Timing","page":"Function Evaluation Timing - Logistic Model","title":"Univariate Profile Timing","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"To time the number of likelihood and optimisation function evaluations required to find each 95% parameter confidence interval and find 20 points within the interval we use:","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"TO.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\nTO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\ntimer_df = DataFrame(parameter=zeros(Int, model.core.num_pars), \n                        optimisation_calls=zeros(Int, model.core.num_pars),\n                        likelihood_calls=zeros(Int, model.core.num_pars))\n\n\nopt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfor i in 1:model.core.num_pars\n    univariate_confidenceintervals!(model, [i], num_points_in_interval=20, optimizationsettings=opt_settings)\n    timer_df[i, :] .= i, TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"]),\n            TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"][\"Likelihood evaluation\"])\n\n    TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\nend\n\nTO.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis)","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"You can also print the timer results, which can be useful for identifying the strings that represent the different sections timed (e.g. \"Univariate confidence interval\") as well as providing the amount of time taken.","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"print_timer(LikelihoodBasedProfileWiseAnalysis.timer)","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"Similarly, if we wish to use asymptotic confidence intervals as the starting guess for the parameter confidence intervals we can first evaluate them using the EllipseApproxAnalytical profile type and set the keyword argument use_ellipse_approx_analytical_start to true.","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"TO.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\nTO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\ntimer_df = DataFrame(parameter=zeros(Int, model.core.num_pars), \n                        optimisation_calls=zeros(Int, model.core.num_pars),\n                        likelihood_calls=zeros(Int, model.core.num_pars))\n\n\nopt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nunivariate_confidenceintervals!(model, profile_type=EllipseApproxAnalytical())\nfor i in 1:model.core.num_pars\n    TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\n    univariate_confidenceintervals!(model, [i], num_points_in_interval=20, \n        use_ellipse_approx_analytical_start=true, optimizationsettings=opt_settings)\n    timer_df[i, :] .= i, TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"]),\n            TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"][\"Likelihood evaluation\"])\n\n    TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\nend\n\nTO.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis)","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"If we wish to evaluate the average (mean) number of function evaluations required during a simulation (such as our coverage simulations) we need to define additional functions. For comparison to coverage simulations it is recommended that a random seed is set prior to training data generation here and prior to calling e.g. check_univariate_parameter_coverage. This ensures that the training data used is consistent between the two simulations. ","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"The previous procedure could also just be used for each parameter in turn with check_univariate_parameter_coverage, with the mean figure obtained by dividing through by the number of simulations. However, this would prevent using Distributed with the simulation which could make it infeasible. We are less concerned with the exact accuracy of the number of function evaluations in a coverage simulation and more concerned with the general magnitude so here we only evaluate the average number of function evaluations across 100 iterations. Note, we need to use initialise_LikelihoodModel for each new data set.","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"function record_CI_LL_evaluations!(N)\n    timer_df = DataFrame(parameter=zeros(Int, model.core.num_pars), \n                        optimisation_calls=zeros(Int, model.core.num_pars),\n                        likelihood_calls=zeros(Int, model.core.num_pars))\n    \n    Random.seed!(1234)\n    training_data = [data_generator(θ_true, training_gen_args) for _ in 1:N]\n    total_opt_calls = zeros(Int, model.core.num_pars)\n    total_ll_calls = zeros(Int, model.core.num_pars)\n\n    for j in 1:N\n        opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5))\n        model = initialise_LikelihoodModel(loglhood, training_data[j], θnames, θG, lb, ub, par_magnitudes, optimizationsettings=opt_settings)\n\n        opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n        for i in 1:model.core.num_pars\n            TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\n            univariate_confidenceintervals!(model, [i], existing_profiles=:overwrite, optimizationsettings=opt_settings)\n\n            total_opt_calls[i] += TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"])\n\n            total_ll_calls[i] += TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Univariate confidence interval\"][\"Likelihood nuisance parameter optimisation\"][\"Likelihood evaluation\"])\n        end\n    end\n    \n    timer_df = DataFrame(parameter=zeros(Int, model.core.num_pars),\n        mean_optimisation_calls=zeros(model.core.num_pars),\n        mean_likelihood_calls=zeros(model.core.num_pars))\n\n    timer_df[:, 1] .= 1:model.core.num_pars\n    timer_df[:, 2] .= total_opt_calls ./ N\n    timer_df[:, 3] .= total_ll_calls ./ N\n\n    return timer_df\nend\n\nTO.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\nTO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\ntimer_df = record_CI_LL_evaluations!(N)\n\nTO.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis)","category":"page"},{"location":"examples/logistic_timing_estimates/#Bivariate-Profile-Timing","page":"Function Evaluation Timing - Logistic Model","title":"Bivariate Profile Timing","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"We can do the same with bivariate profiles by modifying the string used to access the relevant timer section.","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"TO.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\nTO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\nlen = length(combinations(1:model.core.num_pars, 2))\ntimer_df = DataFrame(parameter=zeros(Int, len), \n                        optimisation_calls=zeros(Int, len),\n                        likelihood_calls=zeros(Int, len))\n\nopt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfor (i, pars) in enumerate(collect(combinations(1:model.core.num_pars, 2)))\n    bivariate_confidenceprofiles!(model, [pars], 50, method=IterativeBoundaryMethod(20, 5, 5, 0.15, 1.0, use_ellipse=true), optimizationsettings=opt_settings)\n    timer_df[i, :] .= i, TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Bivariate confidence boundary\"][\"Likelihood nuisance parameter optimisation\"]),\n            TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Bivariate confidence boundary\"][\"Likelihood nuisance parameter optimisation\"][\"Likelihood evaluation\"])\n\n    TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\nend\n\nTO.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis)","category":"page"},{"location":"examples/logistic_timing_estimates/#Dimensional-Samples","page":"Function Evaluation Timing - Logistic Model","title":"Dimensional Samples","text":"","category":"section"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"And similarly for dimensional samples. Note, these are just sampled versions of profiles with interest parameter dimension in 1 theta where theta is the total number of model parameters, model.core.num_pars. For example, for bivariate dimensional samples (2D):","category":"page"},{"location":"examples/logistic_timing_estimates/","page":"Function Evaluation Timing - Logistic Model","title":"Function Evaluation Timing - Logistic Model","text":"TO.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\nTO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\nlen = length(combinations(1:model.core.num_pars, 2))\ntimer_df = DataFrame(parameter=zeros(Int, len), \n                        optimisation_calls=zeros(Int, len),\n                        likelihood_calls=zeros(Int, len))\n\nopt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfor (i, pars) in enumerate(collect(combinations(1:model.core.num_pars, 2)))\n    dimensional_likelihood_samples!(model, [pars], 1000, optimizationsettings=opt_settings)\n    timer_df[i, :] .= i, TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Dimensional likelihood sample\"][\"Likelihood nuisance parameter optimisation\"]),\n            TO.ncalls(\n                LikelihoodBasedProfileWiseAnalysis.timer[\"Dimensional likelihood sample\"][\"Likelihood nuisance parameter optimisation\"][\"Likelihood evaluation\"])\n\n    TO.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\nend\n\nTO.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis)","category":"page"},{"location":"user_interface/coverage/bivariate_boundaries/#Bivariate-Parameter-Confidence-Boundaries","page":"Bivariate Parameter Confidence Boundaries","title":"Bivariate Parameter Confidence Boundaries","text":"","category":"section"},{"location":"user_interface/coverage/bivariate_boundaries/","page":"Bivariate Parameter Confidence Boundaries","title":"Bivariate Parameter Confidence Boundaries","text":"Pages = [\"bivariate_boundaries.md\"]","category":"page"},{"location":"user_interface/coverage/bivariate_boundaries/","page":"Bivariate Parameter Confidence Boundaries","title":"Bivariate Parameter Confidence Boundaries","text":"Usage on several models can be seen in the examples section, such as for the Logistic Model.","category":"page"},{"location":"user_interface/coverage/bivariate_boundaries/#Boundary-Coverage-of-True-Interest-Parameters","page":"Bivariate Parameter Confidence Boundaries","title":"Boundary Coverage of True Interest Parameters","text":"","category":"section"},{"location":"user_interface/coverage/bivariate_boundaries/","page":"Bivariate Parameter Confidence Boundaries","title":"Bivariate Parameter Confidence Boundaries","text":"check_bivariate_parameter_coverage","category":"page"},{"location":"user_interface/coverage/bivariate_boundaries/#LikelihoodBasedProfileWiseAnalysis.check_bivariate_parameter_coverage","page":"Bivariate Parameter Confidence Boundaries","title":"LikelihoodBasedProfileWiseAnalysis.check_bivariate_parameter_coverage","text":"check_bivariate_parameter_coverage(data_generator::Function,\n    generator_args::Union{Tuple,NamedTuple},\n    model::LikelihoodModel,\n    N::Int,\n    num_points::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real},\n    θcombinations::Union{Vector{Vector{Int}}, Vector{Tuple{Int,Int}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the coverage of bivariate confidence boundaries for two-way sets of interest parameters in θcombinations given a model by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue and fitting the model. \nTesting if each of the true bivariate interest parameters, given nuisance parameters, have log-likelihood values within the confidence threshold. \nIf these pass then bivariate confidence boundaries of num_points are found using method and MPPHullMethod is used to construct 2D polygon hulls of the boundary points. \nFinally, testing if the boundary polygons contain the true bivariate parameter values in θtrue. The estimated coverage is returned with a default 95% confidence interval within a DataFrame. \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nN: a positive number of coverage simulations.\nnum_points: positive number of points to find on the boundary at the specified confidence level using a single method. Or a vector of positive numbers of boundary points to find for each method in method (if method is a vector of AbstractBivariateMethod). Set to at least 3 within the function as some methods need at least three points to work. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθcombinations: a vector of pairs of parameters to profile, as a vector of vectors of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval coverage at. Default is 0.95 (95%).\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nmethod: a method of type AbstractBivariateMethod or a vector of methods of type AbstractBivariateMethod (if so num_points needs to be a vector of the same length). For a list of available methods use bivariate_methods() (bivariate_methods). Default is RadialRandomMethod(3) (RadialRandomMethod).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual bivariate boundary calculations within each iteration (true). Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of bivariate parameter confidence boundaries. The simulation uses Distributed.jl to parallelise the workload.\n\nFor a 95% confidence boundary of a pair of interest parameters [θi, θj] it is expected that under repeated experiments from an underlying true model (data generation) which are used to construct a 2D confidence boundary for [θi, θj], 95% of the true boundaries, would contain the true value [θi, θj]. In the simulation where the values of the true parameters, θtrue, are known, this is equivalent to whether the minimum perimeter polygon of the 2d boundary points for [θi, θj] AND the true confidence boundary contains the value θtrue[[θi, θj]].\n\nAll of the methods for constructing an approximation of the 2D boundary using bivariate_confidenceprofiles! will approach an exact representation of the 2D 95% confidence boundary, assuming bounds are not in the way, as the number of boundary points approaches infinity. Resultantly, for lower numbers of boundary points the polygon representation of the boundary will be an approximation, with straight edges that do not exactly represent the true boundary. This is why the coverage check also checks if a point is inside the true boundary, as the polygon approximation might be right by accident. This is the same logic [sample_bivariate_internal_points!] uses to find additional internal points within a boundary polygon.\n\nFor estimates of how well the methods approximate the true 2D boundary after turning their boundary points into a polygon hull using a AbstractBivariateHullMethod, check_bivariate_boundary_coverage can be used. \n\nThe uncertainty in estimates of the coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Simultaneous bivariate profiles\nCalculating the coverage of simultaneous bivariate profiles is not currently supported (i.e. for dof ≠ 2)\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: May not work correctly on bimodal confidence boundaries\nThe current implementation constructs a single polygon with minimum polygon perimeter from the set of boundary points as the confidence boundary. If there are multiple distinct boundaries represented, then there will be edges connecting the distinct boundaries which the true parameter might be inside (but not inside either of the distinct boundaries). \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/bivariate_boundaries/#Boundary-Coverage-of-True-Boundary","page":"Bivariate Parameter Confidence Boundaries","title":"Boundary Coverage of True Boundary","text":"","category":"section"},{"location":"user_interface/coverage/bivariate_boundaries/","page":"Bivariate Parameter Confidence Boundaries","title":"Bivariate Parameter Confidence Boundaries","text":"check_bivariate_boundary_coverage","category":"page"},{"location":"user_interface/coverage/bivariate_boundaries/#LikelihoodBasedProfileWiseAnalysis.check_bivariate_boundary_coverage","page":"Bivariate Parameter Confidence Boundaries","title":"LikelihoodBasedProfileWiseAnalysis.check_bivariate_boundary_coverage","text":"check_bivariate_boundary_coverage(data_generator::Function,\n    generator_args::Union{Tuple,NamedTuple},\n    model::LikelihoodModel,\n    N::Int,\n    num_points::Union{Int, Vector{<:Int}},\n    num_points_to_sample::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real},\n    θcombinations::Union{Vector{Vector{Int}}, Vector{Tuple{Int,Int}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the coverage of approximate bivariate confidence boundaries with num_points constructed using method and hullmethod for two-way sets of interest parameters in θcombinations given a model of the true bivariate confidence boundary by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue and fitting the model. \nnum_points_to_sample points are then sampled in interest parameter space using sample_type and those that are inside the true bivariate confidence boundary are extracted. \nThen bivariate confidence boundaries of num_points are found using method and hullmethod is used to construct 2D polygon hulls of the boundary points. \nFinally, the percentage of extracted samples that are contained within the 2D polygon hull is extracted. The median and mean percentage (coverage) across all N simulations of the true boundary is recorded and returned with a default 95% simulation quantile interval within a DataFrame. The median may be more reliable for use than the mean due to expected coverage approaching 1.0 when the polygon is a very good representation of the boundary. The 95% simulation quantile interval is the 2.5% and 97.5% quantiles of the coverage across the N simulations. \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nN: a positive number of coverage simulations.\nnum_points: positive number of points to find on the boundary at the specified confidence level using a single method. Or a vector of positive numbers of boundary points to find for each method in method (if method is a vector of AbstractBivariateMethod). Set to at least 3 within the function as some methods need at least three points to work. \nnum_points_to_sample: integer number of points to sample (for UniformRandomSamples and LatinHypercubeSamples sample types) from interest parameter space. For the UniformGridSamples sample type, if integer it is the number of points to grid over in each parameter dimension. If it is a vector of integers each index of the vector is the number of points to grid over in the corresponding parameter dimension. For example, [1,2] would mean a single point in dimension 1 and two points in dimension 2. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθcombinations: a vector of pairs of parameters to profile, as a vector of vectors of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval coverage at. Default is 0.95 (95%).\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nmethod: a method of type AbstractBivariateMethod or a vector of methods of type AbstractBivariateMethod (if so num_points needs to be a vector of the same length). For a list of available methods use bivariate_methods() (bivariate_methods). Default is RadialRandomMethod(3) (RadialRandomMethod).\nsample_type: the sampling method used to sample parameter space of type [AbstractSampleType]. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nhullmethod: method of type AbstractBivariateHullMethod used to create a 2D polygon hull that approximates the bivariate boundary from a set of boundary points and internal points (method dependent) (or vector of type AbstractBivariateHullMethod if comparison between hull methods is1 desired). For available methods see bivariate_hull_methods(). Default is MPPHullMethod() (MPPHullMethod).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\ncoverage_estimate_quantile_level: a number ∈ (0.0, 1.0) for the level of the quantile interval of the estimated coverage (intervals are formed from simulation quantiles). Default is 0.95 (95%).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual bivariate boundary calculations within each iteration (true). Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of the approximations of the true bivariate parameter confidence boundaries. Namely, how well the approximation contains the true boundary. The simulation uses Distributed.jl to parallelise the workload.\n\nTests how well the boundary polygon created by a method with a given number of points and turned into a polygon hull using hullmethod contains the theoretical boundary by testing how many samples from a AbstractSampleType within the true boundary are within the boundary polygon.\n\nIf MPPHullMethod is the hullmethod used, it is expected that the approximation of the true bivariate parameter confidence boundary created by bivariate_confidenceprofiles! will be an exact representation, as the number of boundary points approaches infinity. For ConcaveHullMethod this is also likely to be the case, but it may fail due to being a heuristic. For ConvexHullMethod this will be true if the true boundary is convex. If the true boundary is concave then the approximation that uses ConvexHullMethod will fully contain the true boundary, but will also contain parameter space that is not part of the true boundary. \n\nThis check is useful for determining how to most efficiently sample internal points from bivariate confidence boundaries with [sample_bivariate_internal_points] as it shows how the interaction between the method, hullmethod and the number of boundary points impact the coverage of the true boundary. For example, using ConvexHullMethod will generally give the highest coverage of the true boundary, but may cause the rejection rate to be higher because it contains a greater area that is not part of the true boundary.\n\nThe uncertainty in estimates of the coverage under the simulated model will become more accurate as the number of simulations, N, is increased. Simulation quantile intervals for the coverage estimate are provided to quantify this uncertainty. \n\nnote: Simultaneous bivariate profiles\nCalculating the coverage for approximations of simultaneous bivariate profiles is not currently supported (i.e. for dof ≠ 2)\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#Univariate-Profiles","page":"Univariate Profiles","title":"Univariate Profiles","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/univariate/","page":"Univariate Profiles","title":"Univariate Profiles","text":"The key function for evaluating univariate profiles (the parameter confidence interval and corresponding points along the profile as desired) is univariate_confidenceintervals!.  The evaluated univariate profile(s) will be contained within a UnivariateConfidenceStruct that is stored in the LikelihoodModel.","category":"page"},{"location":"user_interface/profiles_and_samples/univariate/","page":"Univariate Profiles","title":"Univariate Profiles","text":"univariate_confidenceintervals!\nget_points_in_intervals!\nget_uni_confidence_intervals\nget_uni_confidence_interval\nget_uni_confidence_interval_points","category":"page"},{"location":"user_interface/profiles_and_samples/univariate/#LikelihoodBasedProfileWiseAnalysis.univariate_confidenceintervals!","page":"Univariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.univariate_confidenceintervals!","text":"univariate_confidenceintervals!(model::LikelihoodModel, \n    θs_to_profile::Vector{<:Int64}=collect(1:model.core.num_pars); \n    <keyword arguments>)\n\nComputes likelihood-based confidence interval profiles for the provided θs_to_profile interest parameters, where θs_to_profile is a vector of Int corresponding to the parameter indexes in model.core.θnames. Saves these profiles by modifying model in place. Returns the evaluated confidence intervals in a DataFrame using get_uni_confidence_intervals.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nθs_to_profile: vector of parameters to profile, as a vector of model parameter indexes. Default is collect(1:model.core.num_pars), or all parameters.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval. Default is 0.95 (95%).\ndof: an integer ∈ [1, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the extremities of the univariate profile, i.e. the confidence interval. For parameter confidence intervals that are considered individually, it should be set to 1. For intervals that are considered simultaneously, it should be set to the number of intervals that are being calculated, i.e. model.core.num_pars when we wish the confidence interval for every parameter to hold simultaneously. Default is 1. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 1 and model.core.num_pars may not have a clear statistical interpretation.\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\nuse_existing_profiles: boolean variable specifying whether to use existing profiles of a parameter θi to decrease the width of the bracket used to search for the desired confidence interval using LikelihoodBasedProfileWiseAnalysis.get_interval_brackets. Existing profiles must have been calculated using the same value of dof. Default is false.\nuse_ellipse_approx_analytical_start: boolean variable specifying whether to use existing profiles at confidence_level and dof of type EllipseApproxAnalytical of a parameter θi to decrease the width of the bracket used to search for the desired confidence interval. Can decrease search times significantly for LogLikelihood profile types. Default is false.\nnum_points_in_interval: an integer number of points to optionally evaluate within the confidence interval for each interest parameter using get_points_in_intervals!. Points are linearly spaced in the interval and have their optimised log-likelihood value recorded. Useful for plots that visualise the confidence interval or for predictions from univariate profiles. Default is 0. \nadditional_width: a Real number greater than or equal to zero. Specifies the additional width to optionally evaluate outside the confidence interval's width if num_points_in_interval is greater than 0 using get_points_in_intervals!. Half of this additional width will be placed on either side of the confidence interval. If the additional width goes outside a bound on the parameter, only up to the bound will be considered. The spacing of points in the additional width will try to match the spacing of points evaluated inside the interval. Useful for plots that visualise the confidence interval as it shows the trend of the log-likelihood profile outside the interval range. Default is 0.0.\nexisting_profiles: Symbol ∈ [:ignore, :overwrite] specifying what to do if profiles already exist for a given interest parameter, confidence_level and profile_type. See below for each symbol's meanings. Default is :ignore.\nfind_zero_atol: a Real number greater than zero for the absolute tolerance of the log-likelihood function value from the target value to be used when searching for confidence intervals. Default is model.find_zero_atol.\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θs_to_profile completed and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across combinations of interest parameters. Set this variable to false if Distributed.jl is not being used. Default is true.\nuse_threads: boolean variable specifying, if use_distributed is false, whether to use a parallelised for loop across Threads.nthreads() threads or a non-parallel for loop within the call to get_points_in_intervals!. Default is true.\n\nnote: existing_profiles meanings\n:ignore means profiles that already exist will not be recomputed. \n:overwrite means profiles that already exist will be overwritten. Predictions evaluated from the existing profile will be forgotten.\n\nDetails\n\nBy calling LikelihoodBasedProfileWiseAnalysis.univariate_confidenceinterval this function finds each side of the confidence interval using a bracketing method for interest parameters in θs_to_profile (depending on the setting for existing_profiles if these profiles already exist). Nuisance parameters of each point in univariate interest parameter space are found by maximising the log-likelihood function given by profile_type. Updates model.uni_profiles_df for each successful profile and saves their results as a UnivariateConfidenceStruct in model.uni_profiles_dict, where the keys for the dictionary is the row number in model.uni_profiles_df of the corresponding profile. model.uni_profiles_df.num_points is the number of points currently saved within the confidence interval inclusive.\n\nExtended help\n\nValid bounds\n\nThe bracketing method utilised via Roots.jl's find_zero will be unlikely to converge to the true confidence interval for a given parameter if the bounds on that parameter are +/- Inf or the log-likelihood function evaluates to +/- Inf. Bounds should be set to prevent this from occurring.\n\nDistributed Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true, then the univariate profiles of each interest parameter will be computed in parallel across Distributed.nworkers() workers. If use_distributed is false and use_threads is true then after the confidence intervals of each interest parameter have been computed, any interval points specified using num_points_in_interval will be computed in parallel across Threads.nthreads() threads for each interest parameter.\n\nIteration Speed Of the Progress Meter\n\nAn iteration within the progress meter is specified as one iteration per side of the confidence interval found and an additional iteration for once points within the interval have been found if num_points_in_interval > 0. This means on a per interest parameter basis, there are either two or three iterations counted in time/it calculation.\n\n\n\n\n\nunivariate_confidenceintervals!(model::LikelihoodModel, \n    θs_to_profile::Vector{<:Symbol}; \n    <keyword arguments>)\n\nProfiles only the provided θs_to_profile interest parameters, where θs_to_profile is a vector of Symbol corresponding to the parameter symbols in model.core.θnames. Returns the evaluated confidence intervals in a DataFrame using get_uni_confidence_intervals.\n\n\n\n\n\nunivariate_confidenceintervals!(model::LikelihoodModel, \n    profile_m_random_combinations::Int; \n    <keyword arguments>)\n\nProfiles m random interest parameters (sampling without replacement), where 0 < m ≤ model.core.num_pars. Returns the evaluated confidence intervals in a DataFrame using get_uni_confidence_intervals.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#LikelihoodBasedProfileWiseAnalysis.get_points_in_intervals!","page":"Univariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.get_points_in_intervals!","text":"get_points_in_intervals!(model::LikelihoodModel, \n    num_points_in_interval::Int; \n    <keyword arguments>)\n\nEvaluate and save num_points_in_interval linearly spaced points between the confidence intervals of existing univariate profiles that meet the requirements of the univariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments), as well as any additional width on the sides of the interval. Modifies model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nnum_points_in_interval: an integer number of points to evaluate within the confidence interval. Points are linearly spaced in the interval and have their optimised log-likelihood value recorded (standardised to 0.0 at the MLE point). Useful for plots that visualise the confidence interval or for predictions from univariate profiles. \n\nKeyword Arguments\n\nadditional_width: a Real number greater than or equal to zero. Specifies the additional width to optionally evaluate outside the confidence interval's width. Half of this additional width will be placed on either side of the confidence interval. If the additional width goes outside a bound on the parameter, only up to the bound will be considered. The spacing of points in the additional width will try to match the spacing of points evaluated inside the interval. Useful for plots that visualise the confidence interval as it shows the trend of the log-likelihood profile outside the interval range. Default is 0.0.\nconfidence_levels: a vector of confidence levels. If empty, all confidence levels of univariate profiles will be considered for finding interval points. Otherwise, only confidence levels in confidence_levels will be considered. Default is Float64[] (any confidence level).\ndofs: a vector of integer degrees of freedom used to define the asymptotic threshold for the extremities of a univariate profile. If empty, all degrees of freedom for univariate profiles will be considered for evaluating predictions from. Otherwise, only degrees of freedom in dofs will be considered. Default is Int[] (any degree of freedom).\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types of univariate profiles are considered. Otherwise, only profiles with matching profile types will be considered. Default is AbstractProfileType[] (any profile type).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\nnot_evaluated_predictions: a boolean specifying whether to only get points in intervals of profiles that have not had predictions evaluated (true) or for all profiles (false). If false, then any existing predictions will be forgotten by the model and overwritten the next time predictions are evaluated for each profile. Default is true.\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θs_to_profile completed and estimated time of completion. Default is model.show_progress.\nuse_threads: boolean variable specifying, if the number of workers for distributed computing is not greater than 1 (!Distributed.nworkers()>1), to use a parallelised for loop across Threads.nthreads() threads to evaluate the interval points. Default is false.\n\nDetails\n\nInterval points and their corresponding log-likelihood values are stored in the interval_points field of a UnivariateConfidenceStruct. These are updated using LikelihoodBasedProfileWiseAnalysis.update_uni_dict_internal!. Nuisance parameters of each point in univariate interest parameter space are found by maximising the log-likelihood function given by the profile_type of the profile. \n\nIf get_points_in_intervals! has already been used on a univariate profile, with the same values of num_points_in_interval and additional_width, it will not be recomputed for that profile.\n\nParallel Computing Implementation\n\nIf Distributed.jl is being used then each set of interval points for distinct interest parameters will be computed in parallel across Distributed.nworkers() workers. If it is not being used (Distributed.nworkers() is equal to 1) and use_threads is true then the interval points of each interest parameter will be computed in parallel across Threads.nthreads() threads . It is highly recommended to set use_threads to true in that situation.\n\nIteration Speed Of the Progress Meter\n\nAn iteration within the progress meter is specified as the time it takes for all internal points within a univariate confidence interval to be found (as well as any outside, if additional_width is greater than zero).\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_intervals","page":"Univariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_intervals","text":"get_uni_confidence_intervals(model::LikelihoodModel, uni_row_numbers::AbstractVector{<:Int})\n\nReturns the confidence intervals corresponding to the profile in rows uni_row_numbers of model.uni_profiles_df as a DataFrame. If an entry has value NaN, that side of the confidence interval is outside the corresponding bound on the interest parameter.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_interval","page":"Univariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_interval","text":"get_uni_confidence_interval(model::LikelihoodModel, uni_row_number::Int)\n\nReturns the confidence interval corresponding to the profile in row uni_row_number of model.uni_profiles_df as a vector of length two. If an entry has value NaN, that side of the confidence interval is outside the corresponding bound on the interest parameter.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_interval_points","page":"Univariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.get_uni_confidence_interval_points","text":"get_uni_confidence_interval_points(model::LikelihoodModel, uni_row_number::Int)\n\nReturns the interval points PointsAndLogLikelihood struct corresponding to the profile in row uni_row_number of model.uni_profiles_df.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/univariate/#Index","page":"Univariate Profiles","title":"Index","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/univariate/","page":"Univariate Profiles","title":"Univariate Profiles","text":"Pages = [\"univariate.md\"]","category":"page"},{"location":"user_interface/timing_and_profiling/#Timing-and-Profiling","page":"Timing and Profiling","title":"Timing and Profiling","text":"","category":"section"},{"location":"user_interface/timing_and_profiling/","page":"Timing and Profiling","title":"Timing and Profiling","text":"Progress meters are implemented using ProgressMeter.jl and can be used to keep track of how much longer any profile or prediction evaluation has remaining. They can be disabled on a per function basis using the kwarg show_progress (which by default is model.show_progress). They can be disabled for all functions by setting it to false when calling initialise_LikelihoodModel.","category":"page"},{"location":"user_interface/timing_and_profiling/#TimerOutputs.jl","page":"Timing and Profiling","title":"TimerOutputs.jl","text":"","category":"section"},{"location":"user_interface/timing_and_profiling/","page":"Timing and Profiling","title":"Timing and Profiling","text":"Timing and counting of the number of likelihood function evaluations (and other metrics of interest) are implemented using the @timeit_debug macro from TimerOutputs.jl. The debug version of the macro means that it is not used within the package (and thus doesn't impact runtime) unless it is enabled using LikelihoodBasedProfileWiseAnalysis.TimerOutputs.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis). Similarly, after being enabled it can be disabled again using LikelihoodBasedProfileWiseAnalysis.TimerOutputs.disable_debug_timings(LikelihoodBasedProfileWiseAnalysis). Enabling debug timings will cause methods that use the macro to recompile.","category":"page"},{"location":"user_interface/timing_and_profiling/","page":"Timing and Profiling","title":"Timing and Profiling","text":"The macro will only work correctly if distributed computing via Distributed.jl is not used - the timer on the main worker will not record the timings made on other workers (i.e. using Distributed; nworkers() returns 1). The macro cannot be used for dimensional and full likelihood sampling when their keyword argument use_threads is true - an exception will be raised.","category":"page"},{"location":"user_interface/timing_and_profiling/","page":"Timing and Profiling","title":"Timing and Profiling","text":"The exact timings extracted using this macro may not be quite true if function evaluation is very fast due to it's overhead. However, it's main value is in recording the number of function evaluations that are made. An example of this can be seen in Function Evaluation Timing - Logistic Model.","category":"page"},{"location":"user_interface/timing_and_profiling/","page":"Timing and Profiling","title":"Timing and Profiling","text":"using LikelihoodBasedProfileWiseAnalysis\n\n# model definition ...\n\nLikelihoodBasedProfileWiseAnalysis.TimerOutputs.enable_debug_timings(LikelihoodBasedProfileWiseAnalysis)\n\nLikelihoodBasedProfileWiseAnalysis.TimerOutputs.reset_timer!(LikelihoodBasedProfileWiseAnalysis.timer)\n\nunivariate_confidenceintervals!(model)\n\nLikelihoodBasedProfileWiseAnalysis.timer","category":"page"},{"location":"user_interface/coverage/predictions_and_realisations/#Predictions-and-Realisations","page":"Predictions and Realisations","title":"Predictions and Realisations","text":"","category":"section"},{"location":"user_interface/coverage/predictions_and_realisations/","page":"Predictions and Realisations","title":"Predictions and Realisations","text":"Pages = [\"predictions_and_realisations.md\"]","category":"page"},{"location":"user_interface/coverage/predictions_and_realisations/","page":"Predictions and Realisations","title":"Predictions and Realisations","text":"Usage on several models can be seen in the examples section, such as for the Logistic Model.","category":"page"},{"location":"user_interface/coverage/predictions_and_realisations/#Predictions","page":"Predictions and Realisations","title":"Predictions","text":"","category":"section"},{"location":"user_interface/coverage/predictions_and_realisations/","page":"Predictions and Realisations","title":"Predictions and Realisations","text":"check_dimensional_prediction_coverage\ncheck_univariate_prediction_coverage\ncheck_bivariate_prediction_coverage","category":"page"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_dimensional_prediction_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_dimensional_prediction_coverage","text":"check_dimensional_prediction_coverage(data_generator::Function, \n    generator_args::Union{Tuple, NamedTuple},\n    t::AbstractVector,\n    model::LikelihoodModel, \n    N::Int, \n    num_points_to_sample::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real}, \n    θindices::Union{Vector{Vector{Int}}, Vector{Vector{Symbol}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue;\n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction coverage of dimensional confidence samples (including full likelihood samples) for parameters in θindices given a model by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model. \nSampling points using sample_type.\nEvaluating predictions from the points in the samples and finding the prediction extrema.\nChecking whether the prediction extrema contain the true prediction value(s), in a pointwise and simultaneous fashion. The estimated simultaneous coverage is returned with a default 95% confidence interval within a DataFrame. \n\nThe prediction coverage from combining the prediction sets of multiple confidence profiles, choosing 1 to length(θindices) random combinations of θindices, is also evaluated (i.e. the final result is the union over all profiles in θindices). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nnum_points_to_sample: integer number of points to sample (for UniformRandomSamples and LatinHypercubeSamples sample types). For the UniformGridSamples sample type, if integer it is the number of points to grid over in each parameter dimension. If it is a vector of integers each index of the vector is the number of points to grid over in the corresponding parameter dimension. For example, [1,2] would mean a single point in dimension 1 and two points in dimension 2. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθindices: a vector of vectors of parameter indexes for the combinations of interest parameters to samples points from.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to find samples within and evaluate coverage at. Default is 0.95 (95%).\nsample_type: the sampling method used to sample parameter space. Available sample types are UniformGridSamples, UniformRandomSamples and LatinHypercubeSamples. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nlb: optional vector of lower bounds on parameters. Use to specify parameter lower bounds to sample over that are different than those contained in model.core. Default is Float64[] (use lower bounds from model.core).\nub: optional vector of upper bounds on parameters. Use to specify parameter upper bounds to sample over that are different than those contained in model.core. Default is Float64[] (use upper bounds from model.core).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence profile calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0. Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating dimensional samples into prediction space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Support for `dof`\nSetting the degrees of freedom of a sampled parameter confidence set to a value other than the interest parameter dimensionality is not currently supported (e.g. as supported for univariate and bivariate profiles). Support may be added in the future.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_univariate_prediction_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_univariate_prediction_coverage","text":"check_univariate_prediction_coverage(data_generator::Function, \n    generator_args::Union{Tuple, NamedTuple},\n    model::LikelihoodModel, \n    N::Int, \n    θtrue::AbstractVector{<:Real}, \n    θs::AbstractVector{<:Int64},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction coverage of univariate confidence profiles for parameters in θs given a model by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model and univariate confidence intervals. \nSampling points along the profile within the confidence intervals.\nEvaluating predictions from the points in the profile and finding the prediction extrema.\nChecking whether the prediction extrema contain the true prediction value(s), in a pointwise and simultaneous fashion. The estimated simultaneous coverage is returned with a default 95% confidence interval within a DataFrame. \n\nThe prediction coverage from combining the prediction sets of multiple confidence profiles, choosing 1 to length(θs) random combinations of θs, is also evaluated (i.e. the final result is the union over all profiles in θs). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nθtrue: a vector of true parameters values of the model for simulating data with. \nθs: a vector of parameters to profile, as a vector of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nnum_points_in_interval: an integer number of points to optionally evaluate within the confidence interval for each interest parameter using get_points_in_intervals!. Points are linearly spaced in the interval. Useful for predictions from univariate profiles. Default is 0. \nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval coverage at. Default is 0.95 (95%).\ndof: an integer ∈ [1, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the extremities of the univariate profile, i.e. the confidence interval. For parameter confidence intervals that are considered individually, it should be set to 1. For intervals that are considered simultaneously, it should be set to the number of intervals that are being calculated, i.e. model.core.num_pars when we wish the confidence interval for every parameter to hold simultaneously. Default is 1. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 1 and model.core.num_pars may not have a clear statistical interpretation.\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence interval calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0.  Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating univariate parameter confidence intervals into prediction space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: Not intended for use on bimodal univariate profile likelihoods\nThe current implementation only considers two extremes of the log-likelihood and whether the truth is between these two points. If the profile likelihood function is bimodal, it's possible the method has only found one set of correct confidence intervals (estimated coverage will be correct, but less than expected) or found one extrema on distinct sets (estimated coverage may be incorrect and will either be larger than expected or much lower than expected). \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_bivariate_prediction_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_bivariate_prediction_coverage","text":"check_bivariate_prediction_coverage(data_generator::Function, \n    generator_args::Union{Tuple, NamedTuple},\n    t::AbstractVector,\n    model::LikelihoodModel, \n    N::Int, \n    num_points::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real}, \n    θcombinations::Union{Vector{Vector{Int}}, Vector{Tuple{Int,Int}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction coverage of bivariate confidence profiles for parameters in θcombinations given a model by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model and bivariate confidence boundaries. \nSampling points within the polygon hull of the confidence boundaries.\nEvaluating predictions from the points in the profile and finding the prediction extrema.\nChecking whether the prediction extrema contain the true prediction value(s), in a pointwise and simultaneous fashion. The estimated simultaneous coverage is returned with a default 95% confidence interval within a DataFrame. \n\nThe prediction coverage from combining the prediction sets of multiple confidence profiles, choosing 1 to length(θcombinations) random combinations of θcombinations, is also evaluated (i.e. the final result is the union over all profiles in θcombinations). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nnum_points: positive number of points to find on the boundary at the specified confidence level using a single method. Or a vector of positive numbers of boundary points to find for each method in method (if method is a vector of AbstractBivariateMethod). Set to at least 3 within the function as some methods need at least three points to work. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθcombinations: a vector of pairs of parameters to profile, as a vector of vectors of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nnum_internal_points: an integer number of points to optionally evaluate within the a polygon hull approximation of a bivariate boundary for each interest parameter pair using sample_bivariate_internal_points!. Default is 0. \nhullmethod: method of type AbstractBivariateHullMethod used to create a 2D polygon hull that approximates the bivariate boundary from a set of boundary points and internal points (method dependent). For available methods see bivariate_hull_methods(). Default is MPPHullMethod() (MPPHullMethod).\nsample_type: either a UniformRandomSamples or LatinHypercubeSamples struct for how to sample internal points from the polygon hull. UniformRandomSamples are homogeneously sampled from the polygon and LatinHypercubeSamples use the intersection of a heuristically optimised Latin Hypercube sampling plan with the polygon. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level on which to find the profile_type boundary. Default is 0.95 (95%).\ndof: an integer ∈ [2, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the boundary of the bivariate profile. For bivariate profiles that are considered individually, it should be set to 2. For profiles that are considered simultaneously, it should be set to model.core.num_pars. Default is 2. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 2 and model.core.num_pars may not have a clear statistical interpretation.\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence interval calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0.  Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating bivariate parameter confidence intervals into prediction space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: May not work correctly on bimodal confidence boundaries\nThe current implementation constructs a single polygon with minimum polygon perimeter from the set of boundary points as the confidence boundary. If there are multiple distinct boundaries represented, then there will be edges connecting the distinct boundaries which the true parameter might be inside (but not inside either of the distinct boundaries). \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/predictions_and_realisations/#Realisations","page":"Predictions and Realisations","title":"Realisations","text":"","category":"section"},{"location":"user_interface/coverage/predictions_and_realisations/","page":"Predictions and Realisations","title":"Predictions and Realisations","text":"check_dimensional_prediction_realisations_coverage\ncheck_univariate_prediction_realisations_coverage\ncheck_bivariate_prediction_realisations_coverage","category":"page"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_dimensional_prediction_realisations_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_dimensional_prediction_realisations_coverage","text":"check_dimensional_prediction_realisations_coverage(data_generator::Function,\n    reference_set_generator::Function,\n    training_generator_args::Union{Tuple,NamedTuple},\n    testing_generator_args::Union{Tuple,NamedTuple},\n    t::AbstractVector,\n    model::LikelihoodModel, \n    N::Int, \n    num_points_to_sample::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real}, \n    θindices::Union{Vector{Vector{Int}}, Vector{Vector{Symbol}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue;\n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction reference set and realisation coverage of dimensional confidence samples (including full likelihood samples) for parameters in θindices given a model by: \n\nConstructing the confidence_level reference set for predictions from the fixed true parameter values, θ_true.\nRepeatedly drawing new observed training data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model using training data.\nSampling points using sample_type.\nEvaluating predictions from the points in the samples and finding the prediction extrema (reference tolerance sets).\nDrawing new observed testing data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nChecking whether the prediction extrema (reference tolerance set) contains the prediction reference set from Step 1, in a pointwise and simultaneous fashion. \nChecking whether the prediction extrema contain the observed testing data, in a pointwise and simultaneous fashion. \nThe estimated simultaneous coverage of the reference set and the prediction realisations (observed testing data) is returned with a default 95% confidence interval, alongside pointwise coverage, within a DataFrame. We also provided an alternate 'simultaneous' statistic for prediction realisation coverage; rather than testing whether 100% of prediction realisations are covered we test whether simultaneous_alternate_proportion proportion of prediction realisations are covered. \n\nThe prediction coverage from combining the prediction sets of multiple confidence profiles, choosing 1 to length(θindices) random combinations of θindices, is also evaluated (i.e. the final result is the union over all profiles in θindices). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. When used with training_generator_args, it outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model. When used with testing_generator_args, it outputs an array containing the observed data to use as the test data set.\nreference_set_generator: a function with three arguments which generates the confidence_level data reference set for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The three arguments must be the vector of true model parameters, θtrue, a Tuple or NamedTuple, generator_args, and a number (0.0, 1.0) for the confidence level at which to evaluate the reference set. When used with testing_generator_args it outputs a tuple of two arrays, (lq, uq), which contain the lower and upper quantiles of the reference set. \ntraining_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the training set of data. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via training_generator_args. \ntesting_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the test data set. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via testing_generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at, which are the same as the time points used to create the test data set.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nnum_points_to_sample: integer number of points to sample (for UniformRandomSamples and LatinHypercubeSamples sample types). For the UniformGridSamples sample type, if integer it is the number of points to grid over in each parameter dimension. If it is a vector of integers each index of the vector is the number of points to grid over in the corresponding parameter dimension. For example, [1,2] would mean a single point in dimension 1 and two points in dimension 2. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθindices: a vector of vectors of parameter indexes for the combinations of interest parameters to samples points from.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to find samples within and evaluate coverage at. Default is 0.95 (95%).\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nsample_type: the sampling method used to sample parameter space. Available sample types are UniformGridSamples, UniformRandomSamples and LatinHypercubeSamples. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nlb: optional vector of lower bounds on parameters. Use to specify parameter lower bounds to sample over that are different than those contained in model.core. Default is Float64[] (use lower bounds from model.core).\nub: optional vector of upper bounds on parameters. Use to specify parameter upper bounds to sample over that are different than those contained in model.core. Default is Float64[] (use upper bounds from model.core).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\nsimultaneous_alternate_proportion: a number ∈ (0.0, 1.0) for the alternate 'simultaneous' coverage statistic, testing whether at least this proportion of prediction realisations are covered. Recommended to be equal to region. Default is 0.95 (95%). \noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence profile calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0. Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating dimensional samples into prediction realisation space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction realisation coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Support for `dof`\nSetting the degrees of freedom of a sampled parameter confidence set to a value other than the interest parameter dimensionality is not currently supported (e.g. as supported for univariate and bivariate profiles). Support may be added in the future.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_univariate_prediction_realisations_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_univariate_prediction_realisations_coverage","text":"check_univariate_prediction_realisations_coverage(data_generator::Function, \n    training_generator_args::Union{Tuple, NamedTuple},\n    testing_generator_args::Union{Tuple, NamedTuple},\n    model::LikelihoodModel, \n    N::Int, \n    θtrue::AbstractVector{<:Real}, \n    θs::AbstractVector{<:Int64},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction reference set and realisation coverage of univariate confidence profiles for parameters in θs given a model by: \n\nConstructing the confidence_level reference set for predictions from the fixed true parameter values, θ_true.\nRepeatedly drawing new observed training data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model using training data.\nFitting the model and univariate confidence intervals using training data. \nSampling points along the profile within the confidence intervals.\nEvaluating predictions from the points in the profile and finding the prediction extrema (reference tolerance sets).\nDrawing new observed testing data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nChecking whether the prediction extrema (reference tolerance set) contains the prediction reference set from Step 1, in a pointwise and simultaneous fashion. \nChecking whether the prediction extrema contain the observed testing data, in a pointwise and simultaneous fashion. \nThe estimated simultaneous coverage of the reference set and the prediction realisations (observed testing data) is returned with a default 95% confidence interval, alongside pointwise coverage, within a DataFrame. We also provided an alternate 'simultaneous' statistic for prediction realisation coverage; rather than testing whether 100% of prediction realisations are covered we test whether simultaneous_alternate_proportion proportion of prediction realisations are covered. \n\nThe coverage from combining the prediction reference sets of multiple confidence profiles, choosing 1 to length(θs) random combinations of θs, is also evaluated (i.e. the final result is the union over all profiles in θs). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. When used with training_generator_args, it outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model. When used with testing_generator_args, it outputs an array containing the observed data to use as the test data set.\nreference_set_generator: a function with three arguments which generates the confidence_level data reference set for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The three arguments must be the vector of true model parameters, θtrue, a Tuple or NamedTuple, generator_args, and a number (0.0, 1.0) for the confidence level at which to evaluate the reference set. When used with testing_generator_args it outputs a tuple of two arrays, (lq, uq), which contain the lower and upper quantiles of the reference set. \ntraining_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the training set of data. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via training_generator_args. \ntesting_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the test data set. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via testing_generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at, which are the same as the time points used to create the test data set.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nθtrue: a vector of true parameters values of the model for simulating data with. \nθs: a vector of parameters to profile, as a vector of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nnum_points_in_interval: an integer number of points to optionally evaluate within the confidence interval for each interest parameter using get_points_in_intervals!. Points are linearly spaced in the interval. Useful for predictions from univariate profiles. Default is 0. \nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval coverage at. Default is 0.95 (95%).\ndof: an integer ∈ [1, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the extremities of the univariate profile, i.e. the confidence interval. For parameter confidence intervals that are considered individually, it should be set to 1. For intervals that are considered simultaneously, it should be set to the number of intervals that are being calculated, i.e. model.core.num_pars when we wish the confidence interval for every parameter to hold simultaneously. Default is 1. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 1 and model.core.num_pars may not have a clear statistical interpretation.\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\nsimultaneous_alternate_proportion: a number ∈ (0.0, 1.0) for the alternate 'simultaneous' coverage statistic, testing whether at least this proportion of prediction realisations are covered. Recommended to be equal to region. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence interval calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0. Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating univariate parameter confidence intervals into prediction realisation space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction realisation coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: Not intended for use on bimodal univariate profile likelihoods\nThe current implementation only considers two extremes of the log-likelihood and whether the truth is between these two points. If the profile likelihood function is bimodal, it's possible the method has only found one set of correct confidence intervals (estimated coverage will be correct, but less than expected) or found one extrema on distinct sets (estimated coverage may be incorrect and will either be larger than expected or much lower than expected). \n\n\n\n\n\n","category":"function"},{"location":"user_interface/coverage/predictions_and_realisations/#LikelihoodBasedProfileWiseAnalysis.check_bivariate_prediction_realisations_coverage","page":"Predictions and Realisations","title":"LikelihoodBasedProfileWiseAnalysis.check_bivariate_prediction_realisations_coverage","text":"check_bivariate_prediction_realisations_coverage(data_generator::Function, \n    reference_set_generator::Function,\n    training_generator_args::Union{Tuple, NamedTuple},\n    testing_generator_args::Union{Tuple, NamedTuple},\n    t::AbstractVector,\n    model::LikelihoodModel, \n    N::Int, \n    num_points::Union{Int, Vector{<:Int}},\n    θtrue::AbstractVector{<:Real}, \n    θcombinations::Union{Vector{Vector{Int}}, Vector{Tuple{Int,Int}}},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the prediction reference set and realisation coverage of bivariate confidence profiles for parameters in θcombinations given a model by:\n\nConstructing the confidence_level reference set for predictions from the fixed true parameter values, θ_true.\nRepeatedly drawing new observed training data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nFitting the model using training data.\nFitting the model and bivariate confidence boundaries using training data. \nSampling points within the polygon hull of the confidence boundaries.\nEvaluating predictions from the points in the profile and finding the prediction extrema (reference tolerance sets).\nDrawing new observed testing data using data_generator and training_generator_args for fixed true parameter values, θtrue, and fixed true prediction value. \nChecking whether the prediction extrema (reference tolerance set) contains the prediction reference set from Step 1, in a pointwise and simultaneous fashion. \nChecking whether the prediction extrema contain the observed testing data, in a pointwise and simultaneous fashion. \nThe estimated simultaneous coverage of the reference set and the prediction realisations (observed testing data) is returned with a default 95% confidence interval, alongside pointwise coverage, within a DataFrame. We also provided an alternate 'simultaneous' statistic for prediction realisation coverage; rather than testing whether 100% of prediction realisations are covered we test whether simultaneous_alternate_proportion proportion of prediction realisations are covered. \n\nThe prediction coverage from combining the prediction reference sets of multiple confidence profiles, choosing 1 to length(θcombinations) random combinations of θcombinations, is also evaluated (i.e. the final result is the union over all profiles in θcombinations). \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. When used with training_generator_args, it outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model. When used with testing_generator_args, it outputs an array containing the observed data to use as the test data set.\nreference_set_generator: a function with three arguments which generates the confidence_level data reference set for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The three arguments must be the vector of true model parameters, θtrue, a Tuple or NamedTuple, generator_args, and a number (0.0, 1.0) for the confidence level at which to evaluate the reference set. When used with testing_generator_args it outputs a tuple of two arrays, (lq, uq), which contain the lower and upper quantiles of the reference set. \ntraining_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the training set of data. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via training_generator_args. \ntesting_generator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at, used to create the test data set. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via testing_generator_args. \nt: a vector of time points to compute predictions and evaluate coverage at, which are the same as the time points used to create the test data set.\nmodel: a LikelihoodModel containing model information.\nN: a positive number of coverage simulations.\nnum_points: positive number of points to find on the boundary at the specified confidence level using a single method. Or a vector of positive numbers of boundary points to find for each method in method (if method is a vector of AbstractBivariateMethod). Set to at least 3 within the function as some methods need at least three points to work. \nθtrue: a vector of true parameters values of the model for simulating data with. \nθcombinations: a vector of pairs of parameters to profile, as a vector of vectors of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nnum_internal_points: an integer number of points to optionally evaluate within the a polygon hull approximation of a bivariate boundary for each interest parameter pair using sample_bivariate_internal_points!. Default is 0. \nhullmethod: method of type AbstractBivariateHullMethod used to create a 2D polygon hull that approximates the bivariate boundary from a set of boundary points and internal points (method dependent). For available methods see bivariate_hull_methods(). Default is MPPHullMethod() (MPPHullMethod).\nsample_type: either a UniformRandomSamples or LatinHypercubeSamples struct for how to sample internal points from the polygon hull. UniformRandomSamples are homogeneously sampled from the polygon and LatinHypercubeSamples use the intersection of a heuristically optimised Latin Hypercube sampling plan with the polygon. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level on which to find the profile_type boundary. Default is 0.95 (95%).\ndof: an integer ∈ [2, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the boundary of the bivariate profile. For bivariate profiles that are considered individually, it should be set to 2. For profiles that are considered simultaneously, it should be set to model.core.num_pars. Default is 2. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 2 and model.core.num_pars may not have a clear statistical interpretation.\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\nsimultaneous_alternate_proportion: a number ∈ (0.0, 1.0) for the alternate 'simultaneous' coverage statistic, testing whether at least this proportion of prediction realisations are covered. Recommended to be equal to region. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence interval calculations within each iteration (true). Default is false.\nmanual_GC_calls: boolean variable specifying whether to manually call garbage collection, GC.gc(), after every 10 iterations (distributed_over_parameters=true) or after every iteration on that worker (distributed_over_parameters=false). May be important to correctly free up memory for coverage simulations that use distributed or threaded workloads for Julia versions prior to v1.10.0.  Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of propagating bivariate parameter confidence intervals into prediction realisation space. The simulation uses Distributed.jl to parallelise the workload.\n\nThe uncertainty in estimates of the prediction realisation coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few pairs of model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of pairs of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: May not work correctly on bimodal confidence boundaries\nThe current implementation constructs a single polygon with minimum polygon perimeter from the set of boundary points as the confidence boundary. If there are multiple distinct boundaries represented, then there will be edges connecting the distinct boundaries which the true parameter might be inside (but not inside either of the distinct boundaries). \n\n\n\n\n\n","category":"function"},{"location":"user_interface/saving_and_loading/#Saving-and-Loading-LikelihoodModels","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"","category":"section"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"Models can be saved easily using BSON.jl as they are defined using a Julia struct. ","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"The only thing to be careful of is ensuring that empty dataframe rows within the model are removed prior to saving using trim_model_dfs! (as they include undefined references). ","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"using LikelihoodBasedProfileWiseAnalysis\n\n# function definitions and initialisation...\nmodel = \ntrim_model_dfs!(model)\nusing BSON: @save\n\n@save \"mymodel.bson\" model","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"To load model in a new session/file:","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"using LikelihoodBasedProfileWiseAnalysis\nusing BSON: @load\n\n# function definitions....\n\n@load \"mymodel.bson\" model","category":"page"},{"location":"user_interface/saving_and_loading/#Potential-Issues-When-Loading","page":"Saving and Loading LikelihoodModels","title":"Potential Issues When Loading","text":"","category":"section"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"There are a couple of things to watch out for if the model saved had functions defined in model.core:","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"The log-likelihood function (and if defined, the prediction and error functions) must be defined with the same name and be available in the scope we are loading model in. Functions used within model.core.optimizationsettings must also be defined in the scope we are loading model in (i.e. by loading LikelihoodBasedProfileWiseAnalysis).\nThe variable name model has when saved is the same name it needs to be loaded with.","category":"page"},{"location":"user_interface/saving_and_loading/#Fixing-Issues","page":"Saving and Loading LikelihoodModels","title":"Fixing Issues","text":"","category":"section"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"The first of these issues we can get around by converting our CoreLikelihoodModel to a BaseLikelihoodModel before saving. The only difference between these two structs is that BaseLikelihoodModel doesn't contain fields for the log-likelihood, prediction and error functions. This means we can load a saved model without needing those functions defined in the local scope, which may be useful for workflows where the computation is performed in one file and plotting of outputs is performed in another file.","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"We can use remove_functions_from_core! to perform this task, pulling out the original model.core so we can put it back into model later if desired: ","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"core_original = remove_functions_from_core!(model)\n@save \"mymodel.bson\" model\n\n# restore the core that has the functions if desired\nmodel.core = core_original ","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"If we want to add the log-likelihood function to this loaded version of the model we can use add_loglikelihood_function! after loading LikelihoodBasedProfileWiseAnalysis.","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"using LikelihoodBasedProfileWiseAnalysis\n# log-likelihood function function definition\nfunction loglikefunction(θ, data); return ... end\nadd_loglikelihood_function!(model, loglikefunction)","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"danger: Missing log-likelihood function\nTrying to use a profile-related function will result in an error if the log-likelihood function is not defined in model.core.","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"The prediction function can be added in the same fashion using add_prediction_function!. However, the log-likelihood function must have been added first.","category":"page"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"The error function can also be added in the same fashion using add_error_function!. However, the log-likelihood must have been added first.","category":"page"},{"location":"user_interface/saving_and_loading/#Saving-and-Loading-Functions","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading Functions","text":"","category":"section"},{"location":"user_interface/saving_and_loading/","page":"Saving and Loading LikelihoodModels","title":"Saving and Loading LikelihoodModels","text":"trim_model_dfs!\nremove_functions_from_core!\nadd_loglikelihood_function!","category":"page"},{"location":"user_interface/saving_and_loading/#LikelihoodBasedProfileWiseAnalysis.trim_model_dfs!","page":"Saving and Loading LikelihoodModels","title":"LikelihoodBasedProfileWiseAnalysis.trim_model_dfs!","text":"trim_model_dfs!(model::LikelihoodModel)\n\nRemoves any unitialised rows of model.uni_profiles_df, model.bivprofilesdf and model.dimsamplesdf in place, which contain undefined references and will prevent saving using BSON.jl.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/saving_and_loading/#LikelihoodBasedProfileWiseAnalysis.remove_functions_from_core!","page":"Saving and Loading LikelihoodModels","title":"LikelihoodBasedProfileWiseAnalysis.remove_functions_from_core!","text":"remove_functions_from_core!(model::LikelihoodModel)\n\nRemoves the functions from model.core by replacing the CoreLikelihoodModel at model.core with a BaseLikelihoodModel, returning the CoreLikelihoodModel.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/saving_and_loading/#LikelihoodBasedProfileWiseAnalysis.add_loglikelihood_function!","page":"Saving and Loading LikelihoodModels","title":"LikelihoodBasedProfileWiseAnalysis.add_loglikelihood_function!","text":"add_loglikelihood_function!(model::LikelihoodModel, loglikefunction::Function; \n    optimizationsettings::OptimizationSettings=default_OptimizationSettings())\n\nAdds a log-likelihood function, loglikefunction, to model as well as optimization settings, optimizationsettings, using default_OptimizationSettings.\n\nRequirements for loglikefunction: loglikelihood function which takes two arguments, θ and data, in that order, where θ is a vector containing the values of each parameter in θnames and data is a Tuple or NamedTuple containing any additional information required by the log-likelihood function, such as the time points to be evaluated at.\n\n\n\n\n\n","category":"function"},{"location":"examples/lotka-volterra/#Lotka-Volterra-Model","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"The code included in this example is compiled into a single file here.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"The Lotka-Volterra model with a normal data distribution [1] has the following differential equations for the population size of the prey species x(t) and predator species y(t):","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"    fracmathrmdx(t)mathrmdt = alpha x(t) - x(t)y(t)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"    fracmathrmdy(t)mathrmdt = beta x(t)y(t) - y(t)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"where the model parameter vector is given by theta^M = (alpha beta x(0) y(0)). The corresponding additive Gaussian data distribution, with a fixed standard deviation, has a density function for the observed data given by: ","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"    y_i sim p(y_i  theta) sim mathcalN(z_i(theta^M) sigma^2 mathbbI)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"where  z_i(theta^M)=z(t_i theta^M) = (x(t_i theta^M) y(t_i theta^M)) is the model solution of the differential equations, meaning at each t_i we have an observation of both x(t) and y(t), y_i^textrmo=(x_i^textrmo y_i^textrmo), mathbbI is a 2times2 identity matrix and sigma=02.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"The true parameter values are theta^M =(09 11 08 03). The corresponding lower and upper parameter bounds are a = (07 07 05 01) and b = (12 14 12 05). Observation times are t_1I = 005107. The times considered for predictions are extended up to t_I=10. The original implementation can be found at https://github.com/ProfMJSimpson/Workflow. Example realisations, the true model trajectory and 95% population reference set under this parameterisation can be seen in the figure below:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/#Initial-Setup","page":"Lotka-Volterra Model","title":"Initial Setup","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Here we add six worker processes, which matches the number of bivariate profiles. For coverage testing we recommend setting this number as discussed in Import Package and Set Up Distributed Environment. We're also using StaticArrays to slightly speed up the differential equation solver.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"using Distributed\nif nprocs()==1; addprocs(6, env=[\"JULIA_NUM_THREADS\"=>\"1\"]) end\n@everywhere using Random, Distributions, DifferentialEquations, StaticArrays\n@everywhere using LikelihoodBasedProfileWiseAnalysis\nusing Combinatorics","category":"page"},{"location":"examples/lotka-volterra/#Model-and-Likelihood-Function-Definition","page":"Lotka-Volterra Model","title":"Model and Likelihood Function Definition","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"@everywhere function lotka_static(C,p,t)\n    dC_1=p[1]*C[1] - C[1]*C[2];\n    dC_2=p[2]*C[1]*C[2] - C[2];\n    SA[dC_1, dC_2]\nend\n\n@everywhere function odesolver(t,α,β,C01,C02)\n    p=SA[α,β]\n    C0=SA[C01,C02]\n    tspan=(0.0,t[end])\n    prob=ODEProblem(lotka_static,C0,tspan,p)\n    sol=solve(prob, AutoTsit5(Rosenbrock23()), saveat=t);\n    return sol[1,:], sol[2,:]\nend\n\n@everywhere function ODEmodel(t,θ)\n    return odesolver(t,θ[1],θ[2],θ[3],θ[4])\nend\n\n@everywhere function loglhood(θ, data)\n    (y1, y2) = ODEmodel(data.t, θ)\n    e=loglikelihood(data.dist, data.y_obs[:, 1] .- y1)  \n    f=loglikelihood(data.dist, data.y_obs[:, 2] .- y2)\n    return e+f\nend","category":"page"},{"location":"examples/lotka-volterra/#Initial-Data-and-Parameter-Definition","page":"Lotka-Volterra Model","title":"Initial Data and Parameter Definition","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"# true parameters\nα_true=0.9; β_true=1.1; x0_true=0.8; y0_true=0.3\n@everywhere global σ=0.2\nθ_true=[α_true, β_true, x0_true, y0_true]\n\nt=LinRange(0,7,15)\ny_true = hcat(ODEmodel(t, θ_true)...)\ny_obs = [0.99 0.22; 1.02 0.26; 1.28 0.38; 1.92 0.36; 2.03 0.80; 1.41 1.78;\n         1.54 2.04; 0.67 1.63; 0.18 1.45; 0.44 1.13; 0.74 0.94; 0.37 0.86;\n         0.01 0.16; 0.65 0.52; 0.54 0.32]\n\n# Named tuple of all data required within the log-likelihood function\ndata = (y_obs=y_obs, t=t, dist=Normal(0, σ))\n\n# Bounds on model parameters \nαmin, αmax = (0.7, 1.2)\nβmin, βmax = (0.7, 1.4)\nx0min, x0max = (0.5, 1.2)\ny0min, y0max = (0.1, 0.5)\nlb_original = [αmin, βmin, x0min, y0min] .* 1.0\nub_original = [αmax, βmax, x0max, y0max] .* 1.0\n\nαmin, αmax = (0.4, 1.5)\nβmin, βmax = (0.7, 1.8)\nx0min, x0max = (0.4, 1.3)\ny0min, y0max = (0.02, 0.8)\nlb = [αmin,βmin,x0min,y0min]\nub = [αmax,βmax,x0max,y0max]\n\nθG = θ_true\nθnames = [:α, :β, :x0, :y0]\npar_magnitudes = [1,1,1,1]","category":"page"},{"location":"examples/lotka-volterra/#LikelihoodModel-Initialisation","page":"Lotka-Volterra Model","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5,))\nmodel = initialise_LikelihoodModel(loglhood, data, θnames, θG, lb, ub, par_magnitudes, optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/#Full-Parameter-Vector-Confidence-Set-Evaluation","page":"Lotka-Volterra Model","title":"Full Parameter Vector Confidence Set Evaluation","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To evaluate the full parameter vector confidence set at a 95% confidence level we use:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"full_likelihood_sample!(model, 500000, use_distributed=true)","category":"page"},{"location":"examples/lotka-volterra/#Profiling","page":"Lotka-Volterra Model","title":"Profiling","text":"","category":"section"},{"location":"examples/lotka-volterra/#Univariate-Profiles","page":"Lotka-Volterra Model","title":"Univariate Profiles","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To find the confidence intervals for all three parameters at a 95% confidence level (the default), we use:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"univariate_confidenceintervals!(model)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Similarly, if we wish to find simultaneous 95% confidence intervals for the parameters we set the degrees of freedom parameter, dof, to the number of model parameters (instead of 1).","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"univariate_confidenceintervals!(model, dof=model.core.num_pars) # model.core.num_pars=4","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"We also evaluate some points within the intervals for the purposes of profile visualisation.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"get_points_in_intervals!(model, 20, additional_width=0.2)","category":"page"},{"location":"examples/lotka-volterra/#Bivariate-Profiles","page":"Lotka-Volterra Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To evaluate the bivariate boundaries for all six bivariate parameter combinations, here we use the IterativeBoundaryMethod, which uses a 10 point ellipse approximation of the boundary as a starting guess using RadialMLEMethod. The boundaries in this example are reasonably convex, which makes this starting guess appropriate. To speed up computation we provide stronger optimization settings.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 30, \n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 1.0, use_ellipse=true), \n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Similarly, if we wish to evaluate simultaneous 95% bivariate profiles we set the degrees of freedom parameter, dof, to the number of model parameters (instead of 2).","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 30, \n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 1.0, use_ellipse=true), \n    dof=model.core.num_pars,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/#Plots-of-Profiles","page":"Lotka-Volterra Model","title":"Plots of Profiles","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To visualise plots of these profiles we load Plots alongside a plotting backend. Here we use GR.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"using Plots, Plots.PlotMeasures; gr()\nPlots.reset_defaults(); Plots.scalefontsizes(0.75)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Univariate and bivariate profiles can either be visualised individually or in comparison to profiles at the same confidence level and degrees of freedom. ","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Here we plot the univariate profiles formed at a 95% confidence level and 1 degree of freedom.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"plts = plot_univariate_profiles(model, confidence_levels=[0.95], dofs=[1])\n\nplt = plot(plts..., layout=(2,2),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Similarly, here we plot the simultaneous bivariate profiles formed at a 95% confidence level and 4 degrees of freedom.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"plts = plot_bivariate_profiles(model, confidence_levels=[0.95], dofs=[model.core.num_pars])\n\nplt = plot(plts..., layout=(2,3),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/#Predictions","page":"Lotka-Volterra Model","title":"Predictions","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To make predictions for the model trajectory and the 1-delta population reference set we define the following functions, which then need to be added to our LikelihoodModel. The region variable in errorfunction should be set equal to 1-delta when generating predictions. These could also be added in initialise_LikelihoodModel.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"@everywhere function predictfunction(θ, data, t=data.t)\n    y1, y2 = ODEmodel(t, θ) \n    y = hcat(y1,y2)\n    return y\nend\n\n@everywhere function errorfunction(predictions, θ, region); normal_error_σ_known(predictions, θ, region, σ) end\n\nadd_prediction_function!(model, predictfunction)\nadd_error_function!(model, errorfunction)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To generate profile-wise predictions for each of the evaluated profiles we first define the desired time points for prediction and then evaluate the approximate model trajectory confidence sets and (1-delta 1-alpha) population reference tolerance sets. By default, the population reference tolerance set evaluates reference interval regions at the same level as the default confidence level (1-delta = 1-alpha = 095); however, this is not required. We are making predictions outside the range of the data used to evaluate the profiles.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"t_pred=LinRange(0,10,201)\n\ngenerate_predictions_univariate!(model, t_pred)\ngenerate_predictions_bivariate!(model, t_pred)\ngenerate_predictions_dim_samples!(model, t_pred) # for the full likelihood sample","category":"page"},{"location":"examples/lotka-volterra/#Plotting-Predictions","page":"Lotka-Volterra Model","title":"Plotting Predictions","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"We can plot the predictions of individual profiles or the union of all profiles at a given number of interest parameters, confidence level, degrees of freedom and reference interval region (if relevant). When plotting the union of these predictions we can compare it to the result of the full likelihood sample, which here used LatinHypercubeSamples, the default. Here we plot the results from simultaneous profiles.","category":"page"},{"location":"examples/lotka-volterra/#Model-Trajectory","page":"Lotka-Volterra Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"model_trajectory = ODEmodel(t_pred, θ_true)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"using Plots; gr()\nplt=plot_predictions_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # univariate profiles\n\nplot!(plt; dpi=150, size=(450, 300), xlims=(t_pred[1], t_pred[end]))\nplot!(plt[1], t_pred, model_trajectory[1],\n    lw=3, color=:turquoise4, linestyle=:dash)\nplot!(plt[2], t_pred, model_trajectory[2],\n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"plt=plot_predictions_union(model, t_pred, 2, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # bivariate profiles\n\nplot!(plt; dpi=150, size=(450, 300), xlims=(t_pred[1], t_pred[end]))\nplot!(plt[1], t_pred, model_trajectory[1],\n    lw=3, color=:turquoise4, linestyle=:dash)\nplot!(plt[2], t_pred, model_trajectory[2],\n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/#1-\\delta-Population-Reference-Set","page":"Lotka-Volterra Model","title":"1-delta Population Reference Set","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"lq, uq = errorfunction(hcat(ODEmodel(t_pred, θ_true)...), θ_true, 0.95)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"using Plots; gr()\nplt = plot_realisations_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # univariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3],\n    xlims=(t_pred[1], t_pred[end]), dpi=150, size=(450, 300))","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"plt = plot_realisations_union(model, t_pred, 2, dof=model.core.num_pars, \n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # bivariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3],\n    xlims=(t_pred[1], t_pred[end]), dpi=150, size=(450, 300))","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"(Image: )","category":"page"},{"location":"examples/lotka-volterra/#Coverage-Testing","page":"Lotka-Volterra Model","title":"Coverage Testing","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To conduct an investigation into the coverage properties of the profiles and profile-wise predictions sets we can perform a simulation study using the provided coverage functions. The procedures are effectively identical to those used for the Logistic Model; the commentary for that example remains true for this example. ","category":"page"},{"location":"examples/lotka-volterra/#Data-Generation","page":"Lotka-Volterra Model","title":"Data Generation","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"First we define functions and arguments which we use to simulate new training and testing data, and evaluate the true 1-delta population reference set, given the true parameter values. ","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"# DATA GENERATION FUNCTION AND ARGUMENTS\n@everywhere function data_generator(θtrue, generator_args::NamedTuple)\n    y_obs = generator_args.y_true .+ rand(generator_args.dist, length(generator_args.t), 2)\n    if generator_args.is_test_set; return y_obs end\n    data = (y_obs=y_obs, generator_args...)\n    return data\nend\n\n@everywhere function reference_set_generator(θtrue, generator_args::NamedTuple, region::Float64)\n    lq, uq = errorfunction(generator_args.y_true, θtrue, region)\n    return (lq, uq)\nend\n\ntraining_gen_args = (y_true=y_true, t=t, dist=Normal(0, σ), is_test_set=false)\ntesting_gen_args = (y_true=hcat(ODEmodel(t_pred, θ_true)...), t=t_pred, dist=Normal(0, σ), is_test_set=true)","category":"page"},{"location":"examples/lotka-volterra/#Parameter-Confidence-Intervals","page":"Lotka-Volterra Model","title":"Parameter Confidence Intervals","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Coverage of parameter confidence intervals:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nuni_coverage_df = check_univariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, θ_true, collect(1:model.core.num_pars),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/#Bivariate-Profiles-2","page":"Lotka-Volterra Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Coverage of the true value of each set of bivariate interest parameters:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_coverage_df = check_bivariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, 50, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method = IterativeBoundaryMethod(20, 5, 5, 0.15, 0.1, use_ellipse=true), \n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Coverage of the true bivariate boundary. 5000 samples corresponds to around 200-400 retained points:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_boundary_coverage_df = check_bivariate_boundary_coverage(data_generator,\n    training_gen_args, model, 100, 30, 5000, θ_true,\n    collect(combinations(1:model.core.num_pars, 2)); \n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 0.1, use_ellipse=true), \n    coverage_estimate_quantile_level=0.9,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/#Prediction-Coverage","page":"Lotka-Volterra Model","title":"Prediction Coverage","text":"","category":"section"},{"location":"examples/lotka-volterra/#Model-Trajectory-2","page":"Lotka-Volterra Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To test the coverage of the true model trajectory we can use check_dimensional_prediction_coverage, check_univariate_prediction_coverage and check_bivariate_prediction_coverage. Again we use the default 95% confidence level here. Given a sufficient number of sampled points we expect the model trajectory coverage from the trajectory confidence set from propagating forward the full parameter vector 95% confidence set to have 95% simultaneous coverage. ","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfull_trajectory_coverage_df = check_dimensional_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 500000, \n    θ_true, [collect(1:model.core.num_pars)])\n\nuni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"Repeating the coverage of univariate and bivariate profiles using the profile path approach:","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"uni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars,\n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/lotka-volterra/#1-\\delta-Population-Reference-Set-and-Observations","page":"Lotka-Volterra Model","title":"1-delta Population Reference Set and Observations","text":"","category":"section"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"To test the coverage of the 1-delta population reference set as well as observations we can use check_dimensional_prediction_realisations_coverage, check_univariate_prediction_realisations_coverage and check_bivariate_prediction_realisations_coverage. Here we will only look at the coverage for simultaneous profiles.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors.","category":"page"},{"location":"examples/lotka-volterra/","page":"Lotka-Volterra Model","title":"Lotka-Volterra Model","text":"full_reference_coverage_df = check_dimensional_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, 500000, \n    θ_true, [collect(1:model.core.num_pars)])\n\nuni_reference_coverage_df = check_univariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars,\n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_reference_coverage_df = check_bivariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, 20, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"user_interface/plots/#Plots","page":"Plots","title":"Plots","text":"","category":"section"},{"location":"user_interface/plots/","page":"Plots","title":"Plots","text":"We provide functions for plotting univariate profiles, bivariate profiles and predictions of the model trajectory and (1-delta) population reference set from evaluated parameter confidence sets (i.e. profiles).","category":"page"},{"location":"user_interface/plots/#Univariate-Profiles","page":"Plots","title":"Univariate Profiles","text":"","category":"section"},{"location":"user_interface/plots/","page":"Plots","title":"Plots","text":"plot_univariate_profiles\nplot_univariate_profiles_comparison","category":"page"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_univariate_profiles","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_univariate_profiles","text":"plot_univariate_profiles(model::LikelihoodModel,\n    xlim_scaler::Real=0.2,\n    ylim_scaler::Real=0.2;\n    θs_to_plot::Vector=Int[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=AbstractProfileType[], \n    num_points_in_interval::Int=0,\n    palette_to_use::Symbol=:Paired_6, \n    kwargs...)\n\nReturns a vector of plots of univariate profiles contained with the model struct that meet the requirements of the univariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). \n\nThe profiles plotted are based on the specified θs_to_plot, confidence_levels, dofs and profile_types. By default, will plot all univariate profiles generated.\n\nIf num_points_in_interval is greater than 0 then get_points_in_intervals! will be called - use to obtain smoother profile plots.\n\nxlim_scaler and ylim_scaler are used to uniformly push the xlimits and ylimits away from the location of the confidence interval - if they are zero, then the confidence interval gives the location of the xlimits and the lower of the ylimits. If they are 1 then the corresponding limits have a range 100% wider than the confidence interval.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_univariate_profiles_comparison","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_univariate_profiles_comparison","text":"plot_univariate_profiles_comparison(model::LikelihoodModel, \n    xlim_scaler::Real=0.2,\n    ylim_scaler::Real=0.2;\n    θs_to_plot::Vector=Int[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=AbstractProfileType[], \n    num_points_in_interval::Int=0,\n    palette_to_use::Symbol=:Paired_6,\n    label_only_lines::Bool=false,\n    kwargs...)\n\nReturns a vector of comparison plots of univariate profiles contained with the model struct that meet the requirements of the univariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). Comparisons are between profile_types at the same confidence_level and dof for a given parameter.\n\nThe profiles plotted are based on the specified θs_to_plot, confidence_levels, dofs and profile_types. By default, will plot all univariate profiles generated.\n\nIf num_points_in_interval is greater than 0 then get_points_in_intervals! will be called - use to obtain smoother profile plots.\n\nIf label_only_lines=true then only the vertical and horizontal MLE point and confidence threshold lines will be labelled in the legend. Otherwise, profiles will be labelled by their profile_type.\n\nxlim_scaler and ylim_scaler are used to uniformly push the xlimits and ylimits away from the location of the confidence interval - if they are zero, then the confidence interval gives the location of the xlimits and the lower of the ylimits. If they are 1 then the corresponding limits have a range 100% wider than the confidence interval.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#Bivariate-Profiles-and-Samples","page":"Plots","title":"Bivariate Profiles and Samples","text":"","category":"section"},{"location":"user_interface/plots/","page":"Plots","title":"Plots","text":"plot_bivariate_profiles\nplot_bivariate_profiles_comparison\nplot_bivariate_profiles_iterativeboundary_gif","category":"page"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles","text":"plot_bivariate_profiles(model::LikelihoodModel,\n    xlim_scaler::Real=0.2,\n    ylim_scaler::Real=0.2;\n    for_dim_samples::Bool=false,\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=AbstractProfileType[],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    palette_to_use::Symbol=:Paired_6,\n    include_internal_points::Bool=true,\n    max_internal_points::Int=1000,\n    markeralpha=1.0,\n    kwargs...)\n\nReturns a vector of plots of bivariate profiles contained with the model struct that meet the requirements of the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments).\n\nThe profiles plotted are based on the specified θcombinations_to_plot, confidence_levels, dofs, profile_types, methods and sample_types. By default, will plot all bivariate profiles generated. If for_dim_samples=false it will plot bivariate profiles generated by an AbstractBivariateMethod. Otherwise, it will plot bivariate profiles generated by an AbstractSampleType.\n\nIf include_internal_points=true then points inside the boundary up to max_internal_points will be plotted (these are chosen randomly). Otherwise, only the boundary of the profile will be plotted. If plotting bivariate profiles from an AbstractSampleType this boundary will be estimated using LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull.\n\nxlim_scaler and ylim_scaler are used to uniformly push the xlimits and ylimits away from the location of the confidence boundary - if they are zero, then the extrema of the confidence boundary gives the location of the xlimits and the ylimits. If they are 1 then the corresponding limits have a range 100% wider than the extrema of the confidence boundary.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles_comparison","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles_comparison","text":"plot_bivariate_profiles_comparison(model::LikelihoodModel,\n    xlim_scaler::Real=0.2,\n    ylim_scaler::Real=0.2;\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=AbstractProfileType[],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    compare_within_methods::Bool=false,\n    include_dim_samples::Bool=false,\n    palette_to_use::Symbol=:Paired_6, \n    markeralpha::Number=0.7,\n    label_only_MLE::Bool=false,\n    kwargs...)\n\nReturns a vector of comparison plots of bivariate profiles contained with the model struct that meet the requirements of the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). Comparisons are between profile_types at the same confidence_level and dof for a given parameter combination; will also be within methods if compare_within_methods=true.\n\nThe profiles plotted are based on the specified θcombinations_to_plot, confidence_levels, dofs, profile_types, methods and sample_types. By default, will plot all bivariate profiles generated. If include_dim_samples=true it will also include the concave hull boundary of bivariate profiles generated by an AbstractSampleType in the comparison (using LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull).\n\nIf label_only_MLE=true, then only the MLE point will be labelled in the legend. Otherwise, profiles will be labelled by their profile_type or sample_type.\n\nxlim_scaler and ylim_scaler are used to uniformly push the xlimits and ylimits away from the location of the confidence boundary - if they are zero, then the extrema of the confidence boundary gives the location of the xlimits and the ylimits. If they are 1 then the corresponding limits have a range 100% wider than the extrema of the confidence boundary.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles_iterativeboundary_gif","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_bivariate_profiles_iterativeboundary_gif","text":"plot_bivariate_profiles_iterativeboundary_gif(model::LikelihoodModel,\n    xlim_scaler::Real=0.2,\n    ylim_scaler::Real=0.2;\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=AbstractProfileType[],\n    palette_to_use::Symbol=:Paired_6,\n    save_as_separate_plots::Bool=false,\n    markeralpha=1.0,\n    save_folder=nothing,\n    kwargs...)\n\nSaves a gif of the boundary of bivariate profiles generated using IterativeBoundaryMethod in save_folder that also meet the requirements of the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments).\n\nThe profiles plotted are based on the specified θcombinations_to_plot, confidence_levels, dofs and profile_types.\n\nxlim_scaler and ylim_scaler are used to uniformly push the xlimits and ylimits away from the location of the final confidence boundary - if they are zero, then the extrema of the confidence boundary gives the location of the xlimits and the ylimits. If they are 1 then the corresponding limits have a range 100% wider than the extrema of the confidence boundary. \n\nIf save_as_separate_plots=true then alongside the saved gif, each frame of the gif will also be saved as a .png.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#Predictions","page":"Plots","title":"Predictions","text":"","category":"section"},{"location":"user_interface/plots/","page":"Plots","title":"Plots","text":"plot_predictions_individual\nplot_predictions_union\nplot_realisations_individual\nplot_realisations_union","category":"page"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_predictions_individual","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_predictions_individual","text":"plot_predictions_individual(model::LikelihoodModel,\n    t::AbstractVector,\n    profile_dimension::Int=1;\n    xlabel::String=\"t\",\n    ylabel::Union{Nothing,String,Vector{String}}=nothing,\n    for_dim_samples::Bool=false,\n    include_MLE::Bool=true,\n    θs_to_plot::Vector=Int[],\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    θindices_to_plot::Vector=Vector{Int}[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    profile_types::Vector{<:AbstractProfileType}=[LogLikelihood()],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    linealpha=0.4, \n    kwargs...)\n\nReturns a vector of plots of profile-wise predictions of the model trajectory formed from profiles with interest parameter dimension profile_dimension that meet the requirement of the relevant method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). \n\nThe plotted extrema are the extrema of approximate profile-wise confidence_level trajectory confidence set from each profile.\n\nt should be the same points used to generate predictions in generate_predictions_univariate!, generate_predictions_bivariate! and generate_predictions_dim_samples!. \n\nThe profiles plotted are based on the specified θs_to_plot, θcombinations_to_plot, θs_to_plot, confidence_levels, dofs, profile_types, methods and sample_types. By default, will plot all predictions generated from profiles with profile_dimension. If for_dim_samples=true then profile-wise trajectory confidence sets will be plotted from profiles sampled using an AbstractSampleType.\n\nlinealpha is the alpha value used for plotting each individual model trajectory line contained within a profile-wise trajectory confidence set. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_predictions_union","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_predictions_union","text":"plot_predictions_union(model::LikelihoodModel,\n    t::AbstractVector,\n    profile_dimension::Int=1,\n    confidence_level::Float64=0.95;\n    dof::Int=profile_dimension,\n    xlabel::String=\"t\",\n    ylabel::Union{Nothing,String,Vector{String}}=nothing,\n    for_dim_samples::Bool=false,\n    include_MLE::Bool=true,\n    θs_to_plot::Vector = Int[],\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    θindices_to_plot::Vector=Vector{Int}[],\n    profile_types::Vector{<:AbstractProfileType}=[LogLikelihood()],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    compare_to_full_sample_type::Union{Missing, AbstractSampleType}=missing,\n    include_lower_confidence_levels::Bool=false,\n    linealpha=0.4,\n    kwargs...)\n\nReturns a plot of the union of profile-wise predictions of the model trajectory formed from profiles with interest parameter dimension profile_dimension that meet the requirement of the relevant method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). \n\nThe plotted extrema are the extrema of the approximate profile-wise confidence_level trajectory confidence set. \n\nt should be the same points used to generate predictions in generate_predictions_univariate!, generate_predictions_bivariate! and generate_predictions_dim_samples!. \n\nThe profiles plotted are based on the specified θs_to_plot, θcombinations_to_plot, θs_to_plot, confidence_levels, dofs, profile_types, methods and sample_types. By default, will plot all predictions generated from profiles with profile_dimension. If for_dim_samples=true then the profile-wise trajectory confidence set will be plotted from profiles sampled using an AbstractSampleType.\n\ninclude_lower_confidence_levels is only relevant for profiles of dimension 2 evaluated using an AbstractBivariateMethod.\n\nIf compare_to_full_sample_type isa AbstractSampleType then will also plot the extrema of the trajectory confidence set from a full parameter confidence set evaluated using the specified AbstractSampleType. For example use compare_to_full_sample_type=LatinHypercubeSamples().\n\nlinealpha is the alpha value used for plotting each individual model trajectory line contained within a profile-wise trajectory confidence set. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_realisations_individual","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_realisations_individual","text":"plot_realisations_individual(model::LikelihoodModel,\n    t::AbstractVector,\n    profile_dimension::Int=1;\n    xlabel::String=\"t\",\n    ylabel::Union{Nothing,String,Vector{String}}=nothing,\n    for_dim_samples::Bool=false\n    include_MLE::Bool=true,\n    θs_to_plot::Vector=Int[],\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    θindices_to_plot::Vector=Vector{Int}[],\n    confidence_levels::Vector{<:Float64}=Float64[],\n    dofs::Vector{<:Int}=Int[],\n    regions::Vector{<:Real}=Float64[],\n    profile_types::Vector{<:AbstractProfileType}=[LogLikelihood()],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    linealpha=0.4, \n    kwargs...)\n\nReturns a vector of plots of profile-wise predictions of the region population reference set formed from profiles with interest parameter dimension profile_dimension that meet the requirement of the relevant method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). \n\nThe plotted extrema are the extrema of the approximate profile-wise (region, confidence_level) reference tolerance set from each profile. \n\nt should be the same points used to generate predictions in generate_predictions_univariate!, generate_predictions_bivariate! and generate_predictions_dim_samples!. \n\nThe profiles plotted are based on the specified θs_to_plot, θcombinations_to_plot, θs_to_plot, confidence_levels, dofs, regions, profile_types, methods and sample_types. By default, will plot all predictions generated from profiles with profile_dimension. If for_dim_samples=true then profile-wise trajectory confidence sets will be plotted from profiles sampled using an AbstractSampleType.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#LikelihoodBasedProfileWiseAnalysis.plot_realisations_union","page":"Plots","title":"LikelihoodBasedProfileWiseAnalysis.plot_realisations_union","text":"plot_realisations_union(model::LikelihoodModel,\n    t::AbstractVector,\n    profile_dimension::Int=1,\n    confidence_level::Float64=0.95;\n    dof::Int=profile_dimension,\n    region::Real=0.95,\n    xlabel::String=\"t\",\n    ylabel::Union{Nothing,String,Vector{String}}=nothing,\n    for_dim_samples::Bool=false,\n    include_MLE::Bool=true,\n    θs_to_plot::Vector = Int[],\n    θcombinations_to_plot::Vector=Tuple{Int,Int}[],\n    θindices_to_plot::Vector=Vector{Int}[],\n    profile_types::Vector{<:AbstractProfileType}=[LogLikelihood()],\n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[],\n    sample_types::Vector{<:AbstractSampleType}=AbstractSampleType[],\n    compare_to_full_sample_type::Union{Missing, AbstractSampleType}=missing,\n    include_lower_confidence_levels::Bool=false,\n    linealpha=0.4,\n    kwargs...)\n\nReturns a plot of the union of profile-wise predictions of the region population reference set formed from profiles with interest parameter dimension profile_dimension that meet the requirement of the relevant method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). \n\nThe plotted extrema are the extrema of the approximate profile-wise (region, confidence_level) reference tolerance set. \n\nt should be the same points used to generate predictions in generate_predictions_univariate!, generate_predictions_bivariate! and generate_predictions_dim_samples!. \n\nThe profiles plotted are based on the specified θs_to_plot, θcombinations_to_plot, θs_to_plot, confidence_levels, dofs, regions, profile_types, methods and sample_types. By default, will plot all predictions generated from profiles with profile_dimension. If for_dim_samples=true then the profile-wise reference tolerancce set will be plotted from profiles sampled using an AbstractSampleType.\n\ninclude_lower_confidence_levels is only relevant for profiles of dimension 2 evaluated using an AbstractBivariateMethod.\n\nIf compare_to_full_sample_type isa AbstractSampleType then will also plot the extrema of the reference tolerance set from a full parameter confidence set evaluated using the specified AbstractSampleType. For example use compare_to_full_sample_type=LatinHypercubeSamples().\n\n\n\n\n\n","category":"function"},{"location":"user_interface/plots/#Index","page":"Plots","title":"Index","text":"","category":"section"},{"location":"user_interface/plots/","page":"Plots","title":"Plots","text":"Pages = [\"plots.md\"]","category":"page"},{"location":"internal_library/predictions/#Prediction-Functions","page":"Prediction Functions","title":"Prediction Functions","text":"","category":"section"},{"location":"internal_library/predictions/","page":"Prediction Functions","title":"Prediction Functions","text":"Pages = [\"predictions.md\"]","category":"page"},{"location":"internal_library/predictions/","page":"Prediction Functions","title":"Prediction Functions","text":"LikelihoodBasedProfileWiseAnalysis.generate_prediction\nLikelihoodBasedProfileWiseAnalysis.generate_prediction_univariate\nLikelihoodBasedProfileWiseAnalysis.generate_prediction_bivariate\nLikelihoodBasedProfileWiseAnalysis.predict_realisations","category":"page"},{"location":"internal_library/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_prediction","page":"Prediction Functions","title":"LikelihoodBasedProfileWiseAnalysis.generate_prediction","text":"generate_prediction(predictfunction::Function,\n    errorfunction::Function,\n    data,\n    t::AbstractVector,\n    data_ymle::AbstractArray{<:Real},\n    parameter_points::Matrix{Float64},\n    proportion_to_keep::Real,\n    region::Real,\n    channel::Union{RemoteChannel,Missing}=missing)\n\nGenerates the predictions for response variables from a predictfunction which meets the requirements specified in add_prediction_function!, given data, at time points t for each parameter combination in the columns of parameter_points. The extrema of all predictions is computed and proportion_to_keep of the individual predictions are kept. errorfunction is used to predict the lower and upper quartiles of realisations at each prediction point; the highest density region is returned. In particular it's predicting the region populuation reference interval at each prediction point. When computed from a parameter confidence set like a profile, the extrema gives the extrema of the profile-wise (region, confidence_level) reference tolerance set.\n\nReturns a [PredictionStruct] containing the kept predictions, prediction extrema, lower and upper quartiles of realisations from the error model of region at each predicted point and the realisation extrema. \n\nThe prediction at each timepoint is stored in the corresponding row (1st dimension). The prediction for each parameter combination is stored in the corresponding column (2nd dimension). The prediction for multiple response variables is stored in the 3rd dimension.\n\n\n\n\n\ngenerate_prediction(predictfunction::Function,\n    errorfunction::Missing,\n    data,\n    t::AbstractVector,\n    data_ymle::AbstractArray{<:Real},\n    parameter_points::Matrix{Float64},\n    proportion_to_keep::Real,\n    region::Real,\n    channel::Union{RemoteChannel,Missing}=missing)\n\nGenerates the predictions for response variables from a predictfunction which meets the requirements specified in add_prediction_function!, given data, at time points t for each parameter combination in the columns of parameter_points. The extrema of all predictions is computed and proportion_to_keep of the individual predictions are kept.\n\nReturns a [PredictionStruct] containing the kept predictions and prediction extrema. \n\nThe prediction at each timepoint is stored in the corresponding row (1st dimension). The prediction for each parameter combination is stored in the corresponding column (2nd dimension). The prediction for multiple response variables is stored in the 3rd dimension.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_prediction_univariate","page":"Prediction Functions","title":"LikelihoodBasedProfileWiseAnalysis.generate_prediction_univariate","text":"generate_prediction_univariate(model::LikelihoodModel,\n    errorfunction::Union{Function, Missing},\n    sub_df,\n    row_i::Int,\n    t::AbstractVector,\n    proportion_to_keep::Real,  \n    channel::RemoteChannel)\n\nGenerates predictions for the univariate profile in sub_df that corresponds to row_i at timepoints t.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_prediction_bivariate","page":"Prediction Functions","title":"LikelihoodBasedProfileWiseAnalysis.generate_prediction_bivariate","text":"generate_prediction_bivariate(model::LikelihoodModel,\n    sub_df,\n    row_i::Int,\n    t::AbstractVector,\n    proportion_to_keep::Real, \n    channel::RemoteChannel)\n\nGenerates predictions for the bivariate profile in sub_df that corresponds to row_i at timepoints t.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/predictions/#LikelihoodBasedProfileWiseAnalysis.predict_realisations","page":"Prediction Functions","title":"LikelihoodBasedProfileWiseAnalysis.predict_realisations","text":"predict_realisations(errorfunction::Function, \n    predictions::AbstractArray, \n    θ::AbstractVector,\n    region::Float64)\n\nUses errorfunction to make prediction for realisations; it forms region reference intervals for region population reference intervals. Returns two arrays of the same dimension as predictions, one containing the lower quantile and one containing the upper quantile (the extrema) of the reference interval.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#Initialisation","page":"Initialisation","title":"Initialisation","text":"","category":"section"},{"location":"user_interface/initialisation/#Model-Initialisation","page":"Initialisation","title":"Model Initialisation","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"To initialise a model for profile likelihood evaluation we use initialise_LikelihoodModel which returns a struct of type LikelihoodModel. This struct contains all the information we require for the PWA workflow and will also contain computed profiles and profile-wise predictions.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"initialise_LikelihoodModel","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.initialise_LikelihoodModel","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.initialise_LikelihoodModel","text":"initialise_LikelihoodModel(loglikefunction::Function,\n    predictfunction::Union{Function, Missing},\n    errorfunction::Union{Function, Missing}\n    data::Union{Tuple, NamedTuple},\n    θnams::Vector{<:Symbol},\n    θinitialguess::AbstractVector{<:Real},\n    θlb::AbstractVector{<:Real},\n    θub::AbstractVector{<:Real},\n    θmagnitudes::AbstractVector{<:Real}=Float64[];\n    <keyword arguments>)\n\nInitialises a LikelihoodModel struct, which contains all model information, profiles, samples and predictions. Solves for the maximum likelihood estimate of loglikefunction.\n\nArguments\n\nloglikefunction: a log-likelihood function to maximise which takes two arguments, θ and data, in that order, where θ is a vector containing the values of each parameter in θnames and data is a Tuple or NamedTuple - see data below. Set up to be used in a maximisation objective.\npredictfunction: a prediction function to generate model predictions from that is paired with the loglikefunction. Requirements for the prediction function can be seen in add_prediction_function!. It can also be missing if no function is provided to initialise_LikelihoodModel, because predictions are not required when evaluating parameter profiles. The function can be added at a later point using add_prediction_function!.\nerrorfunction: an error function used to predict realisations from predictions generated with predictfunction. Requirements for the error function can be seen in add_error_function!. It can also be missing if no function is provided to initialise_LikelihoodModel, because predictions are not required when evaluating parameter profiles. The function can be added at a later point using add_error_function!.\ndata: a Tuple or a NamedTuple containing any additional information required by the log-likelihood function, such as the time points to be evaluated at.\nθnames: a vector of symbols containing the names of each parameter, e.g. [:λ, :K, :C0].\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point.\nθlb: a vector of lower bounds on parameters. \nθub: a vector of upper bounds on parameters. \nθmagnitudes: a vector of the relative magnitude of each parameter. If not provided, it will be estimated using the difference of θlb and θub with LikelihoodBasedProfileWiseAnalysis.calculate_θmagnitudes. Can be updated after initialisation using setmagnitudes!.\n\nKeyword Arguments\n\noptimizationsettings: optimization settings used to optimize the log-likelihood function using Optimization.jl. Default is default_OptimizationSettings() (see default_OptimizationSettings).\nuni_row_prealloaction_size: number of rows of uni_profiles_df to preallocate. Default is missing (a single row).\nbiv_row_preallocation_size: number of rows of biv_profiles_df to preallocate. Default is missing (a single row).\ndim_row_preallocation_size: number of rows of dim_samples_df to preallocate. Default is missing (a single row).\nfind_zero_atol: a Real number greater than zero for the absolute tolerance of the log-likelihood function value from the target value to be used when searching for confidence intervals/boundaries. Default is 0.001.\nshow_progress: Whether to show the progress of profiling and predictions. \n\nnote: Array initialisation within a log-likelihood function\nIf you initialise an array within the provided log-likelihood function, e.g. using zeros, then for automatic differentiation methods to work you need to also initialise the type of the array to be based on the type of the input values of θ. Otherwise, zeros will by default create an array with element types Float64 which will likely return errors. For example, use:my_new_array = zeros(eltype(θ), dimensions)where eltype passes the element type of the θ vector into the zeros function.\n\n\n\n\n\ninitialise_LikelihoodModel(loglikefunction::Function,\n    predictfunction::Function,\n    data::Union{Tuple, NamedTuple},\n    θnames::Vector{<:Symbol},\n    θinitialGuess::Vector{<:Float64},\n    θlb::Vector{<:Float64},\n    θub::Vector{<:Float64},\n    θmagnitudes::Vector{<:Real}=zeros(0);\n    <keyword arguments>)\n\nAlternate version of initialise_LikelihoodModel that can be called without a error function. The function can be added at a later point using add_error_function!.\n\n\n\n\n\ninitialise_LikelihoodModel(loglikefunction::Function,\n    data::Union{Tuple, NamedTuple},\n    θnames::Vector{<:Symbol},\n    θinitialGuess::Vector{<:Float64},\n    θlb::Vector{<:Float64},\n    θub::Vector{<:Float64},\n    θmagnitudes::Vector{<:Real}=zeros(0);\n    <keyword arguments>)\n\nAlternate version of initialise_LikelihoodModel that can be called without a prediction and error function. The functions can be added at a later point using add_prediction_function! and add_error_function!.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#Model-Representation","page":"Initialisation","title":"Model Representation","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"LikelihoodModel\nCoreLikelihoodModel\nBaseLikelihoodModel","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.LikelihoodModel","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.LikelihoodModel","text":"LikelihoodModel(core::Union{CoreLikelihoodModel, BaseLikelihoodModel}, \n    ellipse_MLE_approx::Union{Missing, EllipseMLEApprox},\n    num_uni_profiles::Int, \n    num_biv_profiles::Int, \n    num_dim_samples::Int, \n    uni_profiles_df::DataFrame, \n    biv_profiles_df::DataFrame, \n    dim_samples_df::DataFrame, \n    uni_profile_row_exists::Dict{Tuple{Int, AbstractProfileType}, DefaultDict{Float64, Int}},\n    biv_profile_row_exists::Dict{Tuple{Tuple{Int, Int}, AbstractProfileType, AbstractBivariateMethod}, DefaultDict{Float64, Int}},\n    dim_samples_row_exists::Dict{Union{AbstractSampleType, Tuple{Vector{Int}, AbstractSampleType}}, DefaultDict{Float64, Int}},\n    uni_profiles_dict::Dict{Int, UnivariateConfidenceStruct}, \n    biv_profiles_dict::Dict{Int, BivariateConfidenceStruct}, \n    dim_samples_dict::Dict{Int, SampledConfidenceStruct},\n    uni_predictions_dict::Dict{Int, AbstractPredictionStruct}, \n    biv_predictions_dict::Dict{Int, AbstractPredictionStruct}, \n    dim_predictions_dict::Dict{Int, AbstractPredictionStruct},\n    show_progress::Bool)\n\nMutable struct containing all the information required to compute profiles, samples and predictions. Created by initialise_LikelihoodModel.\n\nFields\n\ncore: a CoreLikelihoodModel or BaseLikelihoodModel struct.\nellipse_MLE_approx: a EllipseMLEApprox struct OR a missing value if the ellipse approximation of the log-likelihood at the MLE point has not been evaluated yet. \nnum_uni_profiles: the number of different univariate profiles that have been evaluated (distinct combinations of different confidence levels, AbstractProfileType structs and single interest parameters). Specifies the number of valid rows in uni_profiles_df.  \nnum_biv_profiles: the number of different bivariate profiles that have been evaluated (distinct combinations of different confidence levels, AbstractProfileType structs, AbstractBivariateMethod structs and two interest parameters). Specifies the number of valid rows in biv_profiles_df.  \nnum_dim_samples: the number of different dimensional profiles that have been evaluated (distinct combinations of different confidence levels, AbstractProfileType structs, AbstractSampleType structs and sets of interest parameters). Specifies the number of valid rows in dim_samples_df.  \nuni_profiles_df: a DataFrame with each row containing information on each univariate profile evaluated, where the row index is the key for that profile in uni_profiles_dict and uni_predictions_dict.\nbiv_profiles_df: a DataFrame with each row containing information on each bivariate profile evaluated, where the row index is the key for that profile in biv_profiles_dict and biv_predictions_dict.\ndim_samples_df: a DataFrame with each row containing information on each dimensional sample evaluated, where the row index is the key for that sample in dim_samples_dict and dim_predictions_dict.\nuni_profile_row_exists: a dictionary containing information on whether a row in uni_profiles_df exists for a given combination of interest parameter, degrees of freedom, AbstractProfileType and confidence level. If it does exist, the value of uni_profile_row_exists[(θi, dof, profile_type)][confidence_level] will be the row index in uni_profiles_df, otherwise it will be 0.\nbiv_profile_row_exists: a dictionary containing information on whether a row in biv_profiles_df exists for a given combination of two interest parameters, AbstractProfileType, AbstractBivariateMethod and confidence level. If it does exist, the value of biv_profile_row_exists[((θi, θj), dof, profile_type, bivariate_method)][confidence_level] will be the row index in biv_profiles_df otherwise it will be 0.\ndim_samples_row_exists: a dictionary containing information on whether a row in dim_samples_df exists for a given combination of interest parameter, AbstractProfileType, AbstractSampleType and confidence level. If it does exist, it's value will be the row index in dim_samples_df otherwise it will be 0. For a full likelihood sample this can be accessed using dim_samples_row_exists[sample_type][confidence_level]. For a lower dimensional sample this can be accessed using dim_samples_row_exists[(θindices, dof, sample_type)][confidence_level].\nuni_profiles_dict: a dictionary with keys of type Integer and values of type UnivariateConfidenceStruct containing the profile for each valid row in uni_profiles_df. The row index of uni_profiles_df is the key for the corresponding profile.\nbiv_profiles_dict: a dictionary with keys of type Integer and values of type BivariateConfidenceStruct containing the profile for each valid row in biv_profiles_df. The row index of biv_profiles_df is the key for the corresponding profile.\ndim_samples_dict: a dictionary with keys of type Integer and values of type SampledConfidenceStruct containing the profile for each valid row in dim_samples_df. The row index of dim_samples_df is the key for the corresponding profile.\nuni_predictions_dict: a dictionary with keys of type Integer and values of type PredictionStruct containing the predictions from the profiles in uni_profiles_dict for each valid row in uni_profiles_df. The row index of uni_profiles_df is the key for the corresponding prediction, if that prediction has been calculated using generate_predictions_univariate!. \nbiv_predictions_dict: a dictionary with keys of type Integer and values of type PredictionStruct containing the predictions from the profiles in biv_profiles_dict for each valid row in biv_profiles_df. The row index of biv_profiles_df is the key for the corresponding prediction, if that prediction has been calculated using generate_predictions_bivariate!. \ndim_predictions_dict: a dictionary with keys of type Integer and values of type PredictionStruct containing the predictions from the profiles in dim_samples_dict for each valid row in dim_samples_df. The row index of dim_samples_df is the key for the corresponding prediction, if that prediction has been calculated using generate_predictions_dim_samples!. \nfind_zero_atol: a Real number greater than zero for the absolute tolerance of the log-likelihood function value from the target value to be used when searching for confidence intervals/boundaries.\nshow_progress: a boolean specifying whether to show the progress of profile methods with respect to sets of interest parameter(s).\n\nSupertype Hiearachy\n\nLikelihoodModel <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.CoreLikelihoodModel","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.CoreLikelihoodModel","text":"CoreLikelihoodModel(loglikefunction::Function, \n    predictfunction::Union{Function, Missing}, \n    errorfunction::Union{Function, Missing}\n    data::Union{Tuple, NamedTuple}, \n    θnames::AbstractVector\n    θname_to_index::Dict{Symbol, Int}, \n    θlb::AbstractVector{<:Real}, \n    θub::AbstractVector{<:Real}, \n    θmagnitudes::AbstractVector{<:Real}, \n    θmle::Vector{<:Float64}, \n    ymle::Array{<:Real}, \n    maximisedmle::Float64, \n    num_pars::Int)\n\nStruct containing the core information required to define a LikelihoodModel. For additional information on parameters (where repeated), see initialise_LikelihoodModel.\n\nFields\n\nloglikefunction: a log-likelihood function which takes two arguments, θ and data, in that order. Set up to be used in a maximisation objective.\npredictfunction: a prediction function to generate model predictions from that is paired with the loglikefunction. \nerrorfunction: an error function used to predict realisations from predictions generated with predictfunction. \noptimizationsettings: a OptimizationSettings struct of settings to use for optimisation unless others are specified.\ndata: a Tuple or a NamedTuple containing any additional information required by the log-likelihood function, such as the time points to be evaluated at.\nθnames: a vector of symbols containing the names of each parameter, e.g. [:λ, :K, :C0].\nθname_to_index: a dictionary with keys of type Symbol and values of type Int, with the key being an element of θnames and the value being the corresponding index of the key in θnames.\nθlb: a vector of lower bounds on parameters. \nθub: a vector of upper bounds on parameters. \nθmagnitudes: a vector of the relative magnitude of each parameter. \nθmle: a vector containing the maximum likelihood estimate for each parameter.\nymle: an array containing the output of the prediction function at θmle and data.\nmaximisedmle: the value of the log-likelihood function evaluated at θmle.\nnum_pars: the number of model parameters, length(θnames).\n\nSupertype Hiearachy\n\nCoreLikelihoodModel <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.BaseLikelihoodModel","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.BaseLikelihoodModel","text":"BaseLikelihoodModel(data::Union{Tuple, NamedTuple}, \n    θnames::AbstractVector,\n    θname_to_index::Dict{Symbol, Int}, \n    θlb::AbstractVector{<:Real}, \n    θub::AbstractVector{<:Real}, \n    θmagnitudes::AbstractVector{<:Real}, \n    θmle::Vector{<:Float64}, \n    ymle::Array{<:Real}, \n    maximisedmle::Float64, \n    num_pars::Int)\n\nVersion of CoreLikelihoodModel without functions, allowing loading of the model struct without those functions defined.\n\nSupertype Hiearachy\n\nBaseLikelihoodModel <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/initialisation/#Ellipse-Approximation","page":"Initialisation","title":"Ellipse Approximation","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"In order to evaluate the ellipse approximation of the normalised log-likelihood function we need to evaluate the observed Fisher information matrix (FIM) [2]. The observed FIM is a quadratic approximation of the curvature of the log-likelihood function at the maximum likelihood estimate (MLE) for parameters. This then allows us to define the ellipse approximation of the log-likelihood function, represented as the EllipseApproxAnalytical and EllipseApprox profile types. These have corresponding equations to evaluate their approximation to the log-likelihood function, as given by LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike and LikelihoodBasedProfileWiseAnalysis.ellipse_loglike, respectively.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"getMLE_ellipse_approximation!\ncheck_ellipse_approx_exists!\nEllipseMLEApprox","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.getMLE_ellipse_approximation!","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.getMLE_ellipse_approximation!","text":"getMLE_ellipse_approximation!(model::LikelihoodModel)\n\nCreates the ellipse approximation of the log-likelihood function at the maximum likelihood estimate, modifying model in place, computing the negative hessian of the log-likelihood function and it's pseudoinverse using getMLE_hessian_and_covariance. These matrices are stored as a EllipseMLEApprox struct within model at model.ellipse_MLE_approx.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.check_ellipse_approx_exists!","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.check_ellipse_approx_exists!","text":"check_ellipse_approx_exists!(model::LikelihoodModel)\n\nChecks if the ellipse approximation at the maximum likelihood estimate has been created and if not creates it using getMLE_ellipse_approximation!, modifying model in place.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.EllipseMLEApprox","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.EllipseMLEApprox","text":"EllipseMLEApprox(Hmle::Matrix{<:Float64}, Γmle::Matrix{<:Float64})\n\nStruct containing two n*n arrays representing the ellipse approximation of the log-likelihood function around the MLE point. See getMLE_ellipse_approximation!\n\nFields\n\nHmle: a n*n array, where n is the number of model parameters, containing the negative Hessian of the log-likelihood function evaluated at the MLE point. \nΓmle: a n*n array, where n is the number of model parameters, containing the inverse of Hmle.\n\nSupertype Hiearachy\n\nEllipseMLEApprox <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/initialisation/#Modifying-Parameter-Magnitudes-and-Bounds","page":"Initialisation","title":"Modifying Parameter Magnitudes and Bounds","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"If desired, the defined parameter magnitudes and bounds contained with a LikelihoodModel can be updated using setmagnitudes! and setbounds!.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"setmagnitudes!\nsetbounds!","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.setmagnitudes!","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.setmagnitudes!","text":"setmagnitudes!(model::LikelihoodModel, θmagnitudes::AbstractVector{<:Real})\n\nUpdates the magnitudes of each parameter in model from model.core.θmagnitudes to θmagnitudes.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.setbounds!","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.setbounds!","text":"setbounds!(model::LikelihoodModel; \n    lb::AbstractVector{<:Real}=Float64[], \n    ub::AbstractVector{<:Real}=Float64[])\n\nUpdates the parameter bounds in model from model.core.θlb to lb if specified and from model.core.θub to ub if specified. lb and ub are keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#Parameter-Transformations","page":"Initialisation","title":"Parameter Transformations","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"To assist with parameter transformations we provide functions for transforming parameter bounds given a monotonic forward mapping from the original parameter space to the transformed parameter space.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"transformbounds\ntransformbounds_NLopt","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.transformbounds","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.transformbounds","text":"transformbounds(transformfun::Function, \n    lb::AbstractVector{<:Real}, \n    ub::AbstractVector{<:Real}, \n    independentParameterIndexes::Vector{<:Int}=Int[], \n    dependentParameterIndexes::Vector{<:Int}=Int[])\n\nGiven a monotonic (increasing or decreasing) function, transformfun, that describes a parameter transformation, return the lower and upper bounds in the transformed space that correspond to the lower and upper bounds in the original space. Uses a heuristic to evaluate the bound transformation. We assume that the ordering of parameters stay the same for the purposes of independentParameterIndexes and dependentParameterIndexes.\n\nArguments\n\ntransformfun: a function describing the forward transformation between parameter space θ and Θ. Should take in a single argument, θ, a vector of parameter values in the original space and return Θ, a vector parameter values in the transformed space. These vectors need to be the same length as lb and ub.\nlb: a vector of lower bounds on parameters. \nub: a vector of upper bounds on parameters. \nindependentParameterIndexes: a vector of parameter indexes where the new parameter Θ[i] depends only on transformfun(θ[i]).\ndependentParameterIndexes: a vector of parameter indexes where the new parameter Θ[i] depends on transformfun(θ[i], θ[j], j!=i).\n\nThe heuristic used for dependent parameters may fail if there are multiple local minima for the appropriate bounds to use. In this case transformbounds_NLopt should be used.\n\nWarns if any of the returned bounds are Inf using LikelihoodBasedProfileWiseAnalysis.checkforInf.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.transformbounds_NLopt","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.transformbounds_NLopt","text":"transformbounds_NLopt(transformfun::Function, \n    lb::AbstractVector{<:Real}, \n    ub::AbstractVector{<:Real})\n\nGiven a monotonic (increasing or decreasing) function, transformfun, that describes a parameter transformation, return the lower and upper bounds in the transformed space that correspond to the lower and upper bounds in the original space. Uses a naturally binary integer programme if transformfun is monotonic on θ between lb and ub.\n\nArguments\n\ntransformfun: a function describing the forward transformation between parameter space θ and Θ. Should take in a single argument, θ, a vector of parameter values in the original space and return Θ, a vector parameter values in the transformed space. These vectors need to be the same length as lb and ub.\nlb: a vector of lower bounds on parameters. \nub: a vector of upper bounds on parameters. \n\nWarns if any of the returned bounds are Inf using LikelihoodBasedProfileWiseAnalysis.checkforInf.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#Example-Usage","page":"Initialisation","title":"Example Usage","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"For example, if we want to log transform the parameterisation given a defined log-likelihood function and lower and upper bounds we:","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"Calculate the new, log-transformed bounds using the forward transformation log.(θ). This is easy enough to be done without transformbounds_NLopt, but other more complex transformations may not be.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"lb, ub = ..., ...\n\nf(x) = log.(x)\nlb_log, ub_log = transformbounds_NLopt(f, lb, ub)","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"Define a new log-likelihood function which uses the backward transformation exp.(θ) from the transformed parameter space to the original space.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"function loglike(θ, data)\n    ...\nend\n\nfunction loglike_log(Θ, data)\n    return loglike(exp.(Θ), data)\nend","category":"page"},{"location":"user_interface/initialisation/#Optimization-Settings","page":"Initialisation","title":"Optimization Settings","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"We can set our default optimisation settings using a OptimizationSettings struct. This will be contained within the CoreLikelihoodModel field of a LikelihoodModel and can be passed as an option to initialise_LikelihoodModel. Unless different ones are passed to functions for computing profiles, they will also be used for that purpose. It may be useful to compute the MLE using conservative settings for accuracy, and then use less conservative settings for the optimisation of nuisance parameters along parameter profiles.","category":"page"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"default_OptimizationSettings\ncreate_OptimizationSettings\nset_OptimizationSettings!\nOptimizationSettings\noptimise\noptimise_unbounded","category":"page"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.default_OptimizationSettings","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.default_OptimizationSettings","text":"default_OptimizationSettings()\n\nCreates a OptimizationSettings struct with defaults of:\n\nadtype: SciMLBase.NoAD() (no automatic differentiation). \nsolve_alg: NLopt.LN_BOBYQA().\nsolve_kwargs: (maxtime=15, xtol_rel=1e-12).\n\nIf this function causes an error then LikelihoodBasedProfileWiseAnalysis needs to be loaded. Alternatively, the packages SciMLBase, Optimization and OptimizationNLopt need to be loaded.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.create_OptimizationSettings","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.create_OptimizationSettings","text":"create_OptimizationSettings(model::LikelihoodModel;\n    adtype::Union{SciMLBase.AbstractADType, Missing}=missing, \n    solve_alg=missing, \n    solve_kwargs::Union{NamedTuple, Missing}=missing)\n\nMethod for creating a OptimizationSettings struct with each field of the struct as a keyword argument. If a keyword argument is not provided, then the setting in model.core.optimizationsettings is used (the currently set optimization settings).\n\n\n\n\n\ncreate_OptimizationSettings(;\n    adtype::Union{SciMLBase.AbstractADType, Missing}=missing, \n    solve_alg=missing, \n    solve_kwargs::Union{NamedTuple, Missing}=missing)\n\nMethod for creating a OptimizationSettings struct with each field of the struct as a keyword argument. If a keyword argument is not provided, then the default setting in default_OptimizationSettings is used.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.set_OptimizationSettings!","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.set_OptimizationSettings!","text":"set_OptimizationSettings!(model::LikelihoodModel, optimizationsettings::OptimizationSettings)\n\nUpdates the optimization settings contained with model with optimizationsettings.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.OptimizationSettings","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.OptimizationSettings","text":"OptimizationSettings(adtype::SciMLBase.AbstractADType,\n    solve_alg,\n    solve_kwargs::NamedTuple)\n\nContains the optimization settings used to solve for nuisance parameters of the log-likelihood function using Optimization.jl. Note: Optimization.jl uses a minimisation objective, while the log-likelihood functions are setup with a maximisation objective in mind. This is handled inside the package. Defaults can be seen in default_OptimizationSettings.\n\nFields\n\nadtype: a method for automatically generating derivative functions of the log-likelihood function being optimised. For available settings see Automatic Differentiation Construction Choice Recommendations. Note: the corresponding package to adtype needs to be loaded with using. e.g. setting adtype = AutoFiniteDiff() requires using FiniteDiff. Derivative-based algorithms in solve_alg will require an adtype to be specified. \nAutoFiniteDiff(), AutoForwardDiff(), AutoReverseDiff(), AutoZygote() and AutoTracker() have been tested to work for finding the maximum likelihood estimate with regular model parameters. \nFor profiling, AutoFiniteDiff(), AutoForwardDiff(), AutoReverseDiff() and AutoTracker() have been tested to work with regular model parameters. \nIf the variance of the error distribution is included as a parameter to be estimated, AutoFiniteDiff(), AutoForwardDiff() and AutoTracker() have been tested to work for finding the MLE and profiling. \nAutoFiniteDiff() will always work, regardless of model specification, but may be less optimal than other methods.\nsolve_alg: an algorithm to use to solve for the nuisance parameters of the log-likelihood function defined within Optimization.jl. The package is loaded with the Optimization integration of NLopt, so any of the NLopt algorithms are available without having to load another package (see OptimizationNLopt). Good starting methods may be NLopt.LN_BOBYQA(), NLopt.LN_NELDERMEAD() and NLopt.LD_LBFGS. Other packages can be used as well - see Overview of the Optimizers.\nsolve_kwargs: a NamedTuple of keyword arguments used to set solver options like maxiters and maxtime. For a list of common solver arguments see: Common Solver Options. Other specific package arguments may also be available. For NLopt see Methods.\n\nSupertype Hiearachy\n\nOptimizationSettings <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.optimise","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.optimise","text":"optimise(fun, q, options::OptimizationSettings, θ₀, lb, ub)\n\nOptimization.jl optimiser used for calculating the values of nuisance parameters. Default values of options use NLopt.jl algorithms (seedefault_OptimizationSettings).\n\n\n\n\n\noptimise(fun, options::OptimizationSettings, θ₀, lb, ub)\n\nOptimization.jl optimiser used for calculating the bound transformations. Default values of options use NLopt.jl algorithms (seedefault_OptimizationSettings).\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#LikelihoodBasedProfileWiseAnalysis.optimise_unbounded","page":"Initialisation","title":"LikelihoodBasedProfileWiseAnalysis.optimise_unbounded","text":"optimise_unbounded(fun, q, options::OptimizationSettings, θ₀)\n\nAlternative version of optimise without nuisance parameter bounds. Used for computing the nuisance parameters of EllipseApproxAnalytical profiles. Default values of options use NLopt.jl algorithms (seedefault_OptimizationSettings).\n\n\n\n\n\n","category":"function"},{"location":"user_interface/initialisation/#Index","page":"Initialisation","title":"Index","text":"","category":"section"},{"location":"user_interface/initialisation/","page":"Initialisation","title":"Initialisation","text":"Pages = [\"initialisation.md\"]","category":"page"},{"location":"examples/#Initial-Setup","page":"Initial Setup","title":"Initial Setup","text":"","category":"section"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"Here we go through a series of examples that show the use of this package on various models.","category":"page"},{"location":"examples/#Import-Package-and-Set-Up-Distributed-Environment","page":"Initial Setup","title":"Import Package and Set Up Distributed Environment","text":"","category":"section"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"This package is designed to be run in a distributed computing environment (see Distributed.jl), meaning that the evaluation of profiles can be evaluated in parallel. This is particularly important when running coverage simulations. Many of the methods also work using a multi-threaded approach, albeit often parallised in different locations","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"tip: Recommended number of workers\nThe number of workers to use is recommended to be one or two less than the number of local threads (typically double the number of cores on a given CPU) on your CPU. This is because 1 worker already exists as the master worker and for most efficient distributed computing we want each worker to work on a distinct CPU thread. Furthermore, if you wish to use your computer at the same time as a simulation is running, we recommend that at least two threads are left available for other tasks.","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"We first allocate a number of workers for our simulation, which each use the minimum amount of threads (appears to be 2-3 when testing). If we don't set the JULIA_NUM_THREADS environment variable to 1 then every worker will be initialised with the default number of threads, Threads.nthreads(), which can cause large slowdowns.","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"using Distributed\naddprocs(Threads.nthreads()-2, env=[\"JULIA_NUM_THREADS\"=>\"1\"]) # the number of parallel workers to use - on the author's system Threads.nthreads() returns 10\n# addprocs(8, env=[\"JULIA_NUM_THREADS\"=>\"1\"]) # add 8 workers - total number of workers will be 9","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"We can now check how many workers are allocated.","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"nprocs() # use to check the total number of worker processes allocated\nworkers() # the ids of the allocated workers\n# Threads.nthreads() # the number of execution threads allocated to Julia at startup","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"We import our package with the annotation @everywhere, letting Julia know that we wish to load the package on all allocated workers.","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"warning: Use of `@everywhere`\nIf @everywhere is not used at the beginning of the package import, then the package LikelihoodBasedProfileWiseAnalysis is only loaded onto the master worker's environment and cannot be seen by the other workers we wish to parallelise the simulation on. Similarly, any other functions that need to be loaded onto all workers, such as those that define the log-likelihood function, also need to have the annotation @everywhere. Resultantly, Julia will throw an error if  nprocs()>1. However, an error won't be thrown if we haven't added any worker processes (if nprocs()==1) using the above multiprocessing arguments (this package can run on a single thread).","category":"page"},{"location":"examples/","page":"Initial Setup","title":"Initial Setup","text":"@everywhere using LikelihoodBasedProfileWiseAnalysis","category":"page"},{"location":"user_interface/profiles_and_samples/profile_structs/#Structs-and-Profile-Types","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/profile_structs/#Structs","page":"Structs and Profile Types","title":"Structs","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/profile_structs/","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"The following structs are used to contain information on computed univariate, bivariate and sampled dimensional profiles. After evaluation they are stored in the LikelihoodModel.","category":"page"},{"location":"user_interface/profiles_and_samples/profile_structs/","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"PointsAndLogLikelihood\nAbstractConfidenceStruct\nUnivariateConfidenceStruct\nBivariateConfidenceStruct\nSampledConfidenceStruct","category":"page"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.PointsAndLogLikelihood","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.PointsAndLogLikelihood","text":"PointsAndLogLikelihood(points::Array{Float64}, \n    ll::Vector{<:Float64}, \n    boundary_col_indices::Vector{<:Int64}=zeros(Int, 0)))\n\nStruct that stores an array of parameter points, their corresponding log-likelihood value and, in the case of univariate profiles, the column indices in points of the confidence interval parameters.\n\nFields\n\npoints: an array of points stored in columns, with each row corresponding to the respective index of each model parameter. For the UnivariateConfidenceStruct type, these points are stored in column-wise order of increasing interest parameter magnitude. For the BivariateConfidenceStruct type these points are stored in the order they are found. \nll: a vector of log-likelihood function values corresponding to the point in each column of points. This number is standardised so that regardless of whether the true log-likelihood function or an ellipse approximation of the function is evaluated, the value of the MLE point is 0.0. \nboundary_col_indices: a vector that is empty for the BivariateConfidenceStruct type and of length two for the UnivariateConfidenceStruct type. Contains the column indices in points of the confidence interval parameters for the UnivariateConfidenceStruct type. Default is an empty vector.\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.AbstractConfidenceStruct","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.AbstractConfidenceStruct","text":"AbstractConfidenceStruct\n\nSupertype for confidence boundary storage structs.\n\nSubtypes\n\nUnivariateConfidenceStruct\n\nBivariateConfidenceStruct\n\nSampledConfidenceStruct\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.UnivariateConfidenceStruct","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.UnivariateConfidenceStruct","text":"UnivariateConfidenceStruct(confidence_interval::Vector{<:Float64}, \n    interval_points::PointsAndLogLikelihood)\n\nStruct that stores the confidence interval of a given interest parameter as well as points sampled inside (and outside) the confidence interval and their corresponding log-likelihood values.\n\nFields\n\nconfidence_interval: a vector of length two with the confidence interval for a given interest parameter. If an entry has value NaN, that side of the confidence interval is outside the corresponding bound on the interest parameter.\ninterval_points: a PointsAndLogLikelihood struct containing any points that have been evaluated inside or outside the interval by get_points_in_intervals!, their corresponding log-likelihood function value and the column indices of the confidence_interval points in interval_points.points. Points can be evaluated and stored that are outside the confidence interval so that log-likelihood profile plots are defined outside of the confidence interval. interval_points.points is stored in column-wise order of increasing interest parameter magnitude. \n\nSupertype Hiearachy\n\nUnivariateConfidenceStruct <: AbstractConfidenceStruct <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.BivariateConfidenceStruct","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.BivariateConfidenceStruct","text":"BivariateConfidenceStruct(confidence_boundary::Matrix{Float64}, \n    internal_points::PointsAndLogLikelihood = PointsAndLogLikelihood(zeros(size(x, 1), 0), zeros(0)))\n\nStruct that stores samples produced by an AbstractBivariateMethod that are on the bivariate confidence boundary of two interest parameters at a given confidence level and, if save_internal_points=true, any internal points found during the method with their corresponding log-likelihood values. Use bivariate_methods() for a list of available methods (see bivariate_methods).\n\nFields\n\nconfidence_boundary: an array of boundary points stored in columns, with each row corresponding to the respective index of each model parameter. This array can contain points that are inside the bivariate confidence boundary if the method being used brackets between an internal point and a point on the user-provided bounds: these points will be on a user-provided parameter bound.\ninternal_points: a PointsAndLogLikelihood struct containing points and their corresponding log-likelihood values that were found during a method, if save_internal_points=true. Default is an empty PointsAndLogLikelihood struct (used if save_internal_points=false).\n\nSupertype Hiearachy\n\nBivariateConfidenceStruct <: AbstractConfidenceStruct <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.SampledConfidenceStruct","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.SampledConfidenceStruct","text":"SampledConfidenceStruct(points::Array{Float64}, \n    ll::Vector{<:Float64})\n\nStruct that stores samples produced by an AbstractSampleType that are within the confidence boundary of sample_dimension interest parameters at a given confidence level, with their corresponding log-likelihood values.\n\nFields\n\npoints: an array of points stored in columns, with each row corresponding to the respective index of each model parameter. \nll: a vector of log-likelihood function values corresponding to the point in each column of points. This number is standardised so that regardless of whether the true log-likelihood function or an ellipse approximation of the function is evaluated, the value of the MLE point is 0.0. \n\nSupertype Hiearachy\n\nSampledConfidenceStruct <: AbstractConfidenceStruct <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#Profile-Types","page":"Structs and Profile Types","title":"Profile Types","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/profile_structs/","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"Profile type is a Struct that specifies whether the profile to be taken uses the true loglikelihood function or an ellipse approximation of the loglikelihood function centred at the MLE (with optional use of parameter bounds).","category":"page"},{"location":"user_interface/profiles_and_samples/profile_structs/","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"AbstractProfileType\nAbstractEllipseProfileType\nLogLikelihood\nEllipseApprox\nEllipseApproxAnalytical","category":"page"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.AbstractProfileType","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.AbstractProfileType","text":"AbstractProfileType\n\nSupertype for profile types.\n\nSubtypes\n\nLogLikelihood\n\nAbstractEllipseProfileType\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.AbstractEllipseProfileType","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.AbstractEllipseProfileType","text":"AbstractProfileType\n\nSupertype for ellipse approximation profile types.\n\nSubtypes\n\nEllipseApprox\n\nEllipseApproxAnalytical\n\nSupertype Hiearachy\n\nAbstractProfileType <: AbstractProfileType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.LogLikelihood","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.LogLikelihood","text":"LogLikelihood()\n\nUse the true log-likelihood function for confidence profile evaluation. The methods IterativeBoundaryMethod and RadialRandomMethod are recommended for use with this profile type.\n\nSupertype Hiearachy\n\nLogLikelihood <: AbstractProfileType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.EllipseApprox","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.EllipseApprox","text":"EllipseApprox()\n\nUse an ellipse approximation of the log-likelihood function centred at the MLE with use of parameter bounds for confidence profile evaluation. The method RadialMLEMethod is recommended for use with this profile type.\n\nSupertype Hiearachy\n\nEllipseApprox <: AbstractEllipseProfileType <: AbstractProfileType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#LikelihoodBasedProfileWiseAnalysis.EllipseApproxAnalytical","page":"Structs and Profile Types","title":"LikelihoodBasedProfileWiseAnalysis.EllipseApproxAnalytical","text":"EllipseApproxAnalytical()\n\nUse an ellipse approximation of the log-likelihood function centred at the MLE without use of parameter bounds for confidence profile evaluation. As no parameter bounds are involved, it can be analytically evaluated. The method AnalyticalEllipseMethod is recommended for use with this profile type - it analytically samples points on the confidence profile boundary using EllipseSampling.jl. Other methods can be used, but they will all be unable to find interest parameter points outside user-provided parameter bounds (although nuisance parameters will be allowed outside these bounds).\n\nSupertype Hiearachy\n\nEllipseApproxAnalytical <: AbstractEllipseProfileType <: AbstractProfileType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/profile_structs/#Index","page":"Structs and Profile Types","title":"Index","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/profile_structs/","page":"Structs and Profile Types","title":"Structs and Profile Types","text":"Pages = [\"profile_structs.md\"]","category":"page"},{"location":"user_interface/coverage/univariate_intervals/#Parameter-Confidence-Intervals","page":"Parameter Confidence Intervals","title":"Parameter Confidence Intervals","text":"","category":"section"},{"location":"user_interface/coverage/univariate_intervals/","page":"Parameter Confidence Intervals","title":"Parameter Confidence Intervals","text":"Pages = [\"univariate_intervals.md\"]","category":"page"},{"location":"user_interface/coverage/univariate_intervals/","page":"Parameter Confidence Intervals","title":"Parameter Confidence Intervals","text":"Usage on several models can be seen in the examples section, such as for the Logistic Model.","category":"page"},{"location":"user_interface/coverage/univariate_intervals/","page":"Parameter Confidence Intervals","title":"Parameter Confidence Intervals","text":"check_univariate_parameter_coverage","category":"page"},{"location":"user_interface/coverage/univariate_intervals/#LikelihoodBasedProfileWiseAnalysis.check_univariate_parameter_coverage","page":"Parameter Confidence Intervals","title":"LikelihoodBasedProfileWiseAnalysis.check_univariate_parameter_coverage","text":"check_univariate_parameter_coverage(data_generator::Function, \n    generator_args::Union{Tuple, NamedTuple},\n    model::LikelihoodModel, \n    N::Int, \n    θtrue::AbstractVector{<:Real}, \n    θs::AbstractVector{<:Int64},\n    θinitialguess::AbstractVector{<:Real}=θtrue; \n    <keyword arguments>)\n\nPerforms a simulation to estimate the coverage of univariate confidence intervals for parameters in θs given a model by: \n\nRepeatedly drawing new observed data using data_generator for fixed true parameter values, θtrue. \nFitting the model and univariate confidence intervals. \nChecking whether the confidence interval for each of the parameters of interest contain the true parameter value in θtrue. The estimated coverage is returned with a default 95% confidence interval within a DataFrame. \n\nArguments\n\ndata_generator: a function with two arguments which generates data for fixed time points and true model parameters corresponding to the log-likelihood function contained in model. The two arguments must be the vector of true model parameters, θtrue, and a Tuple or NamedTuple, generator_args. Outputs a data Tuple or NamedTuple that corresponds to the log-likelihood function contained in model.\ngenerator_args: a Tuple or NamedTuple containing any additional information required by both the log-likelihood function and data_generator, such as the time points to be evaluated at. If evaluating the log-likelihood function requires more than just the simulated data, arguments for the data output of data_generator should be passed in via generator_args. \nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nN: a positive number of coverage simulations.\nθtrue: a vector of true parameters values of the model for simulating data with. \nθs: a vector of parameters to profile, as a vector of model parameter indexes.\nθinitialguess: a vector containing the initial guess for the values of each parameter. Used to find the MLE point in each iteration of the simulation. Default is θtrue.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to evaluate the confidence interval coverage at. Default is 0.95 (95%).\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\ncoverage_estimate_confidence_level: a number ∈ (0.0, 1.0) for the level of a confidence interval of the estimated coverage. Default is 0.95 (95%).\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter value. Default is missing (will use default_OptimizationSettings() (see default_OptimizationSettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of simulation iterations completed and estimated time of completion. Default is model.show_progress.\ndistributed_over_parameters: boolean variable specifying whether to distribute the workload of the simulation across simulation iterations (false) or across the individual confidence interval calculations within each iteration (true). Default is false.\n\nDetails\n\nThis simulated coverage check is used to estimate the performance of parameter confidence intervals. The simulation uses Distributed.jl to parallelise the workload.\n\nFor a 95% confidence interval of a interest parameter θi it is expected that under repeated experiments from an underlying true model (data generation) which are used to construct a confidence interval for θi using the method used in univariate_confidenceintervals!, 95% of the intervals constructed would contain the true value for θi. In the simulation where the values of the true parameters, θtrue, are known, this is equivalent to whether the confidence interval for θi contains the value θtrue[θi]. \n\nThe uncertainty in estimates of the coverage under the simulated model will decrease as the number of simulations, N, is increased. Confidence intervals for the coverage estimate are provided to quantify this uncertainty. The confidence interval for the estimated coverage is a Clopper-Pearson interval on a binomial test generated using HypothesisTests.jl.\n\nnote: Simultaneous confidence intervals\nCalculating the coverage of simultaneous confidence intervals is not currently supported (i.e. for dof ≠ 1)\n\nnote: Recommended setting for distributed_over_parameters\nIf the number of processes available to use is significantly greater than the number of model parameters or only a few model parameters are being checked for coverage, false is recommended.   \nIf system memory or model size in system memory is a concern, or the number of processes available is similar or less than the number of model parameters being checked, true will likely be more appropriate. \nWhen set to false, a separate LikelihoodModel struct will be used by each process, as opposed to only one when set to true, which could cause a memory issue for larger models. \n\ndanger: Not intended for use on bimodal univariate profile likelihoods\nThe current implementation only considers two extremes of the log-likelihood and whether the truth is between these two points. If the profile likelihood function is bimodal, it's possible the method has only found one set of correct confidence intervals (estimated coverage will be correct, but less than expected) or found one extrema on distinct sets (estimated coverage may be incorrect and will either be larger than expected or much lower than expected). \n\n\n\n\n\n","category":"function"},{"location":"examples/two-species_logistic/#Two-Species-Logistic-Model","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"The code included in this example is compiled into a single file here.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"The two-species logistic model with a Gaussian data distribution [11] has the following differential equations for the population densities of the two species C_1(t)geq0 and C_2(t)geq0 :","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"    fracmathrmdC(t)mathrmdt = lambda_1 C_1(t) Bigg1-fracS(t)KBigg","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"    fracmathrmdC(t)mathrmdt = lambda_2 C_2(t) Bigg1-fracS(t)KBigg - delta C_2(t) BiggfracC_1(t)KBigg","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"where S(t) = C_1(t)+C_2(t), and the full parameter vector is given by theta = (lambda_1 lambda_2 K delta C_1(0) C_2(0) sigma). The corresponding additive Gaussian data distribution, with an estimated standard deviation, has a density function for the observed data given by:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"    y_i sim p(y_i  theta) sim mathcalN(z_i(theta^M) theta^textrmo mathbbI) sim mathcalN(z_i(theta^M) sigma^2_N mathbbI) ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"where theta^M = (lambda_1 lambda_2 K delta C_1(0) C_2(0)), theta^textrmo = sigma, z_i(theta^M)=z(t_i theta^M) = (C_1(t_i theta^M) C_2(t_i theta^M)) from the previous equations, meaning at each t_i we have an observation of both C_1(t) and C_2(t), y_i^textrmo=(C_1i^textrmo C_2i^textrmo), and mathbbI is a 2times2 identity matrix.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"This model uses real data, so no 'true' parameter values exist. Instead, the MLE values of parameters are used for coverage simulations hattheta =(000293 000315 000164 788 0289 00293 183). The corresponding lower and upper parameter bounds are a = (00001 00001 0 60 001 0001 01) and b = (001 001 001 90 1 1 3); the lower bounds for all the parameters apart from delta were zero [11] but were increased slightly to increase stability. Observation times are t_1I = (0 769 1140 1488 1876 2233 2602 2889 3213 3621 4028). Smaller nuisance parameter bounds are used for univariate profiles, although they are wider than those used in [11]: a_textnuisancej =max(a_j hattheta_jdiv25 )  j in 127 and b_textnuisancej =min(b_j hattheta_jtimes25 )  j in 127. The original implementation can be found at https://github.com/ProfMJSimpson/profile_predictions.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Real observations, the MLE model trajectory and the MLE 95% population reference set under this parameterisation can be seen in the figure below (site 2 data is used):","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/#With-Logit-Normal-Data-Distribution","page":"Two-Species Logistic Model","title":"With Logit-Normal Data Distribution","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"If we instead use a more statistically realistic logit-normal distribution, defined on (0,1), instead of an additive Gaussian data distribution, the density function for the observed data becomes:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"    y_i sim textLogitNormal(textlogit(z_i(theta^M)) sigma^2mathbbI)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"where theta^M = (lambda_1 lambda_2 K delta C_1(0) C_2(0)), theta^textrmo = sigma and textlogit(p)=log(pdiv (1-p)). The model trajectory, z_i(theta^M), is assumed to be a proportion in (01).","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"The 'true' parameter values used for coverage simulations are similar to the MLE values of the parameters when using this data distribution, albeit with a lower value of sigma: theta = (0003 00004 00004 800 04 12 01). Parameter bounds have been adjusted slightly to a = (00005 000001 000001 60 001 01 001) and b = (001 0005 0005 98 2 3 1) and no 'special' nuisance parameter bounds are specified.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"For coverage testing of predictive quantities using the sampled full parameter confidence set, we use much more well-informed parameter bounds that may be overly constrained: a_textsampling = (00022 000001 00001 73 025 07 003) and b_textsampling = (00036 0001 00009 85 065 2 02).","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Example observations, the true model trajectory and the 95% population reference set under this parameterisation can be seen in the figure below:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/#Initial-Setup","page":"Two-Species Logistic Model","title":"Initial Setup","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Here we add 10 worker processes, given a PC with 12 CPU threads and 32GB RAM. For coverage testing we recommend setting this number as discussed in Import Package and Set Up Distributed Environment. We also use LogExpFunctions to define the logit function.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"using Distributed\nif nprocs()==1; addprocs(10, env=[\"JULIA_NUM_THREADS\"=>\"1\"]) end\n@everywhere using Random, Distributions, DifferentialEquations\n@everywhere using LogExpFunctions\n@everywhere using LikelihoodBasedProfileWiseAnalysis\nusing Combinatorics","category":"page"},{"location":"examples/two-species_logistic/#Model-and-Likelihood-Function-Definition","page":"Two-Species Logistic Model","title":"Model and Likelihood Function Definition","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Here, we will use the logit-normal data distribution. The Gaussian distribution can be used by changing the distribution used in the log-likelihood function, as seen for the Logistic Model and Lotka-Volterra Model examples.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"The logit-normal distribution expects a proportion and our data and model produce a percentage so we are required to divide these through by 100.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"@everywhere function DE!(dC, C, p, t)\n    λ1, λ2, δ, KK = p\n    S = C[1] + C[2]\n    dC[1] = λ1 * C[1] * (1.0 - S/KK)\n    dC[2] = λ2 * C[2] * (1.0 - S/KK) - δ*C[2]*C[1]/KK\nend\n\n@everywhere function odesolver(t, λ1, λ2, δ, KK, C01, C02)\n    p=(λ1, λ2, δ, KK)\n    C0=[C01, C02]\n    tspan=(0.0, maximum(t))\n    prob=ODEProblem(DE!, C0, tspan, p)\n    sol=solve(prob, saveat=t)\n    return sol[1,:], sol[2,:]\nend\n\n@everywhere function ODEmodel(t, θ)\n    (y1, y2) = odesolver(t, θ[1], θ[2], θ[3], θ[4], θ[5], θ[6])\n    return y1, y2\nend\n\n@everywhere function loglhood(θ, data)\n    (y1, y2) = ODEmodel(data.t, θ)\n    e=0.0\n    for i in axes(data.y_obs,1)\n        e += (loglikelihood(LogitNormal(logit(y1[i]/100.), θ[7]), data.y_obs[i,1]/100.) + \n                loglikelihood(LogitNormal(logit(y2[i]/100.), θ[7]), data.y_obs[i,2]/100.))\n    end\n    return e\nend","category":"page"},{"location":"examples/two-species_logistic/#Initial-Data-and-Parameter-Definition","page":"Two-Species Logistic Model","title":"Initial Data and Parameter Definition","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"# true data\nt=[0, 769, 1140, 1488, 1876, 2233, 2602, 2889, 3213, 3621, 4028]\ndata11=[0.748717949, 0.97235023, 5.490243902, 17.89100529, 35, 56.38256703, 64.55087666, 66.61940299, 71.67362453, 80.47179487, 79.88291457]\ndata12=[1.927065527, 0.782795699, 1.080487805, 2.113227513, 3.6, 2.74790376, 2.38089652, 1.8, 0.604574153, 1.305128205, 1.700502513]\n\n# true parameters used for coverage testing\nθ_true = [0.003, 0.0004, 0.0004, 80.0, 0.4, 1.2, 0.1]\ny_true = hcat(ODEmodel(t, θ_true)...)\n\n# Named tuple of all data required within the log-likelihood function\ndata = (y_obs=hcat(data11,data12), t=t)\n\n# Bounds on model parameters \nlb_sample = [0.0022, 0.00001, 0.0001, 73.0, 0.25, 0.7, 0.03]\nub_sample = [0.0036,  0.001, 0.0009, 85., 0.65, 2.0, 0.2]\n\nlb = [0.0005, 0.00001, 0.00001, 60.0, 0.01, 0.1, 0.01]\nub = [0.01, 0.005, 0.005, 98.0, 2.0, 3.0, 1.0]\n\nλ1g=0.002; λ2g=0.002; δg=0.001; KKg=80.0; C0g=[1.0, 1.0]; σg=0.5\nθG = [λ1g, λ2g, δg, KKg, C0g[1], C0g[2], σg]\n\nθnames = [:λ1, :λ2, :δ, :K, :C01, :C02, :σ]\npar_magnitudes = [0.001, 0.001, 0.001, 10, 1, 1, 1]","category":"page"},{"location":"examples/two-species_logistic/#LikelihoodModel-Initialisation","page":"Two-Species Logistic Model","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Here we define a single LikelihoodModel for sampling and profiling of parameter confidence sets. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5,))\nmodel = initialise_LikelihoodModel(loglhood, data, θnames, θG, lb, ub, par_magnitudes, optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/#Full-Parameter-Vector-Confidence-Set-Evaluation","page":"Two-Species Logistic Model","title":"Full Parameter Vector Confidence Set Evaluation","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To evaluate the full parameter vector confidence set at a 95% confidence level we use the following. Note the use of the lb and ub keyword arguments to specify the parameter ranges to sample points across.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"full_likelihood_sample!(model, Int(1e7), use_distributed=true, lb=lb_sample, ub=ub_sample)","category":"page"},{"location":"examples/two-species_logistic/#Profiling","page":"Two-Species Logistic Model","title":"Profiling","text":"","category":"section"},{"location":"examples/two-species_logistic/#Univariate-Profiles","page":"Two-Species Logistic Model","title":"Univariate Profiles","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To find the confidence intervals for all seven parameters at a 95% confidence level (the default), we use:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"univariate_confidenceintervals!(model)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Similarly, if we wish to find simultaneous 95% confidence intervals for the parameters we set the degrees of freedom parameter, dof, to the number of model parameters (instead of 1).","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"univariate_confidenceintervals!(model, dof=model.core.num_pars) # model.core.num_pars=7","category":"page"},{"location":"examples/two-species_logistic/#Bivariate-Profiles","page":"Two-Species Logistic Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To evaluate the bivariate boundaries for all 21 bivariate parameter combinations, here we use the RadialMLEMethod, which uses a 20 point ellipse approximation of the boundary as a starting guess. The boundaries in this example are reasonably convex, which makes this starting guess appropriate. To speed up computation we provide stronger optimization settings.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 20, \n    method=RadialMLEMethod(0.15, 0.01),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Similarly, if we wish to evaluate simultaneous 95% bivariate profiles we set the degrees of freedom parameter, dof, to the number of model parameters (instead of 2).","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 20, \n    method=RadialMLEMethod(0.15, 0.01), \n    dof=model.core.num_pars,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/#Plots-of-Profiles","page":"Two-Species Logistic Model","title":"Plots of Profiles","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To visualise plots of these profiles we load Plots alongside a plotting backend. Here we use GR.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"using Plots, Plots.PlotMeasures; gr()\nPlots.reset_defaults(); Plots.scalefontsizes(0.75)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Univariate and bivariate profiles can either be visualised individually or in comparison to profiles at the same confidence level and degrees of freedom. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Here we plot the first two univariate profiles formed at a 95% confidence level and 1 degree of freedom.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plts = plot_univariate_profiles(model, θs_to_plot=[1,2],\n    confidence_levels=[0.95], dofs=[1])\n\nplt = plot(plts..., layout=(1,2),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Similarly, here we plot the first two simultaneous bivariate profiles formed at a 95% confidence level and 7 degrees of freedom.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plts = plot_bivariate_profiles(model, θcombinations_to_plot=[[1,2], [1,3]],\n    confidence_levels=[0.95], dofs=[model.core.num_pars])\n\nplt = plot(plts..., layout=(1,2),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/#Predictions","page":"Two-Species Logistic Model","title":"Predictions","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To make predictions for the model trajectory and the 1-delta population reference set we define the following functions, which then need to be added to our LikelihoodModel. The region variable in errorfunction should be set equal to 1-delta when generating predictions. These could also be added in initialise_LikelihoodModel.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"@everywhere function predictfunction(θ, data, t=data.t)\n    y1, y2 = ODEmodel(t, θ) \n    y = hcat(y1,y2)\n    return y\nend\n\n@everywhere function errorfunction(predictions, θ, region)\n    lq, uq = logitnormal_error_σ_estimated(predictions ./ 100, θ, region, 7)\n    lq .= lq .* 100\n    uq .= uq .* 100\n    return lq, uq\nend\n\nadd_prediction_function!(model, predictfunction)\nadd_error_function!(model, errorfunction)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To generate profile-wise predictions for each of the evaluated profiles we first define the desired time points for prediction and then evaluate the approximate model trajectory confidence sets and (1-delta 1-alpha) population reference tolerance sets. By default, the population reference tolerance set evaluates reference interval regions at the same level as the default confidence level (1-delta = 1-alpha = 095); however, this is not required. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"t_pred=LinRange(t[1], t[end], 201)\n\ngenerate_predictions_univariate!(model, t_pred)\ngenerate_predictions_bivariate!(model, t_pred)\ngenerate_predictions_dim_samples!(model, t_pred) # for the full likelihood sample","category":"page"},{"location":"examples/two-species_logistic/#Plotting-Predictions","page":"Two-Species Logistic Model","title":"Plotting Predictions","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"We can plot the predictions of individual profiles or the union of all profiles at a given number of interest parameters, confidence level, degrees of freedom and reference interval region (if relevant). When plotting the union of these predictions we can compare it to the result of the full likelihood sample, which here used LatinHypercubeSamples, the default. Here we plot the results from simultaneous profiles.","category":"page"},{"location":"examples/two-species_logistic/#Model-Trajectory","page":"Two-Species Logistic Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"using Plots; gr()\nmodel_trajectory = ODEmodel(t_pred, θ_true)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plt = plot_predictions_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # univariate profiles\n\nplot!(plt; dpi=150, size=(450, 300), xlims=(t_pred[1], t_pred[end]))\nplot!(plt[1], t_pred, model_trajectory[1],\n    lw=3, color=:turquoise4, linestyle=:dash)\nplot!(plt[2], t_pred, model_trajectory[2],\n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plt = plot_predictions_union(model, t_pred, 2, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # bivariate profiles\n\nplot!(plt; dpi=150, size=(450, 300), xlims=(t_pred[1], t_pred[end]))\nplot!(plt[1], t_pred, model_trajectory[1],\n    lw=3, color=:turquoise4, linestyle=:dash)\nplot!(plt[2], t_pred, model_trajectory[2],\n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/#1-\\delta-Population-Reference-Set","page":"Two-Species Logistic Model","title":"1-delta Population Reference Set","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"using Plots; gr()\nlq, uq = errorfunction(hcat(ODEmodel(t_pred, θ_true)...), θ_true, 0.95)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plt = plot_realisations_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # univariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3],\n    xlims=(t_pred[1], t_pred[end]), dpi=150, size=(450, 300))","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"plt = plot_realisations_union(model, t_pred, 2, dof=model.core.num_pars, \n    compare_to_full_sample_type=LatinHypercubeSamples(), plot_title=\"\") # bivariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3],\n    xlims=(t_pred[1], t_pred[end]), dpi=150, size=(450, 300))","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/two-species_logistic/#Coverage-Testing","page":"Two-Species Logistic Model","title":"Coverage Testing","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To conduct an investigation into the coverage properties of the profiles and profile-wise predictions sets we can perform a simulation study using the provided coverage functions. The procedures are effectively identical to those used for the Logistic Model; the commentary for that example remains similar for this example, however coverage may be lower than expected due to too few observations. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"danger: Computational time of these tests\nThe computational time of several of the below tests is up to 4 hrs with 10 worker processes on the author's pc.","category":"page"},{"location":"examples/two-species_logistic/#Data-Generation","page":"Two-Species Logistic Model","title":"Data Generation","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"First we define functions and arguments which we use to simulate new training and testing data, and evaluate the true 1-delta population reference set, given the true parameter values. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"# DATA GENERATION FUNCTION AND ARGUMENTS\n@everywhere function data_generator(θtrue, generator_args::NamedTuple)\n    y_obs = zeros(size(generator_args.y_true))\n    for i in eachindex(generator_args.y_true)\n        y_obs[i] = rand(LogitNormal(logit(generator_args.y_true[i]/100.), θtrue[7]))\n    end\n    y_obs .= y_obs .* 100\n    if generator_args.is_test_set; return y_obs end\n    data = (y_obs=y_obs, generator_args...)\n    return data\nend\n\n@everywhere function reference_set_generator(θtrue, generator_args::NamedTuple, region::Float64)\n    lq, uq = errorfunction(generator_args.y_true, θtrue, region)\n    return (lq, uq)\nend\n\ntraining_gen_args = (y_true=y_true, t=t, is_test_set=false)\ntesting_gen_args = (y_true=predictfunction(θ_true, data, t_pred), t=t_pred, is_test_set=true)","category":"page"},{"location":"examples/two-species_logistic/#Parameter-Confidence-Intervals","page":"Two-Species Logistic Model","title":"Parameter Confidence Intervals","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Coverage of parameter confidence intervals:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nuni_coverage_df = check_univariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, θ_true, collect(1:model.core.num_pars),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/#Bivariate-Profiles-2","page":"Two-Species Logistic Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Coverage of the true value of each set of bivariate interest parameters:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_coverage_df = check_bivariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method = RadialMLEMethod(0.15, 0.1), \n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Coverage of the true bivariate boundary. 2000 samples corresponds to around 100-200 retained points:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_boundary_coverage_df = check_bivariate_boundary_coverage(data_generator,\n    training_gen_args, model, 100, 30, 2000, θ_true,\n    collect(combinations(1:model.core.num_pars, 2)); \n    method = RadialMLEMethod(0.15, 0.1), \n    coverage_estimate_quantile_level=0.9,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/#Prediction-Coverage","page":"Two-Species Logistic Model","title":"Prediction Coverage","text":"","category":"section"},{"location":"examples/two-species_logistic/#Model-Trajectory-2","page":"Two-Species Logistic Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To test the coverage of the true model trajectory we can use check_dimensional_prediction_coverage, check_univariate_prediction_coverage and check_bivariate_prediction_coverage. Again we use the default 95% confidence level here. Given a sufficient number of sampled points we expect the model trajectory coverage from the trajectory confidence set from propagating forward the full parameter vector 95% confidence set to have 95% simultaneous coverage. ","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfull_trajectory_coverage_df = check_dimensional_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 200, Int(1e7), \n    θ_true, [collect(1:model.core.num_pars)], lb=lb_sample, ub=ub_sample)\n\nuni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars),\n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method=RadialMLEMethod(0.15, 0.1),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"Repeating the coverage of univariate and bivariate profiles using the profile path approach:","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"uni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars,\n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=RadialMLEMethod(0.15, 0.1),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/two-species_logistic/#1-\\delta-Population-Reference-Set-and-Observations","page":"Two-Species Logistic Model","title":"1-delta Population Reference Set and Observations","text":"","category":"section"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"To test the coverage of the 1-delta population reference set as well as observations we can use check_dimensional_prediction_realisations_coverage, check_univariate_prediction_realisations_coverage and check_bivariate_prediction_realisations_coverage. Here we will only look at the coverage for simultaneous profiles.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors.","category":"page"},{"location":"examples/two-species_logistic/","page":"Two-Species Logistic Model","title":"Two-Species Logistic Model","text":"full_reference_coverage_df = check_dimensional_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 200, Int(1e7), \n    θ_true, [collect(1:model.core.num_pars)], lb=lb_sample, ub=ub_sample)\n\nuni_reference_coverage_df = check_univariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars, \n    optimizationsettings=opt_settings)\n\nbiv_reference_coverage_df = check_bivariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, 30, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=RadialMLEMethod(0.15, 0.1),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/binomial_normal_approximation/#Gaussian-Approximation-of-a-Binomial-Distribution","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"The code included in this example is compiled into a single file here.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"This model is taken from an unreleased paper by the authors of the workflow [1]. The Binomial distribution is defined as:","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"    X sim textB(np)","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"where n is the number of trials and p is the probability of success. For sufficiently large n, this distribution has the following Gaussian approximation:","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"    y_i sim p(y_i  theta) X sim mathcalN(np sqrtnp(1-p))","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"where np is the mean number of successes and sqrtnp(1-p) is the standard deviation of this mean. We take the model parameter vector as theta = (np). ","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"The true parameter values are theta =(100 02). The corresponding lower and upper parameter bounds are a = (00001 00001) and b = (50010). There is no observation time as such, but we take ten samples of the Gaussian approximation under the true parameterisation, y^textrmo_1I=219 223 128 164 164 203 162 200 197 244. ","category":"page"},{"location":"examples/binomial_normal_approximation/#Initial-Setup","page":"Gaussian Approximation of a Binomial Distribution","title":"Initial Setup","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"using Random, Distributions\nusing LikelihoodBasedProfileWiseAnalysis","category":"page"},{"location":"examples/binomial_normal_approximation/#Model-and-Likelihood-Function-Definition","page":"Gaussian Approximation of a Binomial Distribution","title":"Model and Likelihood Function Definition","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"distrib(θ) = Normal(θ[1] * θ[2], sqrt(θ[1] * θ[2] * (1 - θ[2])))\n\nfunction loglhood(θ, data)\n    return sum(logpdf.(distrib(θ), data.samples))\nend\n\nfunction predictfunction(θ, data, t=[\"n*p\"]); [prod(θ)] end","category":"page"},{"location":"examples/binomial_normal_approximation/#Initial-Data-and-Parameter-Definition","page":"Gaussian Approximation of a Binomial Distribution","title":"Initial Data and Parameter Definition","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"# true parameters\nθ_true = [200.0, 0.2]\n\n# Named tuple of all data required within the log-likelihood function\ndata = (samples=[21.9, 22.3, 12.8, 16.4, 16.4, 20.3, 16.2, 20.0, 19.7, 24.4],)\n\n# Bounds on model parameters\nlb = [0.0001, 0.0001]\nub = [500.0, 1.0]\n\nθnames = [:n, :p]\nθG = [50, 0.3]\npar_magnitudes = [100, 1]","category":"page"},{"location":"examples/binomial_normal_approximation/#LikelihoodModel-Initialisation","page":"Gaussian Approximation of a Binomial Distribution","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"model = initialise_LikelihoodModel(loglhood, data, θnames, θG, lb, ub, par_magnitudes)","category":"page"},{"location":"examples/binomial_normal_approximation/#Evaluating-a-Concave-Boundary","page":"Gaussian Approximation of a Binomial Distribution","title":"Evaluating a Concave Boundary","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"This example is particularly interesting because it contains a very concave bivariate boundary - the IterativeBoundaryMethod thus becomes very appropriate to use. However, evaluting this many points may be prohibitive on higher dimensional models.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"bivariate_confidenceprofiles!(model, 200, \n    method=IterativeBoundaryMethod(20,20,20, 0.5, 0.1, use_ellipse=true))","category":"page"},{"location":"examples/binomial_normal_approximation/#Visualising-the-Progress-of-the-IterativeBoundaryMethod","page":"Gaussian Approximation of a Binomial Distribution","title":"Visualising the Progress of the IterativeBoundaryMethod","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"We can visualise the progress of the IterativeBoundaryMethod using plot_bivariate_profiles_iterativeboundary_gif.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"using Plots; gr()\nPlots.reset_defaults(); Plots.scalefontsizes(0.75)\n\nformat = (size=(450, 300), dpi=150, title=\"\",\n    legend_position=:topright, palette=:Paired)\nplot_bivariate_profiles_iterativeboundary_gif(model, 0.2, 0.2; \n    markeralpha=0.5, color=2, save_as_separate_plots=false, save_folder=joinpath(\"docs\", \"src\", \"assets\", \"figures\", \"binomial\"), format...)","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"(Image: )","category":"page"},{"location":"examples/binomial_normal_approximation/#Coordinate-Transformation","page":"Gaussian Approximation of a Binomial Distribution","title":"Coordinate Transformation","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"This is an example that particularly benefits from a coordinate transformation to improve the regularity of the log-likelihood. A natural log transformation is particularly appropriate. ","category":"page"},{"location":"examples/binomial_normal_approximation/#Redefining-Functions","page":"Gaussian Approximation of a Binomial Distribution","title":"Redefining Functions","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"Here we define the new log-likelihood and prediction functions which define the backwards mapping from our logged parameterisation to the original parameterisation. We also define a function which specifies the forward parameter transformation, from the original to the logged parameterisation.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"function loglhood_Θ(Θ, data)\n    return loglhood(exp.(Θ), data)\nend\n\nfunction predictfunctions_Θ(Θ, data, t=[\"n*p\"]); [prod(exp.(Θ))] end\n\nfunction forward_parameter_transformLog(θ)\n    return log.(θ)\nend","category":"page"},{"location":"examples/binomial_normal_approximation/#Transforming-Parameter-Definitions","page":"Gaussian Approximation of a Binomial Distribution","title":"Transforming Parameter Definitions","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"To update the parameter bounds we can use transformbounds_NLopt, which solves an integer program to determine how the old bounds map to the new bounds given the specified transformation.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"lb_Θ, ub_Θ = transformbounds_NLopt(forward_parameter_transformLog, lb, ub)\n\nΘnames = [:ln_n, :ln_p]\nΘG = forward_parameter_transformLog(θG)\npar_magnitudes = [2, 1]","category":"page"},{"location":"examples/binomial_normal_approximation/#LikelihoodModel-Initialisation-2","page":"Gaussian Approximation of a Binomial Distribution","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"model = initialise_LikelihoodModel(loglhood_Θ, data, Θnames, ΘG, lb_Θ, ub_Θ, par_magnitudes)","category":"page"},{"location":"examples/binomial_normal_approximation/#Re-evaluating-the-Bivariate-Boundary","page":"Gaussian Approximation of a Binomial Distribution","title":"Re-evaluating the Bivariate Boundary","text":"","category":"section"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"Re-evaluating the bivariate boundary of the log-likelihood function after the transformation reveals a much more convex shape.","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"bivariate_confidenceprofiles!(model, 40, method=RadialMLEMethod(0.15, 1.))\n\nusing Plots; gr()\n\nplots = plot_bivariate_profiles(model, 0.2, 0.2; include_internal_points=true, markeralpha=0.9, format...)\ndisplay(plots[1])","category":"page"},{"location":"examples/binomial_normal_approximation/","page":"Gaussian Approximation of a Binomial Distribution","title":"Gaussian Approximation of a Binomial Distribution","text":"(Image: )","category":"page"},{"location":"internal_library/bivariate/#Bivariate-Functions","page":"Bivariate Functions","title":"Bivariate Functions","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"Pages = [\"bivariate.md\"]","category":"page"},{"location":"internal_library/bivariate/#Likelihood-Optimisation","page":"Bivariate Functions","title":"Likelihood Optimisation","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.bivariateψ!\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_vectorsearch!\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_continuation!\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_vectorsearch\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_continuation\nLikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_unbounded","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ!","text":"bivariateψ!(ψ::Real, p::NamedTuple)\n\nGiven a log-likelihood function (p.consistent.loglikefunction) which is bounded in parameter space and may be an ellipse approximation, this function finds the values of the nuisance parameters ω that optimise the function fixed values of the interest parameters p.ψ_x[1] and ψ and returns the log-likelihood value minus the confidence boundary target threshold. The returned function value will be zero at the locations of the approximate confidence boundary for p.ψ_x[1] and ψ. Nuisance parameter values are stored in the NamedTuple p at p.ω_opt. Used by Fix1AxisMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_vectorsearch!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_vectorsearch!","text":"bivariateψ_vectorsearch!(ψ::Real, p::NamedTuple)\n\nGiven a log-likelihood function (p.consistent.loglikefunction) which is bounded in parameter space and may be an ellipse approximation, this function finds the values of the nuisance parameters ω that optimise the function fixed values of the interest parameters ψxy = p.pointa + ψ*p.uhat and returns the log-likelihood value minus the confidence boundary target threshold. The returned function value will be zero at the locations of the approximate confidence boundary for ψxy. Nuisance parameter values are stored in the NamedTuple p at p.ω_opt. Used by AbstractBivariateVectorMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_continuation!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_continuation!","text":"bivariateψ_continuation!(ψ::Real, p::NamedTuple)\n\nGiven a log-likelihood function (p.consistent.loglikefunction) which is bounded in parameter space and may be an ellipse approximation, this function finds the values of the nuisance parameters ω that optimise the function fixed values of the interest parameters ψxy = p.pointa + ψ*p.uhat and returns the log-likelihood value minus the confidence boundary target threshold. The returned function value will be zero at the locations of the approximate confidence boundary for ψxy. Nuisance parameter values are stored in the NamedTuple p at p.ω_opt. Used by ContinuationMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical","text":"bivariateψ_ellipse_analytical(ψ::Real, p::NamedTuple)\n\nReturns the approximated log-likelihood value minus the confidence boundary target threshold, given an analytic ellipse approximation of a log-likelihood function (LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike) which is unbounded in parameter space, for values of the interest parameters p.ψ_x[1] and ψ. The returned function value will be zero at the locations of the approximate confidence boundary for p.ψ_x[1] and ψ, which correspond to locations found by LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod. Used by Fix1AxisMethod. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_vectorsearch","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_vectorsearch","text":"bivariateψ_ellipse_analytical_vectorsearch(ψ::Real, p::NamedTuple)\n\nReturns the approximated log-likelihood value minus the confidence boundary target threshold, given an analytic ellipse approximation of a log-likelihood function (LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike) which is unbounded in parameter space, for values of the interest parameters ψxy = p.pointa + ψ*p.uhat. The returned function value will be zero at the locations of the approximate confidence boundary for ψxy = p.pointa + ψ*p.uhat, which correspond to locations found by LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod. Used by AbstractBivariateVectorMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_continuation","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_analytical_continuation","text":"bivariateψ_ellipse_analytical_continuation(ψ::Real, p::NamedTuple)\n\nReturns the approximated log-likelihood value minus the confidence boundary target threshold, given an analytic ellipse approximation of a log-likelihood function (LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike) which is unbounded in parameter space, for values of the interest parameters ψxy = p.pointa + ψ*p.uhat. The returned function value will be zero at the locations of the approximate confidence boundary for ψxy = p.pointa + ψ*p.uhat, which correspond to locations found by LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod. Used by ContinuationMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_unbounded","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariateψ_ellipse_unbounded","text":"bivariateψ_ellipse_unbounded(ψ::Vector, p::NamedTuple)\n\nGiven an ellipse approximation of a log-likelihood function (LikelihoodBasedProfileWiseAnalysis.ellipse_loglike) which is unbounded in parameter space, this function finds the values of the nuisance parameters ω that optimise the function at fixed values of the two interest parameters in ψ and returns the approximated log-likelihood value minus the confidence boundary target threshold. The returned function value will be zero at the locations of the approximate confidence boundary for ψ, which correspond to the locations found by LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod. Nuisance parameter values are stored in the NamedTuple p. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Finding-Points-on-2D-bounds","page":"Bivariate Functions","title":"Finding Points on 2D bounds","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.findpointonbounds","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.findpointonbounds","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.findpointonbounds","text":"findpointonbounds(model::LikelihoodModel, \n    internalpoint::Vector{<:Float64}, \n    direction_πradians::Float64, \n    cosDir::Float64, \n    sinDir::Float64, \n    ind1::Int, \n    ind2::Int,\n    returnboundindex::Bool=false)\n\nGiven an internalpoint and direction in radians (defined as anticlockwise rotation from the x-axis) from this point, returns the first point on the 2D bounds encountered in that direction from internalpoint. Returns which bound the point is on if returnboundindex = true.\n\n\n\n\n\nfindpointonbounds(model::LikelihoodModel, \n    internalpoint::Vector{<:Float64}, \n    direction_πradians::Float64, \n    cosDir::Float64, \n    sinDir::Float64, \n    ind1::Int, \n    ind2::Int,\n    returnboundindex::Bool=false)\n\nAlternate method for LikelihoodBasedProfileWiseAnalysis.findpointonbounds which specifies the direction from internalpoint as a vector rather than an angle in radians. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Main-Confidence-Boundary-Logic","page":"Bivariate Functions","title":"Main Confidence Boundary Logic","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"Note: AnalyticalEllipseMethod is calculated using generate_N_clustered_points from EllipseSampling.jl within LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile.","category":"page"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.add_biv_profiles_rows!\nLikelihoodBasedProfileWiseAnalysis.set_biv_profiles_row!\nLikelihoodBasedProfileWiseAnalysis.get_bivariate_opt_func\nLikelihoodBasedProfileWiseAnalysis.get_ωs_bivariate_ellipse_analytical!\nLikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile\nBase.merge(::BivariateConfidenceStruct, ::BivariateConfidenceStruct)","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.add_biv_profiles_rows!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.add_biv_profiles_rows!","text":"add_biv_profiles_rows!(model::LikelihoodModel, num_rows_to_add::Int)\n\nAdds num_rows_to_add rows to model.biv_profiles_df. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.set_biv_profiles_row!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.set_biv_profiles_row!","text":"set_biv_profiles_row!(model::LikelihoodModel, \n    row_ind::Int, \n    θcombination::Tuple{Int, Int},\n    not_evaluated_internal_points::Bool, \n    not_evaluated_predictions::Bool,\n    boundary_not_ordered::Bool,\n    confidence_level::Float64, \n    dof::Int,\n    profile_type::AbstractProfileType,\n    method::AbstractBivariateMethod, \n    num_points::Int)\n\nSets the columns of row row_ind of model.biv_profiles_df to contain the relevant info about a just conducted profile. model.biv_profiles_dict contains the profile for row row_ind at key row_ind.  \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.get_bivariate_opt_func","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_bivariate_opt_func","text":"get_bivariate_opt_func(profile_type::AbstractProfileType, method::AbstractBivariateMethod)\n\nReturns the correct bivariate optimisation function used to find the optimal values of nuisance parameters at a set of interest parameters for the profile_type log-likelihood function. The optimisation function returns the value of the profile_type log-likelihood function as well as finding the optimal nuisance parameters and saving these in one of it's inputs.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.get_ωs_bivariate_ellipse_analytical!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_ωs_bivariate_ellipse_analytical!","text":"get_ωs_bivariate_ellipse_analytical!(boundary,\n    num_points::Int,\n    consistent::NamedTuple, \n    ind1::Int, \n    ind2::Int, \n    num_pars::Int,\n    initGuess::Vector{<:Float64}, \n    θranges::Tuple{T, T, T}, \n    ωranges::Tuple{T, T, T},\n    optimizationsettings::OptimizationSettings,\n    samples_all_pars::Union{Missing, Matrix{Float64}}=missing) where T<:UnitRange\n\nDetermines the nuisance parameters for a EllipseApproxAnalytical boundary profile by optimising over the unbounded ellipse approximation of the log-likelihood centred at the MLE. At higher confidence levels, where the ellipse approximation is less accurate, it is likely that predictions produced by running the model with these optimised nuisance parameters will be unrealistic and/or the parameters themselves may be infeasible for the model. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile","text":"bivariate_confidenceprofile(bivariate_optimiser::Function,\n    model::LikelihoodModel, \n    num_points::Int,\n    confidence_level::Float64,\n    consistent::NamedTuple,\n    ind1::Int,\n    ind2::Int,\n    dof::Int,\n    profile_type::AbstractProfileType,\n    method::AbstractBivariateMethod,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    find_zero_atol::Real,\n    optimizationsettings::OptimizationSettings,\n    channel::RemoteChannel)\n\nReturns a BivariateConfidenceStruct containing the num_points boundary points and internal points (if save_internal_points=true) for the specified combination of parameters ind1 and ind2, and profile_type at confidence_level using method. Calls the desired method. Called by bivariate_confidenceprofiles!.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Base.merge-Tuple{BivariateConfidenceStruct, BivariateConfidenceStruct}","page":"Bivariate Functions","title":"Base.merge","text":"Base.merge(a::BivariateConfidenceStruct, b::BivariateConfidenceStruct)\n\nSpecifies how to merge two variables with type BivariateConfidenceStruct.\n\n\n\n\n\n","category":"method"},{"location":"internal_library/bivariate/#Minimum-Perimeter-Polygon","page":"Bivariate Functions","title":"Minimum Perimeter Polygon","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.minimum_perimeter_polygon!","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.minimum_perimeter_polygon!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.minimum_perimeter_polygon!","text":"minimum_perimeter_polygon!(points::Array{<:Real,2})\n\nGiven a set of N points that define a boundary polygon in a 2 row, N column array, solve a minimum perimeter polygon TSP problem, reorder these points in place and return the path used (vertices in order of visitation). Uses TravelingSalesmanHeuristics.jl.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Iterative-Boundary-Method","page":"Bivariate Functions","title":"Iterative Boundary Method","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"For IterativeBoundaryMethod.","category":"page"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE!\nLikelihoodBasedProfileWiseAnalysis.edge_length\nLikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi!\nLikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi\nLikelihoodBasedProfileWiseAnalysis.iterativeboundary_init\nLikelihoodBasedProfileWiseAnalysis.newboundarypoint!\nLikelihoodBasedProfileWiseAnalysis.heapupdates_success!\nLikelihoodBasedProfileWiseAnalysis.heapupdates_failure!\nLikelihoodBasedProfileWiseAnalysis.polygon_break_and_rejoin!\nLikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_iterativeboundary","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE!","text":"findNpointpairs_radialMLE!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    ind1::Int, \n    ind2::Int,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    biv_opt_is_ellipse_analytical::Bool, \n    ellipse_confidence_level::Float64,\n    dof::Int,\n    ellipse_start_point_shift::Float64,\n    ellipse_sqrt_distortion::Float64,\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool)\n\nImplementation of finding pairs of points that bracket the bivariate confidence boundary for RadialMLEMethod.\n\nSearch directions from the MLE point are given by points placed on a ellipse approximation around the point using generate_N_clustered_points from EllipseSampling.jl. \n\n\n\n\n\nfindNpointpairs_radialMLE!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    ind1::Int, \n    ind2::Int,\n    biv_opt_is_ellipse_analytical::Bool, \n    radial_start_point_shift::Float64,\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool)\n\nImplementation of finding pairs of points that bracket the bivariate confidence boundary for IterativeBoundaryMethod, searching radially from the MLE point for points on the bounds, in search directions similar to those used by RadialRandomMethod. If a point on the bounds is inside the confidence boundary, that point will represent the boundary in that search direction.\n\nSearch directions are given by distorting uniformly spaced anticlockwise angles on a circle to angles on an ellipse representative of the relative magnitude of each parameter. If the magnitude of a parameter is a NaN value (i.e. either bound is Inf), then the relative magnitude is set to 1.0, as no information is known about its magnitude.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.edge_length","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.edge_length","text":"edge_length(boundary, inds1, inds2, relative_magnitude)\n\nEuclidean distance between two vertices (length of an edge), scaled by the relative magnitude of parameters, so that each dimension has roughly the same weight.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi!","text":"internal_angle_from_pi!(vertex_internal_angle_objs, \n    indexes::UnitRange, \n    boundary, \n    adjacent_vertices)\n\nThe magnitude the internal angle in radians between two adjacent edges is from pi radians - i.e. how far away the two edges are from representing a straight boundary. If a boundary is straight then the objective is 0.0 radians, whereas if the boundary has an internal angle of pi/4 radians (45 deg) the objective is pi*3/4 (135 deg). Computes this by considering the angle between the two vectors that can be used to represent the edges (using AngleBetweenVectors.jl).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi","text":"internal_angle_from_pi(index::Int, boundary, edge_clock, edge_anti, relative_magnitude)\n\nAlternate method of LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi for the internal angle at only one vertex.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.iterativeboundary_init","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.iterativeboundary_init","text":"iterativeboundary_init(bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    p::NamedTuple, \n    ind1::Int, \n    ind2::Int,\n    initial_num_points::Int,\n    biv_opt_is_ellipse_analytical::Bool,\n    radial_start_point_shift::Float64,\n    ellipse_sqrt_distortion::Float64,\n    ellipse_confidence_level::Float64,\n    use_ellipse::Bool,\n    save_internal_points::Bool,\n    find_zero_atol::Real, \n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nFinds the initial boundary of IterativeBoundaryMethod, containing initial_num_points, returning it and initialised parameter values. \n\nIf initial_num_points is equal to num_points then the desired number of boundary points have been found. If use_ellipse = true the boundary will be equivalent to the boundary found by RadialMLEMethod with the same parameter settings - it's value informs the method of LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE! used.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.newboundarypoint!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.newboundarypoint!","text":"newboundarypoint!(p::NamedTuple, \n    point_is_on_bounds::BitVector, \n    edge_anti_on_bounds::BitVector, \n    boundary::Matrix{Float64}, \n    boundary_all::Matrix{Float64}, \n    internal_all::Matrix{Float64}, \n    ll_values::Vector{Float64}, \n    internal_count::Int,\n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    edge_anti::Vector{Int}, \n    num_vertices::Int, \n    ind1::Int, \n    ind2::Int,\n    biv_opt_is_ellipse_analytical::Bool, \n    ve1::Int, \n    ve2::Int, \n    relative_magnitude::Float64, \n    bound_warning::Bool, \n    save_internal_points::Bool)\n\nMethod for trying to find a new boundary point for IterativeBoundaryMethod given a starting boundary polygon. Returns whether finding a new point was successful.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.heapupdates_success!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.heapupdates_success!","text":"heapupdates_success!(edge_heap::TrackingHeap,\n    angle_heap::TrackingHeap, \n    edge_clock::Vector{Int},\n    edge_anti::Vector{Int},\n    point_is_on_bounds::BitVector,\n    edge_anti_on_bounds::BitVector,\n    boundary::Matrix{Float64},\n    num_vertices::Int,\n    vi::Int, \n    adj_vertex::Int,\n    relative_magnitude::Float64,\n    clockwise_from_vi=false)\n\nIf finding a new boundary point for IterativeBoundaryMethod was successful, update the datastructures that represent the boundary as required.        \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.heapupdates_failure!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.heapupdates_failure!","text":"heapupdates_failure!(edge_heap::TrackingHeap,\n    angle_heap::TrackingHeap, \n    edge_clock::Vector{Int},\n    edge_anti::Vector{Int},\n    point_is_on_bounds::BitVector,\n    boundary::Matrix{Float64},\n    num_vertices::Int,\n    ve1::Int,\n    ve2::Int,\n    opposite_edge_ve1::Int,\n    model::LikelihoodModel,\n    ind1::Int, \n    ind2::Int,\n    relative_magnitude::Float64)\n\nIf finding a new boundary point for IterativeBoundaryMethod was not successful, update the datastructures that represent the boundary as required. Failure means it is likely that multiple level sets exist. If so, break the edges of the candidate point and opposite_edge_ve1 and reconnect the vertexes such that there are now multiple boundary polygons.\n\nIf there are only one or two points on one of these boundary polygons, display an info message as no additional points can be found from the method directly.\n\nIf there are three or more points on these boundary polygons, then there should be no problems finding other parts of these polygons.\n\nIf the largest polygon has less than three points the method will display a warning message and terminate, returning the boundary found up until then. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.polygon_break_and_rejoin!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.polygon_break_and_rejoin!","text":"polygon_break_and_rejoin!(edge_clock::Vector{Int},\n    edge_anti::Vector{Int},\n    ve1::Int,\n    ve2::Int,\n    opposite_edge_ve1::Int,\n    opposite_edge_ve2::Int,\n    model::LikelihoodModel,\n    ind1::Int, \n    ind2::Int)\n\nIf finding a new boundary point was not successful, breaks and rejoins the boundary polygon as required to remove the vertices from the main polygon that are likely to be on a distinct level set (boundary).\n\nDisplay an info message if only one vertex was seperated, as that vertex can no longer be used to find additional points within the algorithm.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_iterativeboundary","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_iterativeboundary","text":"bivariate_confidenceprofile_iterativeboundary(bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    consistent::NamedTuple, \n    ind1::Int, \n    ind2::Int,\n    dof::Int,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    initial_num_points::Int,\n    angle_points_per_iter::Int,\n    edge_points_per_iter::Int,\n    radial_start_point_shift::Float64,\n    ellipse_sqrt_distortion::Float64,\n    ellipse_confidence_level::Float64,\n    use_ellipse::Bool,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    find_zero_atol::Real, \n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nImplementation of IterativeBoundaryMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Vectorsearch-Methods","page":"Bivariate Functions","title":"Vectorsearch Methods","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"For the RadialRandomMethod, RadialMLEMethod and SimultaneousMethod.","category":"page"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.generatepoint\nLikelihoodBasedProfileWiseAnalysis.findNpointpairs_simultaneous!\nLikelihoodBasedProfileWiseAnalysis.find_m_spaced_radialdirections\nLikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialrandom!\nLikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.generatepoint","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.generatepoint","text":"generatepoint(model::LikelihoodModel, ind1::Int, ind2::Int)\n\nGenerates a uniform random x and y value between the lower and upper bounds for the parameters at ind1 and ind2.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.findNpointpairs_simultaneous!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_simultaneous!","text":"findNpointpairs_simultaneous!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    ind1::Int, \n    ind2::Int,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    biv_opt_is_ellipse_analytical::Bool,\n    min_proportion_unique::Real,\n    use_MLE_point::Bool,\n    optimizationsettings::OptimizationSettings)\n\nImplementation of finding pairs of points that bracket the bivariate confidence boundary for SimultaneousMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.find_m_spaced_radialdirections","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.find_m_spaced_radialdirections","text":"find_m_spaced_radialdirections(num_directions::Int; start_point_shift::Float64=rand())\n\nReturns num_directions equally spaced anticlockwise angles between 0 and 2pi which are shifted by start_point_shift * 2.0 / convert(Float64, num_directions).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialrandom!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialrandom!","text":"findNpointpairs_radialrandom!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    num_directions::Int, \n    ind1::Int, \n    ind2::Int,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    biv_opt_is_ellipse_analytical::Bool, \n    use_MLE_point::Bool,\n    optimizationsettings::OptimizationSettings)\n\nImplementation of finding pairs of points that bracket the bivariate confidence boundary for RadialRandomMethod.\n\nDistorts uniformly spaced anticlockwise angles on a circle using LikelihoodBasedProfileWiseAnalysis.find_m_spaced_radialdirections to angles on an ellipse representative of the relative magnitude of each parameter. If the magnitude of a parameter is a NaN value (i.e. either bound is Inf), then the relative magnitude is set to 1.0, as no information is known about its magnitude.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch","text":"bivariate_confidenceprofile_vectorsearch(bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    consistent::NamedTuple, \n    ind1::Int, \n    ind2::Int,\n    dof::Int,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    find_zero_atol::Real, \n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel;\n    num_radial_directions::Int=0,\n    min_proportion_unique::Real=1.0,\n    use_MLE_point::Bool=false,\n    ellipse_confidence_level::Float64=-1.0,\n    ellipse_start_point_shift::Float64=0.0,\n    ellipse_sqrt_distortion::Float64=0.0)\n\nImplementation of [AbstractBivariateVectorMethod] boundary search methods SimultaneousMethod, RadialMLEMethod and RadialRandomMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Fix1Axis-Method","page":"Bivariate Functions","title":"Fix1Axis Method","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"For Fix1AxisMethod.","category":"page"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_fix1axis!\nLikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_fix1axis","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.findNpointpairs_fix1axis!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.findNpointpairs_fix1axis!","text":"findNpointpairs_fix1axis!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    i::Int, \n    j::Int,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    biv_opt_is_ellipse_analytical::Bool,\n    channel::RemoteChannel)\n\nImplementation of finding pairs of points that bracket the bivariate confidence boundary for Fix1AxisMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_fix1axis","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_fix1axis","text":"bivariate_confidenceprofile_fix1axis(bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    consistent::NamedTuple, \n    ind1::Int, \n    ind2::Int,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    mle_targetll::Float64,\n    save_internal_points::Bool, \n    find_zero_atol::Real, \n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nImplementation of Fix1AxisMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Continuation-Method","page":"Bivariate Functions","title":"Continuation Method","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"For ContinuationMethod.","category":"page"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.update_targetll!\nLikelihoodBasedProfileWiseAnalysis.normal_vector_i_2d!\nLikelihoodBasedProfileWiseAnalysis.continuation_line_search!\nLikelihoodBasedProfileWiseAnalysis.continuation_inwards_radial_search!\nLikelihoodBasedProfileWiseAnalysis.initial_continuation_solution!\nLikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_continuation\nLikelihoodBasedProfileWiseAnalysis.star_obj\nLikelihoodBasedProfileWiseAnalysis.boundary_smoother!\nLikelihoodBasedProfileWiseAnalysis.refine_search_directions!","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.update_targetll!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.update_targetll!","text":"update_targetll!(p::NamedTuple, target_confidence_ll::Float64)\n\nUpdates p.targetll to target_confidence_level, returning p. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.normal_vector_i_2d!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.normal_vector_i_2d!","text":"normal_vector_i_2d!(gradient_i, index, points)\n\nFirst order finite difference approximation of the normal vector at a point, using the location of the point on each side of it to construct a line and find the vector normal to the line, saving these in place in gradient_i.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.continuation_line_search!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.continuation_line_search!","text":"continuation_line_search!(p::NamedTuple, \n    point_is_on_bounds::BitVector,\n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int,\n    ind1::Int, \n    ind2::Int,\n    target_confidence_ll::Float64, \n    search_directions::Matrix{Float64},\n    start_level_set_2D::Matrix{Float64}, \n    find_zero_atol::Real,\n    channel::RemoteChannel;\n    start_level_set_all::Matrix{Float64}=zeros(0,0),\n    level_set_not_smoothed::Bool=true,\n    is_a_zero::BitVector=falses(num_points))\n\nImplementation of the outward 'continuation' search of ContinuationMethod for getting from a lower log-likelihood threshold level set to a higher level set. \n\np, point_is_on_bounds and search_directions are mutated by this function.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.continuation_inwards_radial_search!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.continuation_inwards_radial_search!","text":"continuation_inwards_radial_search!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    ind1::Int, \n    ind2::Int, \n    target_confidence_ll::Float64,\n    search_directions::Matrix{Float64},\n    start_level_set_2D::Matrix{Float64},\n    is_a_zero::BitVector,\n    find_zero_atol::Real,\n    channel::RemoteChannel)\n\nImplementation of the inwards radial search for an initial level set at target_confidence_ll given an initial ellipse solution for ContinuationMethod. The search_directions for each point is a vector between the maximum likelihood estimate point in interest parameter space and the ellipse solution.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.initial_continuation_solution!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.initial_continuation_solution!","text":"initial_continuation_solution!(p::NamedTuple, \n    bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    ind1::Int, \n    ind2::Int,\n    profile_type::AbstractProfileType,\n    ellipse_confidence_level::Float64,\n    target_confidence_ll::Float64,\n    ellipse_start_point_shift::Float64, \n    find_zero_atol::Real,\n    channel::RemoteChannel)\n\nFinds the initial continuation level set of ContinuationMethod.\n\nThe initial ellipse solution found using EllipseSampling.jl should be in the feasible region, contained within the bounds specified for interest parameters. A warning is raised if it is not - it may cause some unexpected behaviour if the parameter is meant to be ≥ 0, yet is allowed to start there in the initial ellipse solution.\n\nWe use the extrema of the true log likelihoods (for profile_type) of the initial ellipse solution to decide how we search for the first level set. We have three cases, where case one is preferred and warnings are raised for both case two and three.\n\nIf min ll > than target ll of the target confidence level. Then line search from initial ellipse to ll boundary defined by min ll and this is the starting continuation solution. Line search radially from the MLE point.\nIf max ll < than target ll of the target confidence level. Line search radially towards the MLE point from the ellipse to the target confidence level boundary and this is the final continuation solution.\nIf min ll and max ll bracket the target confidence level. Then line search radially towards the mle solution from initial ellipse to ll boundary defined by max ll and this is the starting continuation solution.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_continuation","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_continuation","text":"bivariate_confidenceprofile_continuation(bivariate_optimiser::Function, \n    model::LikelihoodModel, \n    num_points::Int, \n    consistent::NamedTuple, \n    ind1::Int, \n    ind2::Int,\n    dof::Int,\n    profile_type::AbstractProfileType,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    ellipse_confidence_level::Float64, \n    target_confidence_level::Float64,\n    ellipse_start_point_shift::Float64,\n    num_level_sets::Int,\n    level_set_spacing::Symbol,\n    mle_targetll::Float64,\n    save_internal_points::Bool,\n    find_zero_atol::Real,\n    optimizationsettings::OptimizationSettings,\n    channel::RemoteChannel)\n\nImplementation of ContinuationMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.star_obj","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.star_obj","text":"star_obj(centers, points)\n\nReturns an objective for each center in centers for how many boundary points can be seen by center without a edge of the boundary point polygon blocking the view.\n\ncenters have 2D points stored in columns. points have 2D boundary points stored in columns and are in either clockwise or anticlockwise order of polygon connection.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.boundary_smoother!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.boundary_smoother!","text":"smooth_boundary!(points, point_is_on_bounds)\n\npoints has 2D boundary points stored in columns and are in either clockwise or anticlockwise order of polygon connection. points is edited in place.\n\nPoints that we know are on the provided bounds are not moved/smoothed, although their presence will impact what the smoother tries to do.\n\nSmoother must attempt to ensure that smoothed points don't go outside the level set boundary???? - may need to evaluate ll for every smoothed point to guarantee this.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.refine_search_directions!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.refine_search_directions!","text":"refine_search_directions!(search_directions, points, point_is_on_bounds; k_means=9, sample_in_polygon=true, verbose=false)\n\nCreates new search directions to search for the next level set for, given the current level set defined by points.\n\nUse kmeans with k_means clusters on either: the polygon boundary points or points sampled within the polygon boundary.\n\nDefine objectives for the cluster points: how close each point is to being a star point, e.g. by finding which cluster point can see the most boundary points (i.e. there isn’t a edge in the way) - this is a form of regularisation by discretisation; we’re assuming that the boundary defined by a polygon with straight lines between boundary points is relatively consistent with the true shape. Additionally, we assume that our boundary points are relatively well spaced out so that the objective isn’t biased by a ton of our known points being located in a specific portion of the boundary.\n\nIf we find a star point (or multiple) then we should use that point to push out radially (e.g. instead of the MLE point). Note: If boundary is convex, all points in our set are star points by definition. If concave if star point exists use that for continuation, else use kmeans points which are likely to be star points for their local sections\n\nThe second objective we can use to tie break is how close each cluster point is to the centre of the polygon boundary (e.g. just using a simple how many points of all points (either the boundary points OR points that are sampled in the boundary) are on either side in x and y axis of the cluster point (and how close these are to 50%).\n\nIf star point doesn't exist then use each individual kmeans point as the point to push out from based on the cluster the boundary point belongs to.\n\nReturns whether or not a star point was found (if not, cannot guarantee that the ordering of boundary points will stay the same, and require a TSP iteration after solving for the next level set).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Sampling-Internal-Points-From-Boundaries","page":"Bivariate Functions","title":"Sampling Internal Points From Boundaries","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.construct_polygon_hull\nLikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull\nLikelihoodBasedProfileWiseAnalysis.update_biv_dict_internal!\nLikelihoodBasedProfileWiseAnalysis.sample_internal_points_LHC\nLikelihoodBasedProfileWiseAnalysis.sample_internal_points_uniform_random\nLikelihoodBasedProfileWiseAnalysis.sample_internal_points_single_row","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.construct_polygon_hull","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.construct_polygon_hull","text":"construct_polygon_hull(model::LikelihoodModel,\n    θindices::Vector{<:Int},\n    conf_struct::BivariateConfidenceStruct,\n    confidence_level::Float64,\n    dof::Int,\n    boundary_not_ordered::Bool,\n    hullmethod::AbstractBivariateHullMethod,\n    return_boundary_not_mesh::Bool)\n\nConstructs a 2D polygon hull that represents an approximation of a true bivariate log-likelihood confidence boundary, given boundary points and saved internal points in conf_struct, using hullmethod. Optionally returns the boundary as an ordered 2*n array or as a SimpleMesh. For a description of the algorithms used for each AbstractBivariateHullMethod see their docstrings: ConvexHullMethod, ConcaveHullMethod and MPPHullMethod.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull","text":"bivariate_concave_hull(points::AbstractArray{Float64}, \n    ll::Vector{<:Float64},\n    min_proportion_to_keep::Real, \n    min_scaling_from_desired_ll::Real, \n    target_ll::Float64,\n    sample_type::AbstractSampleType=LatinHypercubeSamples())\n\nThe implementation of ConcaveHullMethod(), largely intended for use with dimensional sampling point clouds, but available for use with other methods as well. Uses a heuristic defined number of neighbours with the concave hull algorithm from ConcaveHull.jl on the collection of points given by both boundary and saved internal points, returning the approximate boundary polygon hull as a 2*n array.\n\n\n\n\n\nbivariate_concave_hull(sampled_struct::SampledConfidenceStruct, \n    θindices::Vector{Int},\n    min_proportion_to_keep::Real, \n    min_scaling_from_desired_ll::Real, \n    target_ll::Float64, \n    sample_type::AbstractSampleType)\n\nMethod which unpacks a sampled_struct into the format required to call the other method of LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.update_biv_dict_internal!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.update_biv_dict_internal!","text":"update_biv_dict_internal!(model::LikelihoodModel,\n    biv_row_number::Int,\n    points::PointsAndLogLikelihood)\n\nUpdates the internal_points field of a BivariateConfidenceStruct, for the profile related to biv_row_number stored at model.biv_profiles_dict[biv_row_number], with the internal points stored in points.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.sample_internal_points_LHC","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.sample_internal_points_LHC","text":"sample_internal_points_LHC(model::LikelihoodModel, \n    target_num_points::Int, \n    θindices::Tuple{Int,Int}, \n    profile_type::AbstractProfileType,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    conf_struct::BivariateConfidenceStruct, \n    confidence_level::Float64,\n    dof::Int,\n    boundary_not_ordered::Bool,\n    hullmethod::AbstractBivariateHullMethod,\n    use_threads::Bool)\n\nGiven a hullmethod of type AbstractBivariateHullMethod which creates a 2D polygon hull from a set of boundary and internal points (method dependent) in conf_struct as a representation of the true confidence boundary, sample points from the bounding box, with edges parallel to x and y axes, of a 2D polygon hull using a heuristically optimised Latin Hypercube sampling plan to find approximately target_num_points within the polygon, rejecting any that are not inside the log-likelihood threshold at that confidence_level, dof and profile_type.   \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.sample_internal_points_uniform_random","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.sample_internal_points_uniform_random","text":"sample_internal_points_uniform_random(model::LikelihoodModel, \n    num_points::Int, \n    θindices::Tuple{Int,Int}, \n    profile_type::AbstractProfileType,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    conf_struct::BivariateConfidenceStruct, \n    confidence_level::Float64,\n    dof::Int,\n    boundary_not_ordered::Bool,\n    hullmethod::AbstractBivariateHullMethod,\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool)\n\nGiven a hullmethod of type AbstractBivariateHullMethod which creates a 2D polygon hull from a set of boundary and internal points (method dependent) in conf_struct as a representation of the true confidence boundary, sample points from the polygon hull homogeneously until num_points are found, rejecting any that are not inside the log-likelihood threshold at that confidence_level, dof and profile_type.   \n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.sample_internal_points_single_row","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.sample_internal_points_single_row","text":"sample_internal_points_single_row(model::LikelihoodModel,\n    sub_df::Union{DataFrame, SubDataFrame},\n    i::Int,\n    biv_row_number::Int, \n    num_points::Int, \n    sample_type::AbstractSampleType,\n    hullmethod::AbstractBivariateHullMethod, \n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    t::Union{AbstractVector,Missing},\n    evaluate_predictions_for_samples::Bool,\n    proportion_of_predictions_to_keep::Real,\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nSample internal points from the bivariate profile given by a valid row number in model.biv_profiles_df using either homogeneous sampling (UniformRandomSamples()) to find exactly num_points or a Latin Hypercube sampling plan (LatinHypercubeSamples()) to find approximately num_points.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#Merging-Boundaries-From-Multiple-Methods","page":"Bivariate Functions","title":"Merging Boundaries From Multiple Methods","text":"","category":"section"},{"location":"internal_library/bivariate/","page":"Bivariate Functions","title":"Bivariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.predictions_can_be_merged\nLikelihoodBasedProfileWiseAnalysis.rebuild_bivariate_datastructures!","category":"page"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.predictions_can_be_merged","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.predictions_can_be_merged","text":"predictions_can_be_merged(model::LikelihoodModel, rows::AbstractVector{<:Int})\n\nReturns true if all the prediction struct arrays have the same size in the 1st (and 3rd if relevant) dimensions. Return false otherwise.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/bivariate/#LikelihoodBasedProfileWiseAnalysis.rebuild_bivariate_datastructures!","page":"Bivariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.rebuild_bivariate_datastructures!","text":"rebuild_bivariate_datastructures!(model::LikelihoodModel)\n\nRebuilds all the bivariate datastructures so that the :rowind of rows in `model.bivprofiles_df` begins at 1 and increases in increments of 1, updating dictionary keys with the new row indexes. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#Common-Functions","page":"Common Functions","title":"Common Functions","text":"","category":"section"},{"location":"internal_library/common/","page":"Common Functions","title":"Common Functions","text":"Pages = [\"common.md\"]","category":"page"},{"location":"internal_library/common/#Utility-and-Log-likelihood-Thresholds","page":"Common Functions","title":"Utility and Log-likelihood Thresholds","text":"","category":"section"},{"location":"internal_library/common/","page":"Common Functions","title":"Common Functions","text":"LikelihoodBasedProfileWiseAnalysis.convertθnames_toindices\nLikelihoodBasedProfileWiseAnalysis.ll_correction\nLikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood\nLikelihoodBasedProfileWiseAnalysis.get_consistent_tuple","category":"page"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.convertθnames_toindices","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.convertθnames_toindices","text":"convertθnames_toindices(model::LikelihoodModel, \n    θnames_to_convert::Vector{<:Symbol})\n\nConverts a vector of symbols representing parameters in model to a vector of each symbol's corresponding index in model.core.θnames.\n\n\n\n\n\nconvertθnames_toindices(model::LikelihoodModel, \n    θnames_to_convert::Union{Vector{Vector{Symbol}}, Vector{Tuple{Symbol, Symbol}}})\n\nConverts a vector of vectors or tuples containing symbols representing parameters in model to a vector of vectors containing each symbol's corresponding index in model.core.θnames.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.ll_correction","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.ll_correction","text":"ll_correction(model::LikelihoodModel, \n    profile_type::AbstractProfileType, \n    ll::Float64)\n\nIf a profile_type is LogLikelihood(), it corrects ll such that an input log-likelihood value (which has value of zero at the MLE) will now have a value of model.core.maximisedmle at the MLE. Otherwise, a copy of ll is returned, as both ellipse approximation profile types have a log-likelihood value of 0.0 at the MLE.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood","text":"get_target_loglikelihood(model::LikelihoodModel, \n    confidence_level::Float64, \n    profile_type::AbstractProfileType, \n    dof::Int)\n\nReturns the target log-likelihood / threshold at a confidence level and degrees of freedom, dof (typically the number of interest parameters OR the number of model parameters), required for a particular profile_type to be in the confidence set. Uses LikelihoodBasedProfileWiseAnalysis.ll_correction.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.get_consistent_tuple","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_consistent_tuple","text":"get_consistent_tuple(model::LikelihoodModel, \n    confidence_level::Float64, \n    profile_type::AbstractProfileType, \n    dof::Int)\n\nReturns a tuple containing the values needed for log-likelihood evaluation and finding function zeros, including the target log-likelihood, number of model parameters, log-likelihood function to use and data tuple for evaluating the log-likelihood function.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#DataFrame-Subsets","page":"Common Functions","title":"DataFrame Subsets","text":"","category":"section"},{"location":"internal_library/common/","page":"Common Functions","title":"Common Functions","text":"LikelihoodBasedProfileWiseAnalysis.desired_df_subset","category":"page"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.desired_df_subset","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.desired_df_subset","text":"desired_df_subset(df::DataFrame, \n    num_used_rows::Int, \n    confidence_levels::Union{Float64, Vector{<:Float64}},\n    dofs::Union{Int, Vector{<:Int}}, \n    sample_types::Vector{<:AbstractSampleType}; \n    sample_dimension::Int=0, \n    regions::Union{Real, Vector{<:Real}}=Float64[],\n    for_prediction_generation::Bool=false, \n    for_prediction_plots::Bool=false, \n    include_higher_confidence_levels::Bool=false)\n\nReturns a view of df that includes only valid rows ∈ 1:num_used_rows, and rows that contain all of the values specified within function arguments. For dimensional samples.\n\nArguments\n\ndf: a DataFrame - model.dim_samples_df.\nnum_used_rows: the number of valid rows in df - model.num_dim_samples.\nconfidence_levels: a vector of confidence levels or a Float64 of a single confidence level. If empty, all confidence levels in df are allowed. Otherwise, if include_higher_confidence_levels == true and confidence_levels is a Float64, all confidence levels greater than or equal to confidence_levels are allowed. Else, only matching confidence levels in df are allowed.\ndofs: a vector of integer degrees of freedom or a Int of a single degree of freedom. If empty, all degrees of freedom for dimensional profiles are allowed. Otherwise, only matching degrees of freedom in df are allowed.\nsample_types: a vector of AbstractSampleType structs. If empty, all sample types in df are allowed. Otherwise, only matching sample types in df are allowed.\n\nKeyword Arguments\n\nsample_dimension: an integer greater than or equal to 0; if non-zero only matching dimensions of interest parameters in df are allowed, otherwise all are allowed. Default is 0.\nregions: a vector of Real numbers ∈ [0, 1] or a single Real number specifying the regions in df that are allowed. If empty, all regions are allowed. Otherwise, only matching regions in df are allowed. Default is Float64[].\nfor_prediction_generation: a boolean specifying whether only rows which have not had predictions evaluated are allowed. As predictions do not need to be generated for rows which already have them evaluated. \nfor_prediction_plots: a boolean specifying whether only rows which have had predictions evaluated are allowed. As prediction plots can only include rows which have evaluated predictions. \ninclude_higher_confidence_levels: a boolean specifying whether all confidence levels greater than or equal to confidence_levels are allowed. Useful for prediction plots as a dimensional sample can be evaluated at a high confidence level (e.g. 0.95) and then used at a lower confidence level (e.g. 0.9), extracting only the sample points that are in the 0.9 confidence set.\n\n\n\n\n\ndesired_df_subset(df::DataFrame, \n    num_used_rows::Int, \n    θs_of_interest::Vector{<:Int}, \n    confidence_levels::Union{Float64, Vector{<:Float64}}, \n    dofs::Union{Int, Vector{<:Int}}, \n    profile_types::Vector{<:AbstractProfileType}; \n    regions::Union{Real, Vector{<:Real}}=Float64[],\n    for_points_in_interval::Tuple{Bool,Int,Real}=(false,0,0), \n    for_prediction_generation::Bool=false, \n    for_prediction_plots::Bool=false)\n\nReturns a view of df that includes only valid rows ∈ 1:num_used_rows, and rows that contain all of the values specified within function arguments. For univariate profiles.\n\nArguments\n\ndf: a DataFrame - model.uni_profiles_df.\nnum_used_rows: the number of valid rows in df - model.num_uni_profiles.\nconfidence_levels: a vector of confidence levels or a Float64 of a single confidence level. If empty, all confidence levels in df are allowed. Otherwise, only matching confidence levels in df are allowed.\ndofs: a vector of integer degrees of freedom or a Int of a single degree of freedom. If empty, all degrees of freedom for univariate profiles are allowed. Otherwise, only matching degrees of freedom in df are allowed.\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types in df are allowed. Otherwise, only matching profile types in df are allowed.\n\nKeyword Arguments\n\nregions: a vector of Real numbers ∈ [0, 1] or a single Real number specifying the regions in df that are allowed. If empty, all regions are allowed. Otherwise, only matching regions in df are allowed. Default is Float64[].\nfor_points_in_interval: a tuple used for only extracting the rows that need to have points in the confidence interval evaluated by get_points_in_intervals!. Default is (false, 0, 0).\nfor_prediction_generation: a boolean specifying whether only rows which have not had predictions evaluated are allowed. As predictions do not need to be generated for rows which already have them evaluated. \nfor_prediction_plots: a boolean specifying whether only rows which have had predictions evaluated are allowed. As prediction plots can only include rows which have evaluated predictions. \n\n\n\n\n\ndesired_df_subset(df::DataFrame, \n    num_used_rows::Int, \n    θs_of_interest::Vector{Tuple{Int,Int}}, \n    confidence_levels::Union{Float64, Vector{<:Float64}}, \n    dofs::Union{Int, Vector{<:Int}}, \n    profile_types::Vector{<:AbstractProfileType}, \n    methods::Vector{<:AbstractBivariateMethod}=AbstractBivariateMethod[];\n    regions::Union{Real, Vector{<:Real}}=Float64[], \n    for_prediction_generation::Bool=false, \n    for_prediction_plots::Bool=false, \n    include_lower_confidence_levels::Bool=false)\n\nReturns a view of df that includes only valid rows ∈ 1:num_used_rows, and rows that contain all of the values specified within function arguments. For bivariate profiles.\n\nArguments\n\ndf: a DataFrame - model.biv_profiles_df.\nnum_used_rows: the number of valid rows in df - model.num_biv_profiles.\nconfidence_levels: a vector of confidence levels or a Float64 of a single confidence level. If empty, all confidence levels in df are allowed. Otherwise, if include_lower_confidence_levels == true and confidence_levels is a Float64, all confidence levels less than or equal to confidence_levels are allowed. Else, only matching confidence levels in df are allowed.\ndofs: a vector of integer degrees of freedom or a Int of a integer degree of freedom. If empty, all degrees of freedom for bivariate profiles are allowed. Otherwise, only matching degrees of freedom in df are allowed.\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types in df are allowed. Otherwise, only matching profile types in df are allowed.\nmethods: a vector of AbstractBivariateMethod structs. If empty, all methods in df are allowed. Otherwise, only methods of the same type in df are allowed.\n\nKeyword Arguments\n\nregions: a vector of Real numbers ∈ [0, 1] or a single Real number specifying the regions in df that are allowed. If empty, all regions are allowed. Otherwise, only matching regions in df are allowed. Default is Float64[].\nfor_prediction_generation: a boolean specifying whether only rows which have not had predictions evaluated are allowed. As predictions do not need to be generated for rows which already have them evaluated. \nfor_prediction_plots: a boolean specifying whether only rows which have had predictions evaluated are allowed. As prediction plots can only include rows which have evaluated predictions. \nremove_combined_method: a boolean specifiying whether rows with method of type CombinedBivariateMethod should be removed. Needed by combine_bivariate_boundaries! to ensure that combined boundaries are not removed from df.\ninclude_lower_confidence_levels: a boolean specifying whether all confidence levels less than or equal to confidence_levels are allowed. Useful for prediction plots if a given set of bivariate profiles has few internal points evaluated, meaning some information about predictions may be missing. Bivariate profiles at lower confidence levels are by definition inside the desired confidence profile and may provide additional information on predictions.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#Nuisance-Parameters-and-Array-Mapping","page":"Common Functions","title":"Nuisance Parameters and Array Mapping","text":"","category":"section"},{"location":"internal_library/common/","page":"Common Functions","title":"Common Functions","text":"LikelihoodBasedProfileWiseAnalysis.variablemappingranges\nLikelihoodBasedProfileWiseAnalysis.variablemapping!\nLikelihoodBasedProfileWiseAnalysis.boundsmapping!\nLikelihoodBasedProfileWiseAnalysis.init_nuisance_parameters\nLikelihoodBasedProfileWiseAnalysis.correct_θbounds_nuisance","category":"page"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.variablemappingranges","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.variablemappingranges","text":"variablemappingranges(num_pars::T, index::T) where T <: Int\n\nReturns two tuples of ranges that map variables in nuisance parameter space (ω) to their corresponding indexes in parameter space (θ) given an interest parameter ψ at index.\n\n\n\n\n\nvariablemappingranges(num_pars::T, index1::T, index2::T) where T <: Int\n\nReturns two tuples of ranges that map variables in nuisance parameter space (ω) to their corresponding indexes in parameter space (θ), given interest parameters ψ at index1 and index2 and index1 < index2.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.variablemapping!","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.variablemapping!","text":"variablemapping!(θ::Union{Vector, SubArray}, ω::Union{Vector, SubArray}, θranges::Tuple{T, T}, ωranges::Tuple{T, T}) where T <: UnitRange\n\nModifies the array θ in place, mapping the variable values in the nuisance parameter array ω to their corresponding indexes in the parameter array θ, where the ranges are determined by LikelihoodBasedProfileWiseAnalysis.variablemappingranges. For one interest parameter.\n\n\n\n\n\nvariablemapping!(θ::Union{Vector, SubArray}, ω::Union{Vector, SubArray}, θranges::Tuple{T,T,T}, ωranges::Tuple{T,T,T})) where T <: UnitRange\n\nModifies the array θ in place, mapping the variable values in the nuisance parameter array ω to their corresponding indexes in the parameter array θ, where the ranges are determined by LikelihoodBasedProfileWiseAnalysis.variablemappingranges. For two interest parameters.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.boundsmapping!","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.boundsmapping!","text":"boundsmapping!(newbounds::Vector, bounds::AbstractVector, index::Int)\n\nModifies newbounds in place, mapping all the values in bounds in order into newbounds with the exception of the value at index, which is the interest parameter.\n\n\n\n\n\nboundsmapping!(newbounds::Vector, bounds::AbstractVector, index1::Int, index2::Int)\n\nModifies newbounds in place, mapping all the values in bounds in order into newbounds with the exception of the values at index1 and index2, which are the interest parameters and index1 < index2.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.init_nuisance_parameters","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.init_nuisance_parameters","text":"init_nuisance_parameters(model::LikelihoodModel, index::Int)\n\nInitialises the lower and upper bounds, and initial guess for nuisance parameters using LikelihoodBasedProfileWiseAnalysis.boundsmapping! and ranges that map variables between nuisance parameter and parameter space using LikelihoodBasedProfileWiseAnalysis.variablemappingranges, given an interest parameter at index. The initial guess for nuisance parameters is their corresponding value at the maximum likelihood estimate (model.core.θmle).\n\n\n\n\n\ninit_nuisance_parameters(model::LikelihoodModel, index::Int)\n\nInitialises the lower and upper bounds, and initial guess for nuisance parameters using LikelihoodBasedProfileWiseAnalysis.boundsmapping! and ranges that map variables between nuisance parameter and parameter space using LikelihoodBasedProfileWiseAnalysis.variablemappingranges, given interest parameters at index1 and index2 where index1 < index2. The initial guess for nuisance parameters is their corresponding value at the maximum likelihood estimate (model.core.θmle).\n\n\n\n\n\ninit_nuisance_parameters(model::LikelihoodModel, index::Int)\n\nInitialises the lower and upper bounds, and initial guess for nuisance parameters and indices that map variables between parameter and nuisance parameter space, given interest parameters in θindices. The initial guess for nuisance parameters is their corresponding value at the maximum likelihood estimate (model.core.θmle).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/common/#LikelihoodBasedProfileWiseAnalysis.correct_θbounds_nuisance","page":"Common Functions","title":"LikelihoodBasedProfileWiseAnalysis.correct_θbounds_nuisance","text":"correct_θbounds_nuisance(m::LikelihoodModel,\n    θlb_nuisance::AbstractVector{<:Float64},\n    θub_nuisance::AbstractVector{<:Float64})\n\nMakes sure that nuisance parameter bounds contain the MLE parameter values - if not, set that part of the nuisance parameter bound to the bounds in model.core.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#Predictions","page":"Predictions","title":"Predictions","text":"","category":"section"},{"location":"user_interface/predictions/#Adding-a-Prediction-Function","page":"Predictions","title":"Adding a Prediction Function","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"In the event that a prediction function has not been added to the LikelihoodModel struct yet, we can add one using add_prediction_function!. This prediction function is for model solutions/trajectory and not the additional error we account for when predicting realisations.","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"add_prediction_function!\ncheck_prediction_function_exists","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.add_prediction_function!","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.add_prediction_function!","text":"add_prediction_function!(model::LikelihoodModel, predictfunction::Function)\n\nAdds a prediction function, predictfunction, to model and evaluates the predicted response variable(s) at the data points using the maximum likelihood estimate for model parameters. Modifies model in place.\n\nRequirements for predictfunction\n\nA function to generate model predictions from that is paired with the loglikefunction. \nTakes three arguments, θ, data and t, in that order, where θ and data are the same as for loglikefunction and t needs to be an optional third argument. \nWhen t is not specified, the prediction function should be evaluated for the same time points/independent variable as the data. When t is specified, the prediction function should be evaluated for those specified time points/independent variable. A good practice is to include the time points of the data in the argument, data, so that the function can be specified as predictfunction(θ, data, t=data.t) (here data is a NamedTuple).\nThe output of the function should be a 1D vector when there is a single predicted response variable or a 2D array when there are multiple predicted response variables. \nThe prediction(s) for each response variable should be stored in the columns of the array. In Julia, vectors are stored column-wise, so in the case where there is only one response variable, this will already be the case.\nThe number of rows of each predicted response variable should be the same length as the vector t used to evaluate the response.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.check_prediction_function_exists","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.check_prediction_function_exists","text":"check_prediction_function_exists(model::LikelihoodModel)\n\nChecks if a prediction function is stored in model, returning true if there is. Otherwise false is returned and a warning is logged. Requirements for a prediction function can be seen in add_prediction_function!.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#Adding-an-Error-Function","page":"Predictions","title":"Adding an Error Function","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"Similarly, if a error model function (the data distribution) has not been added to the LikelihoodModel struct yet, we can add one using add_error_function!. This function is used to evaluate region reference intervals around the model trajectory given the data distribution defined as the error model.","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"add_error_function!","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.add_error_function!","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.add_error_function!","text":"add_error_function!(model::LikelihoodModel, errorfunction::Function)\n\nAdds an error function, errorfunction, to model. Modifies model in place. Model must contain a prediction function model.core.predictfunction to add the error function.\n\nRequirements for errorfunction\n\nA function to generate lower and upper confidence quartiles (reference intervals) of each prediction realisation from predictfunction.\nTakes three arguments, predictions, θ, and region in that order, where predictions is the array of predictions generated by predictfunction, θ is the same as for loglikefunction and region is the highest density region to evaluate quartiles of the error at each prediction point. \nThe output of the function should be a lower quartile and a upper quartile array, each with the same dimensions as predictions.\n\nPredefined error functions available to use\n\nnormal_error_σ_known \nnormal_error_σ_estimated\nlognormal_error_σ_known\nlognormal_error_σ_estimated\nlogitnormal_error_σ_known\nlogitnormal_error_σ_estimated\npoisson_error\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#Predefined-Error-models","page":"Predictions","title":"Predefined Error models","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"For convenience, we define example error model functions for four data distributions: Gaussian, log-normal, logit-normal and poisson. We provide versions where the standard deviation parameter σ is known (i.e. fixed at a given value) and where it's estimated (it should be a parameter within the parameter vector).","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"normal_error_σ_known\nnormal_error_σ_estimated\nlognormal_error_σ_known\nlognormal_error_σ_estimated\nlogitnormal_error_σ_known\nlogitnormal_error_σ_estimated\npoisson_error","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.normal_error_σ_known","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.normal_error_σ_known","text":"normal_error_σ_known(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ::Real)\n\nUse a normal error model to quantify the uncertainty in the predictions of realisations, with known value of σ.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with a set value of σ.\n\nTwo equivalent examples of this specification with σ set to 1.3:\n\nerrorfunction(predictions, θ, region) = normal_error_σ_known(predictions, θ, region, 1.3)\n\nfunction errorfunction(predictions, θ, region) \n    normal_error_σ_known(predictions, θ, region, 1.3)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.normal_error_σ_estimated","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.normal_error_σ_estimated","text":"normal_error_σ_estimated(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ_θindex::Int)\n\nUse a normal error model to quantify the uncertainty in the predictions of realisations, where σ is an estimated model parameter in θ.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with the index, σ_θindex, of σ in θ.\n\nTwo equivalent examples of this specification with σ stored at the end of the θ parameter vector, which has 4 elements (length 4):\n\nerrorfunction(predictions, θ, region) = normal_error_σ_estimated(predictions, θ, region, 4)\n\nfunction errorfunction(predictions, θ, region) \n    normal_error_σ_estimated(predictions, θ, region, 4)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.lognormal_error_σ_known","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.lognormal_error_σ_known","text":"lognormal_error_σ_known(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ::Real)\n\nUse a log normal error model to quantify the uncertainty in the predictions of realisations, with known value of σ.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with a set value of σ.\n\nTwo equivalent examples of this specification with σ set to 1.3:\n\nerrorfunction(predictions, θ, region) = lognormal_error_σ_known(predictions, θ, region, 1.3)\n\nfunction errorfunction(predictions, θ, region) \n    lognormal_error_σ_known(predictions, θ, region, 1.3)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.lognormal_error_σ_estimated","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.lognormal_error_σ_estimated","text":"lognormal_error_σ_estimated(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ_θindex::Int)\n\nUse a log normal error model to quantify the uncertainty in the predictions of realisations, where σ is an estimated model parameter in θ.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with the index, σ_θindex, of σ in θ.\n\nTwo equivalent examples of this specification with σ stored at the end of the θ parameter vector, which has 4 elements (length 4):\n\nerrorfunction(predictions, θ, region) = lognormal_error_σ_estimated(predictions, θ, region, 4)\n\nfunction errorfunction(predictions, θ, region) \n    lognormal_error_σ_estimated(predictions, θ, region, 4)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.logitnormal_error_σ_known","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.logitnormal_error_σ_known","text":"logitnormal_error_σ_known(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ::Real)\n\nUse a logit-normal error model to quantify the uncertainty in the predictions of realisations, with known value of σ. Predictions are required to be defined ∈ (0,1).\n\nOutput is the correct highest density region for around σ < 1.43 (at all values of predictions). If predictions are closer to 0 or 1.0 than 0.5, then higher values of σ (closer to 2) will be acceptable as the distribution will still be unimodal. Otherwise, the distribution will not be unimodal and won't identify the correct high density regions.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with a set value of σ.\n\nTwo equivalent examples of this specification with σ set to 0.9:\n\nerrorfunction(predictions, θ, region) = logitnormal_error_σ_known(predictions, θ, region, 0.9)\n\nfunction errorfunction(predictions, θ, region) \n    logitnormal_error_σ_known(predictions, θ, region, 0.9)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.logitnormal_error_σ_estimated","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.logitnormal_error_σ_estimated","text":"logitnormal_error_σ_estimated(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64, \n    σ_θindex::Int)\n\nUse a logit-normal error model to quantify the uncertainty in the predictions of realisations, where σ is an estimated model parameter in θ. Predictions are required to be defined ∈ (0,1).\n\nOutput is the correct highest density region for around σ < 1.43 (at all values of predictions). If predictions are closer to 0 or 1.0 than 0.5, then higher values of σ (closer to 2) will be acceptable as the distribution will still be unimodal. Otherwise, the distribution will not be unimodal and won't identify the correct high density regions.\n\nTo use this function as the error model you must create a new function with only the first three arguments, which calls this function with the index, σ_θindex, of σ in θ.\n\nTwo equivalent examples of this specification with σ stored at the end of the θ parameter vector, which has 4 elements (length 4):\n\nerrorfunction(predictions, θ, region) = logitnormal_error_σ_estimated(predictions, θ, region, 4)\n\nfunction errorfunction(predictions, θ, region) \n    logitnormal_error_σ_estimated(predictions, θ, region, 4)\nend\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.poisson_error","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.poisson_error","text":"poisson_error(predictions::AbstractArray, \n    θ::AbstractVector, \n    region::Float64)\n\nUse a poisson error model to quantify the uncertainty in the predictions of realisations, where each prediction is the mean of the error model.\n\nTo use this function as the error model you don't need to create a new function, just specify this function directly.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#Prediction-Generation","page":"Predictions","title":"Prediction Generation","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"Then to generate predictions we can use one of three functions, depending on whether we want to generate predictions from univariate or bivariate profiles, or dimensional samples.","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"generate_predictions_univariate!\ngenerate_predictions_bivariate! \ngenerate_predictions_dim_samples!","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_predictions_univariate!","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.generate_predictions_univariate!","text":"generate_predictions_univariate!(model::LikelihoodModel,\n    t::AbstractVector,\n    proportion_to_keep::Real;\n    <keyword arguments>)\n\nEvalute and save proportion_to_keep individual predictions and their extrema from existing univariate profiles that meet the requirements of the univariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments) at time points t. Modifies model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nt: a vector of time points to compute predictions at.\nproportion_to_keep: a Real number ∈ [0.0,1.0] of the proportion of individual predictions to save. Default is 1.0.\n\nKeyword Arguments\n\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nconfidence_levels: a vector of confidence levels. If empty, all confidence levels of univariate profiles will be considered for evaluating predictions from. Otherwise, only confidence levels in confidence_levels will be considered. Default is Float64[] (any confidence level).\ndofs: a vector of integer degrees of freedom used to define the asymptotic threshold for the extremities of a univariate profile. If empty, all degrees of freedom for univariate profiles will be considered for evaluating predictions from. Otherwise, only degrees of freedom in dofs will be considered. Default is Int[] (any degree of freedom).\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types of univariate profiles are considered. Otherwise, only profiles with matching profile types will be considered. Default is AbstractProfileType[] (any profile type).\noverwrite_predictions: boolean variable specifying whether to re-evaluate and overwrite predictions for univariate profiles that have already had predictions evaluated. Set to true if predictions need to be evaluated for a new vector of time points. Default is false.\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of predictions evaluated and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across univariate profiles. Default is true.\n\nDetails\n\nFor each univariate profile that meets the requirements of LikelihoodBasedProfileWiseAnalysis.desired_df_subset, it uses LikelihoodBasedProfileWiseAnalysis.generate_prediction to generates the predictions for every parameter point in the profiles. The extrema of these predictions are computed (these are approximate simultaneous confidence bands for the prediction mean). The extrema and proportion_to_keep of the individual predictions are saved as a PredictionStruct in model.uni_predictions_dict, where the keys for the dictionary is the row number in model.uni_profiles_df of the corresponding profile.\n\nDistributed Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true then the predictions from each univariate profile will be computed in parallel across Distributed.nworkers() workers.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for a prediction to be evaluated from a single point in parameter space.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_predictions_bivariate!","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.generate_predictions_bivariate!","text":"generate_predictions_bivariate!(model::LikelihoodModel,\n    t::AbstractVector,\n    proportion_to_keep::Real;\n    <keyword arguments>)\n\nEvalute and save proportion_to_keep individual predictions and their extrema from existing bivariate profiles that meet the requirements of the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments) at time points t. Modifies model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nt: a vector of time points to compute predictions at.\nproportion_to_keep: a Real number ∈ [0.0,1.0] of the proportion of individual predictions to save. Default is 1.0.\n\nKeyword Arguments\n\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nconfidence_levels: a vector of confidence levels. If empty, all confidence levels of bivariate profiles will be considered for evaluating predictions from. Otherwise, only confidence levels in confidence_levels will be considered. Default is Float64[] (any confidence level).\ndofs: a vector of integer degrees of freedom used to define the asymptotic threshold for the boundary of a bivariate profile. If empty, all degrees of freedom for bivariate profiles will be considered for evaluating predictions from. Otherwise, only degrees of freedom in dofs will be considered. Default is Int[] (any degree of freedom).\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types of bivariate profiles are considered. Otherwise, only profiles with matching profile types will be considered. Default is AbstractProfileType[] (any profile type).\nmethods: a vector of AbstractBivariateMethod structs. If empty all methods used to find bivariate profiles are considered. Otherwise, only profiles with matching method types will be considered (struct arguments do not need to be the same). Default is AbstractBivariateMethod[] (any bivariate method).\noverwrite_predictions: boolean variable specifying whether to re-evaluate and overwrite predictions for bivariate profiles that have already had predictions evaluated. Set to true if predictions need to be evaluated for a new vector of time points. Default is false.\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of predictions evaluated and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across bivariate profiles. Default is true.\n\nDetails\n\nFor each bivariate profile that meets the requirements of LikelihoodBasedProfileWiseAnalysis.desired_df_subset, it uses LikelihoodBasedProfileWiseAnalysis.generate_prediction to generates the predictions for every parameter point in the profiles. The extrema of these predictions are computed (these are approximate simultaneous confidence bands for the prediction mean). The extrema and proportion_to_keep of the individual predictions are saved as a PredictionStruct in model.biv_predictions_dict, where the keys for the dictionary is the row number in model.biv_profiles_df of the corresponding profile.\n\nDistributed Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true then the predictions from each bivariate profile will be computed in parallel across Distributed.nworkers() workers.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for a prediction to be evaluated from a single point in parameter space.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.generate_predictions_dim_samples!","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.generate_predictions_dim_samples!","text":"generate_predictions_dim_samples!(model::LikelihoodModel,\n    t::AbstractVector,\n    proportion_to_keep::Real;\n    <keyword arguments>)\n\nEvalute and save proportion_to_keep individual predictions and their extrema from existing dimensional samples that meet the requirements of the dimensional method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments) at time points t. Modifies model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nt: a vector of time points to compute predictions at.\nproportion_to_keep: a Real number ∈ [0.0,1.0] of the proportion of individual predictions to save. Default is 1.0.\n\nKeyword Arguments\n\nregion: a Real number ∈ [0, 1] specifying the proportion of the density of the error model from which to evaluate the highest density region. Default is 0.95.\nconfidence_levels: a vector of confidence levels. If empty, all confidence levels of dimensional samples will be considered for evaluating predictions from. Otherwise, only confidence levels in confidence_levels will be considered. Default is Float64[] (any confidence level).\ndofs: a vector of integer degrees of freedom used to define the asymptotic threshold for the boundary of a dimensional sample. If empty, all degrees of freedom for dimensional sample will be considered for evaluating predictions from. Otherwise, only degrees of freedom in dofs will be considered. Default is Int[] (any degree of freedom).\nsample_types: a vector of AbstractSampleType structs. If empty, all sample types used to find dimensional samples are considered. Otherwise, only samples with matching sample types will be considered. Default is AbstractSampleType[] (any sample type).\noverwrite_predictions: boolean variable specifying whether to re-evaluate and overwrite predictions for dimensional samples that have already had predictions evaluated. Set to true if predictions need to be evaluated for a new vector of time points. Default is false.\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of predictions evaluated and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across dimensional samples. Default is true.\n\nDetails\n\nFor each dimensional sample that meets the requirements of LikelihoodBasedProfileWiseAnalysis.desired_df_subset, it uses LikelihoodBasedProfileWiseAnalysis.generate_prediction to generates the predictions for every parameter point in the samples. The extrema of these predictions are computed (these are approximate simultaneous confidence bands for the prediction mean). The extrema and proportion_to_keep of the individual predictions are saved as a PredictionStruct in model.dim_predictions_dict, where the keys for the dictionary is the row number in model.dim_samples_df of the corresponding sample.\n\nDistributed Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true then the predictions from each dimensional sample will be computed in parallel across Distributed.nworkers() workers.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for a prediction to be evaluated from a single point in parameter space.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"To extract predictions give the relevant profile we can use:","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"get_univariate_prediction_set\nget_bivariate_prediction_set\nget_dimensional_prediction_set","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.get_univariate_prediction_set","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.get_univariate_prediction_set","text":"get_uni_prediction_set(model::LikelihoodModel, uni_row_number::Int)\n\nReturns the PredictionStruct struct corresponding to the profile in row uni_row_number of model.uni_profiles_df. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.get_bivariate_prediction_set","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.get_bivariate_prediction_set","text":"get_bivariate_prediction_set(model::LikelihoodModel, biv_row_number::Int)\n\nReturns the PredictionStruct struct corresponding to the profile in row biv_row_number of model.biv_profiles_df. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.get_dimensional_prediction_set","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.get_dimensional_prediction_set","text":"get_dim_prediction_set(model::LikelihoodModel, dim_row_number::Int)\n\nReturns the PredictionStruct struct corresponding to the profile in row dim_row_number of model.dim_samples_df. \n\n\n\n\n\n","category":"function"},{"location":"user_interface/predictions/#Structs","page":"Predictions","title":"Structs","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"Predictions are stored in a PredictionStruct which will also contain a PredictionRealisationsStruct if the corresponding reference tolerance intervals have been evaluated.","category":"page"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"AbstractPredictionStruct\nPredictionStruct\nPredictionRealisationsStruct","category":"page"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.AbstractPredictionStruct","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.AbstractPredictionStruct","text":"AbstractPredictionStruct\n\nSupertype for the predictions storage struct.\n\nSubtypes\n\nPredictionRealisationsStruct\n\nPredictionStruct\n\n\n\n\n\n","category":"type"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.PredictionStruct","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.PredictionStruct","text":"PredictionStruct(predictions::Array{Real}, extrema::Array{Real}, realisations::PredictionRealisationsStruct)\n\nStruct for containing evaluated predictions corresponding to confidence profiles.\n\nFields\n\npredictions: array of model predictions evaluated at the parameters given by a particular confidence profile parameter set. If a model has multiple response variables, it assumes that model.core.predictfunction stores the prediction for each variable in its columns. Values for each response variable are stored in the 3rd dimension (row=dim1, col=dim2, page/sheet=dim3). Each column corresponds to a column of the confidence profile parameter set. \nextrema: extrema of the predictions array.\nrealisations: a PredictionRealisationsStruct struct.\n\nSupertype Hiearachy\n\nPredictionStruct <: AbstractPredictionStruct <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/predictions/#LikelihoodBasedProfileWiseAnalysis.PredictionRealisationsStruct","page":"Predictions","title":"LikelihoodBasedProfileWiseAnalysis.PredictionRealisationsStruct","text":"PredictionRealisationsStruct(lq::Array{<:Real}, uq::Array{<:Real}, extrema::Array{<:Real})\n\nStruct for containing evaluated lower and upper confidence quartiles of the reference tolerance sets for prediction realisations corresponding to confidence profiles.\n\nFields\n\nlq: array of the lower confidence quartile of the error model evaluated at each prediction point in a corresponding PredictionStruct.\nuq: array of the upper confidence quartile of the error model evaluated at each prediction point in a corresponding PredictionStruct.\nextrema: extrema of the lq and uq arrays.\n\nSupertype Hiearachy\n\nPredictionRealisationsStruct <: AbstractPredictionStruct <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/predictions/#Index","page":"Predictions","title":"Index","text":"","category":"section"},{"location":"user_interface/predictions/","page":"Predictions","title":"Predictions","text":"Pages = [\"predictions.md\"]","category":"page"},{"location":"internal_library/univariate/#Univariate-Functions","page":"Univariate Functions","title":"Univariate Functions","text":"","category":"section"},{"location":"internal_library/univariate/","page":"Univariate Functions","title":"Univariate Functions","text":"Pages = [\"univariate.md\"]","category":"page"},{"location":"internal_library/univariate/#Likelihood-Optimisation","page":"Univariate Functions","title":"Likelihood Optimisation","text":"","category":"section"},{"location":"internal_library/univariate/","page":"Univariate Functions","title":"Univariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.univariateψ_ellipse_unbounded\nLikelihoodBasedProfileWiseAnalysis.univariateψ","category":"page"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.univariateψ_ellipse_unbounded","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.univariateψ_ellipse_unbounded","text":"univariateψ_ellipse_unbounded(ψ::Real, p::NamedTuple)\n\nGiven an ellipse approximation of a log-likelihood function (LikelihoodBasedProfileWiseAnalysis.ellipse_loglike) which is unbounded in parameter space, this function finds the values of the nuisance parameters ω that optimise the function at fixed values of the interest parameter ψ and returns the approximated log-likelihood value minus the confidence interval target threshold. The returned function value will be zero at the locations of the approximate confidence interval for ψ, which correspond to the locations found by LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike_1D_soln. Nuisance parameter values are stored in the NamedTuple p. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.univariateψ","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.univariateψ","text":"univariateψ(ψ::Real, p::NamedTuple)\n\nGiven a log-likelihood function (p.consistent.loglikefunction) which is bounded in parameter space and may be an ellipse approximation, this function finds the values of the nuisance parameters ω that optimise the function fixed values of the interest parameter ψ and returns the log-likelihood value minus the confidence interval target threshold. The returned function value will be zero at the locations of the approximate confidence interval for ψ. Nuisance parameter values are stored in the NamedTuple p at p.ω_opt. \n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#Get-Points-in-Confidence-Interval","page":"Univariate Functions","title":"Get Points in Confidence Interval","text":"","category":"section"},{"location":"internal_library/univariate/","page":"Univariate Functions","title":"Univariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.update_uni_dict_internal!\nLikelihoodBasedProfileWiseAnalysis.get_points_in_interval_single_row","category":"page"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.update_uni_dict_internal!","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.update_uni_dict_internal!","text":"update_uni_dict_internal!(model::LikelihoodModel,\n    uni_row_number::Int,\n    points::PointsAndLogLikelihood)\n\nUpdates the interval_points field of a UnivariateConfidenceStruct, for the profile related to uni_row_number stored at model.uni_profiles_dict[uni_row_number], with the interval points stored in points.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.get_points_in_interval_single_row","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_points_in_interval_single_row","text":"get_points_in_interval_single_row(univariate_optimiser::Function, \n    model::LikelihoodModel,\n    num_points_in_interval::Int,\n    θi::Int,\n    profile_type::AbstractProfileType,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    current_interval_points::PointsAndLogLikelihood,\n    additional_width::Real,\n    use_threads::Bool)\n\nMethod for getting num_points_in_interval points inside a confidence interval for parameter θi, directly called by LikelihoodBasedProfileWiseAnalysis.univariate_confidenceinterval and called via it's other method for get_points_in_intervals!. Adds additional_width outside of the confidence interval, so long as a parameter bound is not reached. If a bound is reached, up until the bound will be considered instead.\n\n\n\n\n\nget_points_in_interval_single_row(model::LikelihoodModel,\n    uni_row_number::Int,\n    num_points_in_interval::Int,\n    additional_width::Real,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nAlternate method called by get_points_in_intervals!.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#Main-Confidence-Interval-Logic","page":"Univariate Functions","title":"Main Confidence Interval Logic","text":"","category":"section"},{"location":"internal_library/univariate/","page":"Univariate Functions","title":"Univariate Functions","text":"LikelihoodBasedProfileWiseAnalysis.get_interval_brackets\nLikelihoodBasedProfileWiseAnalysis.add_uni_profiles_rows!\nLikelihoodBasedProfileWiseAnalysis.set_uni_profiles_row!\nLikelihoodBasedProfileWiseAnalysis.get_univariate_opt_func\nLikelihoodBasedProfileWiseAnalysis.univariate_confidenceinterval","category":"page"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.get_interval_brackets","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_interval_brackets","text":"get_interval_brackets(model::LikelihoodModel, \n    θi::Int, \n    confidence_level::Float64, \n    dof::Int,\n    profile_type::AbstractProfileType)\n\nReturns updated interval brackets (Float64 vectors of length two) if smaller or larger confidence level profiles exist for θi at degrees of freedom, dof, such that the region to bracket over for the left and right sides of the confidence interval is smallest. Otherwise, returns empty brackets (Float64[]).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.add_uni_profiles_rows!","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.add_uni_profiles_rows!","text":"add_uni_profiles_rows!(model::LikelihoodModel, num_rows_to_add::Int)\n\nAdds num_rows_to_add free rows to model.uni_profiles_df by vertically concatenating the existing DataFrame and free rows using LikelihoodBasedProfileWiseAnalysis.init_uni_profiles_df.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.set_uni_profiles_row!","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.set_uni_profiles_row!","text":"set_uni_profiles_row!(model::LikelihoodModel, \n    row_ind::Int, \n    θi::Int,\n    not_evaluated_internal_points::Bool, \n    not_evaluated_predictions::Bool,\n    confidence_level::Float64, \n    dof::Int,\n    profile_type::AbstractProfileType, \n    num_points::Int, \n    additional_width::Real)\n\nSets the relevant fields of row row_ind in model.uni_profiles_df after a profile has been evaluated.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.get_univariate_opt_func","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.get_univariate_opt_func","text":"get_univariate_opt_func(profile_type::AbstractProfileType=LogLikelihood())\n\nReturns the correct univariate optimisation function used to for find the optimal values of nuisance parameters for a given interest parameter value for the profile_type log-likelihood function. The optimisation function returns the value of the profile_type log-likelihood function as well as finding the optimal nuisance parameters and saving these in one of it's inputs.\n\nWill be LikelihoodBasedProfileWiseAnalysis.univariateψ for the LogLikelihood() and EllipseApprox() profiles types and LikelihoodBasedProfileWiseAnalysis.univariateψ_ellipse_unbounded for the EllipseApproxAnalytical profile type.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/univariate/#LikelihoodBasedProfileWiseAnalysis.univariate_confidenceinterval","page":"Univariate Functions","title":"LikelihoodBasedProfileWiseAnalysis.univariate_confidenceinterval","text":"univariate_confidenceinterval!(p::Progress,\n    univariate_optimiser::Function, \n    model::LikelihoodModel, \n    consistent::NamedTuple, \n    θi::Int, \n    confidence_level::Float64,\n    dof::Int,\n    profile_type::AbstractProfileType,\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    mle_targetll::Float64,\n    use_existing_profiles::Bool,\n    use_ellipse_approx_analytical_start::Bool,\n    num_points_in_interval::Int,\n    additional_width::Real,\n    find_zero_atol::Real,\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nReturns a UnivariateConfidenceStruct containing the likelihood-based confidence interval for interest parameter θi at confidence_level, and any additional points within the interval if num_points_in_interval > 0 as well as outside the interval if num_points_in_interval > 0 and additional_width > 0. Log-likelihood values, standardised to 0.0 at the MLE point, for all points found in the interval are also stored in the UnivariateConfidenceStruct.\n\nIf use_existing_profiles=true then the brackets used to find each side of the confidence interval (between each side of the bounds for θi and the MLE point), will be updated and made smaller if confidence profiles for θi already exist at lower and higher confidence levels. \n\nCalled by univariate_confidenceintervals!.\n\n\n\n\n\n","category":"function"},{"location":"workflow_background/#Background","page":"Background","title":"Background","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Here we cover the background / general idea of the PWA workflow as seen in [1]. We also introduce the idea of reference tolerance sets for predictions of realisations/observations within the workflow as formally introduced in my Masters thesis.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"This is a discussion of the workflow formulation from my Masters thesis. Discussions which formed the basis of this section, minus reference tolerance intervals are in [1] and [3]. It is a little notation heavy, but hopefully it helps explain the sets and intervals available in the PWA workflow.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"This summary uses models of the form 'deterministic mathematical model + error model' as in [1] and [3], but it can also be used with stochastic models [4].","category":"page"},{"location":"workflow_background/#Observed-Data","page":"Background","title":"Observed Data","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Within the PWA workflow, observed data y_i^textrmo is measured at discrete time points t_i. The 'o' superscript distinguishes the observed data from the random variable y that generated the data. Given I observations, where i=123 I, observed data is collected into the vector, y_1I^textrmo, which corresponds to the time points t_1I. For multiple observations from the same time point and model component distinct indices in 1I are used. For an observation of multiple model components at the same time point, we will use the same index in 1I, such that, e.g. y_i^textrmo = (x_i^textrmo y_i^textrmo).","category":"page"},{"location":"workflow_background/#Mechanistic-Mathematical-Model","page":"Background","title":"Mechanistic Mathematical Model","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Deterministic mechanistic models take the form:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    z = f(theta^M t)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where theta^M is a vector of mechanistic model parameters, and z is a scalar or vector of model solutions containing no error. Model solutions evaluated at discrete time points t_i are defined as z_i(theta^M) = z(t_i theta^M). This is also referred to as the model trajectory. In the same way as observations, z_1I(theta^M) is a vector of the model solution evaluated at t_1I, while z(theta^M) represents the continuous model solution. ","category":"page"},{"location":"workflow_background/#Data-Distribution-Parameters","page":"Background","title":"Data Distribution Parameters","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The PWA workflow assumes that the observable data distribution can be characterised by a data/statistical model with parameters phi(theta). The data distribution can be considered to come from an underlying mechanistic model with measurement error [1], [3].","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"If we assume that the observed data come from an underlying mechanistic model with measurement error, then we let: ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    phi(theta) = (z(theta^M) theta^textrmo)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where typically (and here) the model solution is used as the mean or median parameter of the data distribution (error model) and theta^textrmo is any additional observation parameters like the observation error standard deviation. This gives us the full parameter vector, theta = (theta^M theta^textrmo), for the mechanistic model and error model. For simplicity, the additional observation parameters, theta^textrmo, may be specified and regarded as known [1]. However, unless they are truly known (as in, e.g. a synthetic coverage experiment), the likelihood function is known as an estimated likelihood function because it does not account for the uncertainty in theta^textrmo [2].","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"At time points t_i we define:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    phi_i(theta) = (z_i(theta^M) theta^textrmo)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where z_i(theta^M) is the mechanistic model solution at observation i.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"In general, given data distribution parameters, we obtain a density function for the observed data y dependent on parameters, theta, of the form:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y sim p(ytheta) = p(yphi(theta))","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"When measured at time point t_i, this becomes:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y_i sim p(y_itheta) = p(y_iphi_i(theta))","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"When the data distribution parameters are for an error model such as the additive Gaussian, log-normal and logit-normal models, the density function for y can be represented in the following ways.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"For the additive Gaussian model, this is represented as [3]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y_i sim p(y_i  theta) sim mathcalN(phi_i(theta)) sim mathcalN(z_i(theta^M) theta^textrmo) sim mathcalN(z_i(theta^M) sigma^2_N)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where theta^textrmo = sigma_N. This is equivalent to representing the observed data as the model trajectory plus error from a normal distribution with zero mean and variance, sigma^2_N.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"For the log-normal model, this is represented as [3]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y_i sim textLogNormal(log(z_i(theta^M)) sigma^2_L)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where theta^textrmo = sigma_L.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The logit-normal model is represented similarly to the log-normal model, noting that the mechanistic model solution is a proportion defined in (01):","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y_i sim textLogitNormal(textlogit(z_i(theta^M)) sigma^2_L)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where theta^textrmo = sigma_L and textlogit(p)=log(pdiv (1-p)).","category":"page"},{"location":"workflow_background/#Likelihood-Function","page":"Background","title":"Likelihood Function","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"If we have a vector of independent observations y_1I^textrmo from our density function, y, which is a function of parameter theta, we can define the normalised likelihood function, hatmathcalL(theta  y_1I^textrmo), and in particular the normalised log-likelihood function, hatell left(theta    y_1I^textrmoright). The 'hat', hat, on mathcalL and ell is used to represent that the functions are normalised.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The normalised likelihood function is [5]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    hatmathcalL(theta  y_1I^textrmo) =\n    fracp(y_1I^textrmo  phi(theta) )suplimits_theta hspace01cm p(y_1I^textrmophi(theta))","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where the numerator is the likelihood function evaluated for data distribution parameters phi(theta) at observations y_1I^textrmo and the denominator is the maximum likelihood estimate (MLE) of the likelihood function as theta varies. The MLE will be estimated using numerical optimisation, with bounds on parameters, which have value hattheta at the MLE.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Here, we will work with the normalised log-likelihood function of the form:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"beginequation\nbeginsplit\n    hatell left(theta    y_1I^textrmoright) = log hatmathcalL(theta  y_1I^textrmo)\n    = log p(y_1I^textrmo  phi(theta) ) - sup_theta log p(y_1I^textrmo  phi(theta) )\n    = sum_i=1^I log p(y_i^textrmo  phi(theta) )- sup_thetasum_i=1^I log p(y_i^textrmo  phi(theta))\nendsplit\nendequation","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Normalisation of the log-likelihood function means that hatell left(theta    y_1I^textrmoright) leq 0 and hatell (hattheta    y_1I^textrmo) = 0.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"We generally recommend using the loglikelihood function implemented in Distributions.jl to determine the value of the log-likelihood function, as in our examples. This is straightforward to compute for the error models described in Data Distribution Parameters given observed data y_1I^textrmo.","category":"page"},{"location":"workflow_background/#Profile-Likelihood-Function","page":"Background","title":"Profile Likelihood Function","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Given a likelihood function, we can define a profile log-likelihood function as a function of interest parameters, psi, and nuisance parameters, omega. These parameters represent a partitioning of the parameter vector theta = (psi omega). The normalised profile log-likelihood function is then:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    hatell_p left(psi    y_1I^textrmoright) = sup_omega    psi hatell_p left(psi  omega    y_1I^textrmoright)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where for given values of the interest parameter psi, the values of omega are optimised out, meaning they are set to the values that maximise the function. We expect that this function will be continuous for the models considered.","category":"page"},{"location":"workflow_background/#Confidence-Sets-For-Parameters","page":"Background","title":"Confidence Sets For Parameters","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Using the log-likelihood function, we define approximate likelihood-based confidence sets for the full parameter vector theta [2], [5]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_theta 1-alpha(y_1I^textrmo) =  theta    hatell left(theta    y_1I^textrmoright) geq ell_c ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where ell_c is a threshold chosen so that the approximate coverage of the confidence interval is 1-alpha. For sufficiently regular problems (see Section \\ref{sssec:loglikelihood_approx}) the threshold is calibrated using the chi-square distribution. Regular means that the likelihood function is well approximated by a quadratic function around the MLE [2]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"ell_c = - fracDelta_nu 1-alpha2","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where Delta_nu 1-alpha is the 1-alpha quantile of the chi^2 distribution with nu degrees of freedom. We will refer to these as full parameter confidence sets. For full parameter confidence sets, we set nu equal to the number of parameters, theta. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Profile likelihood-based confidence sets for the interest parameter(s) psi take the form:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC^psi_theta 1-alpha(y_1I^textrmo) =  theta=(psi omega)   hatell_p left(psi    y_1I^textrmoright) geq ell_c ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where we instead set nu equal to the dimensionality of the interest parameters (e.g. for confidence sets with a single interest parameter nu=1 and with two interest parameters nu=2). We also record the optimised out values of nuisance parameters, omega, in the confidence set for the interest parameter. Recording these nuisance parameters allows this set to be propagated forward into predictive quantities; they do not have to be recorded otherwise. Simultaneous profile likelihood-based confidence sets for interest parameters can be obtained by setting nu=theta [6]. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"We refer to profile likelihood-based confidence sets for one and two interest parameters as univariate profiles and bivariate profiles, respectively. We let the labels 'univariate' or 'bivariate' relate to profiles formed using nu equal to the dimensionality of the interest parameter, psi. If the profiles are instead created using the simultaneous asymptotic threshold with nu=theta, they will be referred to as 'simultaneous univariate' or 'simultaneous bivariate' profiles. Setting nu to any other value may not have a clear statistical interpretation.","category":"page"},{"location":"workflow_background/#Confidence-Sets-For-Data-Distribution-Parameters","page":"Background","title":"Confidence Sets For Data Distribution Parameters","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"By propagating forward full parameter confidence sets and profile likelihood-based confidence sets using the mapping phi(theta), we define approximate likelihood-based confidence sets for the data distribution parameters phi. We refer to the data distribution parameters, phi, as 'predictive' quantities [1]. We use square brackets in the following equations to show that the confidence set for data distribution parameters is the set image of the parameter confidence set under the mapping phi(theta). ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"For the likelihood-based confidence set for data distribution parameters from full parameter confidence sets, this is defined as:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_phi1-alpha(y_1I^textrmo) = phimathcalC_theta1-alpha(y_1I^textrmo) = phi(theta)  vert  theta in mathcalC_theta1-alpha(y_1I^textrmo)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"This set definition implies that the confidence set mathcalC_phi1-alpha(y_1I^textrmo) has at least a coverage of 1-alpha (is conservative) given the following relationship [1]:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    theta in mathcalC_theta1-alpha implies phi(theta) in mathcalC_phi1-alpha","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"For a profile likelihood-based confidence set, a profile-wise confidence set, this is defined as:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_phi1-alpha^psi(y_1I^textrmo) = phimathcalC^psi_theta1-alpha(y_1I^textrmo) = phi(theta)  vert theta in mathcalC^psi_theta1-alpha(y_1I^textrmo)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"More conservative profile-wise confidence sets for data distribution parameters can be formed by taking the union of individual profile confidence sets:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_phi1-alpha approx bigcup_psi mathcalC^psi_phi1-alpha","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Additionally, we can obtain more conservative profile-wise confidence sets for data distribution parameters by forming simultaneous profile confidence sets with nu=theta rather than nu=psi.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"When considered in a mechanistic model context, this allows us to form a trajectory confidence set for the mechanistic model solution, which is typically treated as the mean or median data distribution parameter [1], [3]. If we wish to form a trajectory confidence set for the mechanistic model solution, we consider just that component of the data distribution confidence set. In this case, if the parameter confidence set has the correct coverage properties over all parameters simultaneously, we expect the trajectory confidence set, mathcalC_phi1-alpha, to display curvewise (simultaneous) coverage properties, where the true model solution is fully contained within the confidence set [3].  ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"We refer to likelihood-based confidence sets for the model trajectory from full parameter confidence sets as full trajectory confidence sets. Similarly, we refer to the sets for the data distribution parameters as full data distribution confidence sets. Additionally, we refer to profile-wise confidence sets for the model trajectory from univariate and bivariate profiles as profile-wise trajectory confidence sets. ","category":"page"},{"location":"workflow_background/#Reference-Tolerance-Sets-For-Observed-Data","page":"Background","title":"Reference Tolerance Sets For Observed Data","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"W define (1-delta 1-alpha) reference tolerance sets for observed data given 1-delta population reference sets [7], [8]. For more background, please see my Masters thesis. This is in contrast to more traditional prediction sets for observed data.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The 1-delta population reference interval refers to an interval that contains 1-delta of the population (i.e. of observations) [7], [8]. These population reference intervals are also 'predictive' quantities. Here, a 1-delta population reference set refers to the set of 1-delta population reference intervals across time points, t_j (and similarly for reference tolerance sets and reference tolerance intervals). In general, we will take a 1-delta population reference interval to be given by the 1-delta highest density region [9] of population observations at t_j, as this allows the interval to represent a 'typical' observation best. For the error models discussed in Data Distribution Parameters we use UnivariateUnimodalHighestDensityRegion.jl as seen in Predefined Error models.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Similarly, a (1-delta 1-alpha) reference tolerance interval is a tolerance interval that contains the 1-delta reference interval with probability 1-alpha. We refer to these as reference tolerance intervals unless the (1-delta 1-alpha) designation is important for clarity. These intervals can be used as approximate prediction intervals; they appear to be appropriate for trapping at least 1-delta of observations with confidence 1-alpha, which is a weaker condition than trapping the 1-delta population reference interval. As our notation suggests, a reference tolerance interval can be formed that has a coverage property at a different confidence level to the size of the reference interval (i.e. delta neq alpha).","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Therefore, given a desired confidence level 1-alpha, we form a 1-alpha likelihood-based confidence set for data distribution parameters, mathcalC_phi1-alpha, from the full parameter confidence set. Then, for each phi in the data distribution parameter set we construct a 1-delta reference set, mathcalA_y1-delta(y^textrmo_1I), where phi is related to y as in the density function equation in Data Distribution Parameters. This reference set is constructed by taking a 1-delta region of the data distribution. For symmetric data distributions, we take the delta2 and 1-delta2 quantiles of the probability distribution. For asymmetric distributions, we take the 1-delta highest density region [9]. If the asymmetric distribution is unimodal we can use UnivariateUnimodalHighestDensityRegion.jl to evaluate the highest density region. We then take the union across the reference sets formed from each phi to obtain (1-delta 1-alpha) reference tolerance sets for observed data, y^textrmo, from full parameter confidence sets:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_y (1-delta 1-alpha)(y^textrmo_1I) approx bigcup_phi  in  mathcalC_phi1-alpha(y^textrmo_1I) mathcalA_y1-delta(y^textrmo_1I)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Because each mathcalA_y1-delta(y^textrmo_1I) can only be guaranteed to contain the population reference set if it was derived using the true parameter values, we refer to these only as reference sets. Similarly, because mathcalC_y(1-delta 1-alpha)(y^textrmo_1I) is obtained by taking the union over each reference set, it is much more likely that one of these was obtained from the true parameter values. Hence, we refer to these as reference tolerance sets. Usefully, if the data distribution parameter confidence set, mathcalC_phi1-alpha, has curvewise coverage properties then we also expect the reference tolerance set, mathcalC_y(1-delta 1-alpha)(y^textrmo_1I), to have curvewise coverage properties. We refer to these as full reference tolerance sets.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"We do the same thing for profile-wise reference tolerance sets, beginning from profile-wise data distribution confidence sets:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC^psi_y(1-delta 1-alpha)(y^textrmo_1I) approx bigcup_phi  in  mathcalC^psi_phi1-alpha(y^textrmo_1I) mathcalA^psi_1-delta(y^textrmo_1I)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"More conservative profile-wise reference tolerance sets can again be formed by taking the union of reference tolerance sets from individual profiles:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    mathcalC_y(1-delta 1-alpha) approx bigcup_psi mathcalC^psi_y(1-delta 1-alpha)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"We refer to these as profile-wise reference tolerance sets.","category":"page"},{"location":"workflow_background/#Full-Reference-Tolerance-Set-Coverage","page":"Background","title":"Full Reference Tolerance Set Coverage","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The coverage of the full reference tolerance set in the PWA workflow is straightforward to prove. The (1-delta 1-alpha) reference tolerance set is constructed from the 1-alpha confidence set for data distribution parameters, which is constructed from the 1-alpha confidence set for parameters. Here let 1-delta = 1-alpha = 095 = 95. If a full parameter confidence set has 95% coverage and the model is well-specified, then 95% of the time it will trap the true parameters. Therefore, the full data distribution confidence set will also have 95% coverage. Additionally, the true data distribution parameters give the true 1-delta reference interval for the population at each time point. Hence, if the full data distribution confidence set has 95% coverage, then our full reference tolerance set will contain the 95% reference set at least 95% of the time. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"In the mechanistic model case, if the full data distribution confidence set has curvewise coverage of the model solution, then our full reference tolerance set will also have curvewise coverage of the population reference set. That is, 95% of the full reference tolerance sets constructed in this fashion under repeated sampling of new data will contain the 95% population reference set (all 95% population reference intervals). A similar idea to what we demonstrate here, but in a different context, is found in [10]. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"For example, consider a single parameter scalar model, z(theta^M) = theta^M, from which we have obtained I=100 observations corrupted by i.i.d. Gaussian noise sim mathcalN(0theta^textrmo). Let theta^M=2 and theta^textrmo=1. The density function for observations is represented as:","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"    y sim p(y  theta)  sim mathcalN(phi(theta)) sim mathcalN(z(theta^M) theta^textrmo) sim mathcalN(z(theta^M) sigma)","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"where theta^textrmo=sigma and theta = (theta^M theta^textrmo). Resultantly, our parameter vector, theta, has two parameters. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The 95% population reference interval can be formed by considering the 2.5% and 97.5% quantiles of this density function under the true parameterisation, theta =0040 3960. Then, given the I observations we form a 95% confidence set for theta, mathcalC_theta 095. The mapping of theta onto the data distribution parameters phi is 1:1, hence we also have a 95% confidence set for phi, mathcalC_phi 095. We then use each phi in mathcalC_phi 095 to form 95\\% reference intervals, mathcalA_y 095. We then take the union across each of these reference intervals to form (95%, 95%) reference tolerance intervals, mathcalC_y (095095). If the true parameterisation is in mathcalC_theta 095 with 95% coverage, then it is also in mathcalC_phi 095 with 95% coverage and our full reference tolerance interval, mathcalC_y (095 095), will also contain the at least 95% reference interval with at least 95% coverage. ","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"The 'at least' statement for both the reference interval and the coverage of this interval occurs for the following reasons. For the coverage statement, if theta notin mathcalC_theta 095, then it is possible that we predict the population reference interval using the incorrect parameter or similarly from the union of reference intervals from many incorrect parameters. Similarly, for the reference interval statement, if theta in mathcalC_theta 095, then our 95% reference interval from that theta is the 95% population reference interval. Other parameter values in the parameter confidence set may predict reference intervals outside of this range. Hence, the union of these intervals will contain at least the 95% population reference interval.","category":"page"},{"location":"workflow_background/","page":"Background","title":"Background","text":"Code to visualise and test this example is seen below:","category":"page"},{"location":"workflow_background/#Setup","page":"Background","title":"Setup","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"using Random, Distributions\nusing LaTeXStrings\nusing LikelihoodBasedProfileWiseAnalysis\n\nθ_true = [2,1]\ntrue_dist = Normal(θ_true[1], θ_true[2])\nn = 100\nRandom.seed!(3)\ny_obs = rand(true_dist, n) \n\nref_interval = quantile(true_dist, [0.025, 0.975])\n\ndata = (y_obs=y_obs, dist=Normal(0, θ_true[2]), t=[\"z\"])\n\nfunction lnlike(θ, data)\n    return sum(loglikelihood(Normal(0, θ[2]), data.y_obs .- θ[1]))\nend\n\nfunction predictfunction(θ, data, t=[\"z\"])\n    return [θ[1]*1.0]\nend\n\nerrorfunction(a,b,c) = normal_error_σ_estimated(a,b,c, 2)","category":"page"},{"location":"workflow_background/#Parameter-Confidence-Set-Evaluation","page":"Background","title":"Parameter Confidence Set Evaluation","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"model = initialise_LikelihoodModel(lnlike, predictfunction, errorfunction, data, [:μ, :σ], [2.,1.], [-1., 0.01], [5., 5.], [1.,1.]);\n\nunivariate_confidenceintervals!(model, num_points_in_interval=300)\ndimensional_likelihood_samples!(model, 2, 1000000)","category":"page"},{"location":"workflow_background/#Confidence-Set-Plots","page":"Background","title":"Confidence Set Plots","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"using Plots; gr()\nformat=(size=(400,400), dpi=300, title=\"\", legend_position=:topright)\n\nplt = plot_univariate_profiles(model; format...)\nvline!(plt[1], [θ_true[1]], label=L\"\\theta^M\", xlabel=L\"\\theta^M\", lw=2, linestyle=:dash)\nvline!(plt[2], [θ_true[2]], label=L\"\\theta^\\textrm{o}\", xlabel=L\"\\theta^M\", lw=2, linestyle=:dash)\ndisplay(plt[1])\ndisplay(plt[2])\n\nplt = plot_bivariate_profiles(model; for_dim_samples=true, markeralpha=0.4, max_internal_points=10000, ylabel=latexstring(\"\\\\theta^\\\\textrm{o}\"), xlabel=latexstring(\"\\\\theta^M\"), format...)\nscatter!(plt[1], [θ_true[1]], [θ_true[2]], label=\"θtrue\", color=\"black\", ms=5, msw=0)\ndisplay(plt[1])","category":"page"},{"location":"workflow_background/#Profile-Wise-Reference-Intervals-and-Reference-Tolerance-Intervals-From-Univariate-Profiles","page":"Background","title":"Profile-Wise Reference Intervals and Reference Tolerance Intervals From Univariate Profiles","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"generate_predictions_univariate!(model, [\"z\"], 1.0)\nlq = model.uni_predictions_dict[1].realisations.lq\nuq = model.uni_predictions_dict[1].realisations.uq\n\nplt = plot(1:length(lq), transpose(uq); xlabel=\"Confidence Set Sample\", ylabel=\"Interval\", label=\"Upper\", palette=:Paired_6, format...)\nplot!(1:length(lq), transpose(lq), label=\"Lower\")\n\nlq = model.uni_predictions_dict[2].realisations.lq\nuq = model.uni_predictions_dict[2].realisations.uq\n\nplt = plot(1:length(lq), transpose(uq); xlabel=\"Confidence Set Sample\", ylabel=\"Interval\", label=\"Upper\", palette=:Paired_6, format...)\nplot!(1:length(lq), transpose(lq), label=\"Lower\")\n\nextrema1 = model.uni_predictions_dict[1].realisations.extrema\nextrema2 = model.uni_predictions_dict[2].realisations.extrema\nextrema = [min(extrema1[1],extrema2[1]) max(extrema1[2], extrema2[2])]\n\nusing StatsPlots\nplt = plot(true_dist; xlabel=latexstring(\"y\"), label=\"Density\", fill=(0, 0.3), palette=:Paired_6, format...)\nvline!(ref_interval, label=\"Reference\", lw=2)\nvline!(transpose(extrema1), label=\"Tolerance, \"*L\"\\psi=\\theta^M\", linestyle=:dash, lw=2)\nvline!(transpose(extrema2), label=\"Tolerance, \"*L\"\\psi=\\theta^\\textrm{o}\", linestyle=:dash, lw=3)\nvline!(transpose(extrema), label=\"Tolerance, union\", linestyle=:dashdot, lw=2, alpha=0.7)","category":"page"},{"location":"workflow_background/#Full-Reference-Tolerance-Interval","page":"Background","title":"Full Reference Tolerance Interval","text":"","category":"section"},{"location":"workflow_background/","page":"Background","title":"Background","text":"generate_predictions_dim_samples!(model, [\"z\"], 1.0)\nlq = model.dim_predictions_dict[1].realisations.lq\nuq = model.dim_predictions_dict[1].realisations.uq\nextrema=model.dim_predictions_dict[1].realisations.extrema\n\nplt = plot(1:length(lq), transpose(uq); xlabel=\"Confidence Set Sample\", ylabel=\"Interval\", label=\"Upper\", palette=:Paired_6, format...)\nplot!(1:length(lq), transpose(lq), label=\"Lower\")\n\nplt = plot(true_dist; xlabel=latexstring(\"y\"),label=\"Density\", fill=(0, 0.3), palette=:Paired_6, format...)\nvline!(ref_interval, label=\"Reference\", lw=2)\nvline!(transpose(extrema), label=\"Tolerance\", linestyle=:dash, lw=2)","category":"page"},{"location":"internal_library/ellipse_likelihood/#Ellipse-Functions","page":"Ellipse Functions","title":"Ellipse Functions","text":"","category":"section"},{"location":"internal_library/ellipse_likelihood/","page":"Ellipse Functions","title":"Ellipse Functions","text":"Pages = [\"ellipse_likelihood.md\"]","category":"page"},{"location":"internal_library/ellipse_likelihood/","page":"Ellipse Functions","title":"Ellipse Functions","text":"LikelihoodBasedProfileWiseAnalysis.getMLE_hessian_and_covariance\nLikelihoodBasedProfileWiseAnalysis.test_hessian_identifiability","category":"page"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.getMLE_hessian_and_covariance","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.getMLE_hessian_and_covariance","text":"getMLE_hessian_and_covariance(f::Function, θmle::Vector{<:Float64})\n\nComputes the negative hessian of function f at θmle using ForwardDiff.jl and it's pseudoinverse, returning both matrices.\n\nHessian identifiability is tested using LikelihoodBasedProfileWiseAnalysis.test_hessian_identifiability.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.test_hessian_identifiability","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.test_hessian_identifiability","text":"test_hessian_identifiability(Hmle::Matrix{T}, num_pars::Int) where T<:Float64\n\nModified R code from [12] in 'Code for Book', bassicocc.R.\n\nThe cutoff used to estimate whether a standardised eigenvalue is close enough to zero to indicate non-identifiability is 1e-12*number of parameters, which is a smaller value than used in [12] and [13] due to smaller error in the calculation of Hmle via automatic differentiation. This cutoff is meant as an indication of non-identifiability/singularity; the hessian may still be identifiable/non-singular even if a warning occurs.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/ellipse_likelihood/#Likelihood","page":"Ellipse Functions","title":"Likelihood","text":"","category":"section"},{"location":"internal_library/ellipse_likelihood/","page":"Ellipse Functions","title":"Ellipse Functions","text":"LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike\nLikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike_1D_soln\nLikelihoodBasedProfileWiseAnalysis.ellipse_loglike\nLikelihoodBasedProfileWiseAnalysis.ellipse_like","category":"page"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike","text":"analytic_ellipse_loglike(θ::Vector, θIndexes::Vector{Int}, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}) where T<:Float64\n\nComputes the analytical value of the ellipse approximation of the profile log-likelihood function for the interest parameters in θindexes.\n\nWill produce an equivalent result to optimising out nuisance parameters with LikelihoodBasedProfileWiseAnalysis.ellipse_loglike if there are no bounds on parameters (or not close to the values considered) and there are no issues with the invertibility of the hessian, mathcalH(hattheta).\n\nThe analytic ellipse log-likelihood has no knowledge of lower and upper bounds on parameters. Hence profiles generated by optimising out the nuisance parameters for a given interest parameter may look different to those from the analytical profile if it enters space where a bound would be active. Pushing forward from analytical profiles may thus be infeasible if the profile has entered a space where a parameter bound should be active (e.g. a non-negativity bound).\n\n\n\n\n\n","category":"function"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike_1D_soln","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.analytic_ellipse_loglike_1D_soln","text":"analytic_ellipse_loglike_1D_soln(θIndex::Int, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}, targetll::T)\n\nIn order to find the asympotic confidence interval for an interest parameter at θIndex, also known as Wald intervals [2], we solve the following equation from Ref. [6, Eq. (7)].\n\nell_c  = -frac12(θ_i - hatθ_i)^2 times Γ_ii(hatθ)^-1\n\nθ_i =  hatθ_i + sqrtfrac-2 ell_cΓ_ii(hatθ)^-1 equiv hatθ_i + sqrt-2 ell_c times Γ_ii(hatθ)\n\nNote: C(hatθ) = 2 times H(hatθ)^-1 and Γ(hatθ) = H(hatθ)^-1, and ell_c = -χ^2(α textttdof), so the equation is equivalent to equation 7 in the above reference. χ^2(α textttdof) is the lpha quantile of the chi-squared distribution with dof degrees of freedom, as calculated in LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.ellipse_loglike","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.ellipse_loglike","text":"ellipse_loglike(θ::Vector, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64\n\nReturns the value of the ellipse approximation of the normalised log-likelihood function using a second-order Taylor expansion at the MLE, hatθ.\n\nWhere: ``\\hat{\\ell}(θ) \\approx \\hat{\\ell}^\\mathcal{E} (\\theta) = -\\frac{1}{2} (\\theta-\\hat{\\theta})' \\mathcal{H}(\\hat{\\theta}) (\\theta-\\hat{\\theta}) [2].\n\nmathcalH is the hessian of the log-likelihood function at the MLE, as evaluated using LikelihoodBasedProfileWiseAnalysis.getMLE_hessian_and_covariance.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/ellipse_likelihood/#LikelihoodBasedProfileWiseAnalysis.ellipse_like","page":"Ellipse Functions","title":"LikelihoodBasedProfileWiseAnalysis.ellipse_like","text":"ellipse_like(θ::Vector{T}, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64\n\nThe approximate likelihood function (i.e. the natural base exponential of the log-likelihood function), LikelihoodBasedProfileWiseAnalysis.ellipse_loglike.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#Bivariate-Profiles","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"The key function for evaluating bivariate profile boundaries is bivariate_confidenceprofiles!. The evaluated bivariate profile(s) will be contained within a BivariateConfidenceStruct that is stored in the LikelihoodModel.","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"bivariate_confidenceprofiles!\nget_bivariate_confidence_set","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofiles!","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofiles!","text":"bivariate_confidenceprofiles!(model::LikelihoodModel, \n    θcombinations::Vector{Vector{Int}}, \n    num_points::Int; \n    <keyword arguments>)\n\nFinds num_points profile_type boundary points at a specified confidence_level for each combination of two interest parameters using a specified method, optionally saving any found internal points. Saves these profiles by modifying model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nθcombinations: vector of pairs of parameters to profile, as a vector of vectors of model parameter indexes.\nnum_points: positive number of points to find on the boundary at the specified confidence level. Depending on the method, if a region of the user-provided bounds is inside the boundary some of these points will be on the bounds and inside the boundary. Set to at least 3 within the function as some methods need at least three points to work.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level on which to find the profile_type boundary. Default is 0.95 (95%).\ndof: an integer ∈ [2, model.core.num_pars] for the degrees of freedom used to define the asymptotic threshold (LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood) which defines the boundary of the bivariate profile. For bivariate profiles that are considered individually, it should be set to 2. For profiles that are considered simultaneously, it should be set to model.core.num_pars. Default is 2. Setting it to model.core.num_pars should be reasonable when making predictions for well-identified models with <10 parameters. Note: values other than 2 and model.core.num_pars may not have a clear statistical interpretation.\nprofile_type: whether to use the true log-likelihood function or an ellipse approximation of the log-likelihood function centred at the MLE (with optional use of parameter bounds). Available profile types are LogLikelihood, EllipseApprox and EllipseApproxAnalytical. Default is LogLikelihood() (LogLikelihood).\nmethod: a method of type AbstractBivariateMethod. For a list of available methods use bivariate_methods() (bivariate_methods). Default is RadialRandomMethod(5) (RadialRandomMethod).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\nsave_internal_points: boolean variable specifying whether to save points found inside the boundary during boundary computation. Internal points can be plotted in bivariate profile plots and will be used to generate predictions from a given bivariate profile. Default is true.\nexisting_profiles: Symbol ∈ [:ignore, :merge, :overwrite] specifying what to do if profiles already exist for a given θcombination, confidence_level, profile_type and method. See below for each symbol's meanings. Default is :merge.\nfind_zero_atol: a Real number greater than zero for the absolute tolerance of the log-likelihood function value from the target value to be used when searching for confidence intervals. Default is model.find_zero_atol.\noptimizationsettings: a OptimizationSettings struct containing the optimisation settings used to find optimal values of nuisance parameters for a given pair of interest parameter values. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θcombinations completed and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across combinations of interest parameters. Set this variable to false if Distributed.jl is not being used. Default is true.\nuse_threads: boolean variable specifying, if use_distributed is false, whether to use parallelised for loops across Threads.nthreads() threads or a non-parallel for loops to find boundary points from methods where boundary points are found independently. Default is true.\nFix1AxisMethod and RadialMLEMethod parallelise the finding point pair step and the finding the boundary from point pairs step.\nSimultaneousMethod and RadialRandomMethod do not parallelise the finding point pair step but parallelise finding the boundary from point pairs.\nContinuationMethod is not parallelised at all. \nIterativeBoundaryMethod parallelises finding the initial boundary but not the following boundary improvement steps.\nAnalyticalEllipseMethod does not require parallelisation.\n\nnote: existing_profiles meanings\n:ignore means profiles that already exist will not be recomputed even if they contain fewer num_points boundary points. \n:merge means profiles that already exist will be merged with profiles from the current algorithm run to reach num_points. If the existing profile already has at least num_points boundary points then that profile will not be recomputed. Otherwise, the specified method will be run starting from the difference between num_points and the number of points in the existing profile. The result of that method run will be merged with the existing profile. Predictions evaluated from the existing profile will be forgotten. To keep these predictions see extended help below.\n:overwrite means profiles that already exist will be overwritten, regardless of how many points they contain. Predictions evaluated from the existing profile will be forgotten. To keep these predictions see extended help below.\n\nDetails\n\nUsing LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile this function calls the algorithm/method specified by method for each interest parameter combination in θcombinations (depending on the setting for existing_profiles and num_points if these profiles already exist). Nuisance parameters of each point in bivariate interest parameter space are found by maximising the log-likelihood function given by profile_type. Updates model.biv_profiles_df for each successful profile and saves their results as a BivariateConfidenceStruct in model.biv_profiles_dict, where the keys for the dictionary is the row number in model.biv_profiles_df of the corresponding profile. model.biv_profiles_df.num_points is the number of points found on the bivariate boundary (it does not include the number of saved internal points).\n\nExtended help\n\nValid bounds\n\nFor methods that use points placed on parameter bounds to bracket for the confidence boundary, the bracketing method utilised via Roots.jl's find_zero will be unlikely to converge to the true confidence boundary for a given pair of interest parameters if the bounds on either parameter are +/- Inf or the log-likelihood function evaluates to +/- Inf. Bounds should be set to prevent this from occurring.\n\nPreventing predictions from being forgotten when merging or overwriting profiles\n\nTo prevent predictions from being lost from existing profiles that would be overwritten when calling bivariate_confidenceprofiles!, existing profiles should be converted into a [CombinedBivariateMethod], prior to running new bivariate profiles. To do this use combine_bivariate_boundaries! on model with keyword argument not_evaluated_predictions set to false.\n\nDistributed Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true, then the bivariate profiles of distinct interest parameter combinations will be computed in parallel across Distributed.nworkers() workers. If use_distributed is false and use_threads is true then for methods where finding boundary points is independent they will be computed in parallel across Threads.nthreads() threads for each pair of interest parameters.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for each new boundary point to be found (for all methods except for AnalyticalEllipseMethod and ContinuationMethod). For AnalyticalEllipseMethod this is the time it takes to find all points on the boundary of the ellipse of two interest parameters. For ContinuationMethod this is the time it takes to find each new point, internal or on the boundary.  \n\n\n\n\n\nbivariate_confidenceprofiles!(model::LikelihoodModel, \n    θcombinations_symbols::Union{Vector{Vector{Symbol}}, Vector{Tuple{Symbol, Symbol}}}, \n    num_points::Int; \n    <keyword arguments>)\n\nProfiles just the provided θcombinations_symbols parameter pairs, provided as either a vector of vectors or a vector of tuples.\n\n\n\n\n\nbivariate_confidenceprofiles!(model::LikelihoodModel, \n    profile_m_random_combinations::Int, \n    num_points::Int; \n    <keyword arguments>)\n\nProfiles m random two-way combinations of model parameters (sampling without replacement), where 0 < m ≤ binomial(model.core.num_pars,2).\n\n\n\n\n\nbivariate_confidenceprofiles!(model::LikelihoodModel, \n    num_points::Int; \n    <keyword arguments>)\n\nProfiles all two-way combinations of model parameters.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.get_bivariate_confidence_set","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.get_bivariate_confidence_set","text":"get_bivariate_confidence_set(model::LikelihoodModel, biv_row_number::Int)\n\nReturns the BivariateConfidenceStruct corresponding to the profile in row biv_row_number of model.biv_profiles_df\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#Methods-For-Finding-Boundaries","page":"Bivariate Profiles","title":"Methods For Finding Boundaries","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"We provide several heuristics for evaluating the boundaries of bivariate profiles; they're listed in order of computational efficiency. Available methods can be checked using bivariate_methods. We recommend evaluating up to 50 points; 10-30 should be appropriate for getting a reasonable approximation of a bivariate boundary, particularly if it is relatively elliptical. Note: [AnalyticalEllipseMethod] exclusively works with the [EllipseApproxAnalytical] profile type; it is also highly recommended if that profile type is of interest.","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"AbstractBivariateMethod\nAbstractBivariateVectorMethod\nbivariate_methods\nIterativeBoundaryMethod\nRadialMLEMethod\nRadialRandomMethod\nSimultaneousMethod\nFix1AxisMethod\nAnalyticalEllipseMethod\nContinuationMethod","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.AbstractBivariateMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.AbstractBivariateMethod","text":"AbstractBivariateMethod\n\nSupertype for bivariate boundary finding methods. Use bivariate_methods() for a list of available methods (see bivariate_methods).\n\nSubtypes\n\nAbstractBivariateVectorMethod\n\nCombinedBivariateMethod\n\nFix1AxisMethod\n\nContinuationMethod\n\nAnalyticalEllipseMethod\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.AbstractBivariateVectorMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.AbstractBivariateVectorMethod","text":"AbstractBivariateVectorMethod <: AbstractBivariateMethod\n\nSupertype for bivariate boundary finding methods that search between two distinct points. \n\nSubtypes\n\nIterativeBoundaryMethod\n\nRadialMLEMethod\n\nRadialRandomMethod\n\nSimultaneousMethod\n\nSupertype Hiearachy\n\nAbstractBivariateVectorMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_methods","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_methods","text":"bivariate_methods()\n\nPrints a list of available bivariate methods. Available bivariate methods include IterativeBoundaryMethod, RadialRandomMethod, RadialMLEMethod, SimultaneousMethod, Fix1AxisMethod, ContinuationMethod and AnalyticalEllipseMethod. Note: CombinedBivariateMethod represents a destructive merge of the boundary structs of one or more methods and is not a valid method for bivariate_confidenceprofiles!.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.IterativeBoundaryMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.IterativeBoundaryMethod","text":"IterativeBoundaryMethod(initial_num_points::Int, \n    angle_points_per_iter::Int, \n    edge_points_per_iter::Int, \n    radial_start_point_shift::Float64=rand(), \n    ellipse_sqrt_distortion::Float64=1.0;\n    use_ellipse::Bool=false)\n\nMethod for iteratively improving an initial boundary of initial_num_points, found by pushing out from the MLE point in directions defined by either RadialMLEMethod, when use_ellipse=true, or RadialRandomMethod, when use_ellipse=false (see LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE!, LikelihoodBasedProfileWiseAnalysis.iterativeboundary_init and LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_iterativeboundary).\n\nArguments\n\ninitial_num_points: a positive integer number of initial boundary points to find. \nangle_points_per_iter: a integer ≥ 0 for the number of edges to explore and improve based on the angle objective before switching to the edge objective. If angle_points_per_iter > 0 and edge_points_per_iter > 0 the angle objective is considered first.\nedge_points_per_iter:  a integer ≥ 0 for the number of edges to explore and improve based on the edge objective before switching back to the angle objective. If angle_points_per_iter > 0 and edge_points_per_iter > 0 the angle objective is considered first.\nradial_start_point_shift: a number ∈ [0.0,1.0]. Default is rand() (defined on [0.0,1.0]), meaning that by default a different set of initial points will be found each time.\nellipse_sqrt_distortion: a number ∈ [0.0,1.0]. Default is 0.01. \n\nKeyword Arguments\n\nuse_ellipse: Whether to find initial_num_points by searching in directions defined by an ellipse approximation, as in RadialMLEMethod, or as in RadialRandomMethod. Default is false.\n\nDetails\n\nFor additional information on the radial_start_point_shift and ellipse_sqrt_distortion arguments see the keyword arguments for generate_N_clustered_points in EllipseSampling.jl.\n\nRecommended for use with the LogLikelihood profile type. Radial directions, edge length and internal angle calculations are rescaled by the relative magnitude/scale of the two interest parameters. This is so directions and regions explored and consequently the boundary found are not dominated by the parameter with the larger magnitude.\n\nOnce an initial boundary is found by pushing out from the MLE point in directions defined by either RadialMLEMethod or RadialRandomMethod, the method seeks to improve this boundary by minimising an internal angle and an edge length objective, each considered sequentially, until the desired number of boundary points are found. As such, the method can be thought of as a mesh smoothing or improvement algorithm; we can consider the boundary found at a given moment in time to be a N-sided polygon with edges between adjacent boundary points (vertices). \n\nExtended help\n\nRegions we define as needing improvement are those with:\n\nInternal angles between adjacent edges that are far from being straight (180 degrees or π radians). The region defined by these edges is not well explored as the real boundary edge in this region likely has some degree of smooth curvature. It may also indicate that one of these edges cuts our boundary into a enclosed region and an unexplored region on the other side of the edge. In the event that a vertex is on a user-provided bound for a parameter, this objective is set to zero, as this angle region is a byproduct of user input and not the actual log-likelihood region. This objective is defined in LikelihoodBasedProfileWiseAnalysis.internal_angle_from_pi!.\nEdges between adjacent vertices that have a large euclidean distance. The regions between these vertices is not well explored as it is unknown whether the boundary actually is straight between these vertices. On average the closer two vertices are, the more likely the edge between the two points is well approximated by a straight line, and thus our mesh is a good representation of the true log-likelihood boundary. This objective is defined in LikelihoodBasedProfileWiseAnalysis.edge_length.\n\nThe method is implemented as follows:\n\nCreate edges between adjacent vertices on the intial boundary. Calculate angle and edge length objectives for these edges and vertices and place them into tracked heaps.\nUntil found the desired number of boundary points repeat steps 3 and 4.\nFor angle_points_per_iter points:\nPeek at the top vertex of the angle heap.\nPlace a candidate point in the middle of the edge connected to this vertex that has the larger angle at the vertex the edge connects to. This is so we explore the worse defined region of the boundary.\nUse this candidate to find a new boundary point (see below).\nIf found a new boundary point, break edge we placed candidate on and replace with edges to the new boundary point, updating angle and edge length objectives in the tracked heap (see LikelihoodBasedProfileWiseAnalysis.heapupdates_success!). Else break our polygon into multiple polygons (see LikelihoodBasedProfileWiseAnalysis.heapupdates_failure!).\nFor edge_points_per_iter points:\nPeek at the top edge of the edge heap.\nPlace a candidate point in the middle of this edge.\nSame as for step 3.3.\nSame as for step 3.4.\n\nnote: angle_points_per_iter and edge_points_per_iter\nAt least one of angle_points_per_iter and edge_points_per_iter must be non-zero.\n\nnote: Using a candidate point to find a new boundary point\nUses LikelihoodBasedProfileWiseAnalysis.newboundarypoint!.If a candidate point is on the log-likelihood threshold boundary, we accept the point.If a candidate point is inside the boundary, then we search in the normal direction to the edge until we find a boundary point or hit the parameter bound, accepting either.If a candidate point is outside the boundary we find the edge, e_intersect of our boundary polygon that is intersected by the line in the normal direction of the candidate edge, which passes through the candidate point. Once this edge is found, we find the vertex on e_intersect that is closest to our candidate point, subject to that vertex not also being on the candidate point edge. We setup a 1D line search/bracketing method between these two points. In the event that no boundary points are found between these two points it is likely that multiple boundaries exist. If so, we break the candidate point's edge and e_intersect and reconnect the vertexes such that we now have multiple boundary polygons.\n\nwarning: Largest boundary polygon at any iteration must have at least three points\nIf the largest polygon has less than two points the method will display a warning message and terminate, returning the boundary found up until then. \n\nBoundary finding method\n\nFor the initial boundary, it uses a 1D bracketing algorithm between the MLE point and the point on the user-provided bounds in the search directions to find the boundary at the desired confidence level. For boundary improvement, if the candidate point is inside the boundary, it uses a 1D bracketing algorithm between an internal point and the point on the user-provided bounds in the search direction. If the candidate point is outside the boundary, it uses a derivative-free 1D line search algorithm in the search direction. If the derivative-free approach fails, it switches to a bracketing heuristic between the candidate point and closest vertex on e_intersect.\n\nThis method is unlikely to find boundaries that do not contain the MLE point (if they exist), but it can find them. If a boundary that does not contain the MLE point is found, it is not guaranteed to be explored. In this case the the method will inform the user that multiple boundaries likely exist for this combination of model parameters.\n\nImpact of parameter bounds\n\nIf a parameter bound is in the way of reaching the boundary in a given search direction, the point on that bound will be returned as the boundary point. For the bracketing method to work, parameter values on the bounds need to be legal and return a non Inf value from the log-likelihood function. Interest parameter bounds that have ranges magnitudes larger than the range of the boundary may prevent the true boundary from being found. Additionally, the bracketing method will be less efficient if the interest parameter bounds are far from the true boundary. \n\nThreaded implementation\n\nThis method is partially implemented with Threads parallelisation if use_threads is set to true when calling bivariate_confidenceprofiles!. The initial boundary step is partially parallelised (the finding point pairs step is not parallelised and the finding boundary from point pairs step is parallelised) while the boundary improvement steps are not.\n\nInternal Points\n\nFinds between 0 and num_points - initial_num_points internal points - internal points are found when the edge being considered's midpoint is inside the boundary. \n\nSupertype Hiearachy\n\nIterativeBoundaryMethod <: AbstractBivariateVectorMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.RadialMLEMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.RadialMLEMethod","text":"RadialMLEMethod(ellipse_start_point_shift::Float64=rand(), \n    ellipse_sqrt_distortion::Float64=0.01)\n\nMethod for finding the bivariate boundary of a confidence profile by bracketing between the MLE point and points on the provided bounds in directions given by points found on the boundary of a ellipse approximation of the log-likelihood function around the MLE, e, using EllipseSampling.jl (see LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialMLE! and LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch). \n\nArguments\n\nellipse_start_point_shift: a number ∈ [0.0,1.0]. Default is rand() (defined on [0.0,1.0]), meaning that by default a different set of points will be found each time.\nellipse_sqrt_distortion: a number ∈ [0.0,1.0]. Default is 0.01. \n\nDetails\n\nAs for univariate_confidenceintervals! with keyword argument use_ellipse_approx_analytical_start=true, the profile log-likelihood function will be evaluated at each ellipse point prior to creating the point pair bracket. If:\n\nthe ellipse point is outside of the provided bounds, the MLE point and the point on the provided bounds in the direction of the ellipse point from the MLE is the point pair for bracketing.\nthe ellipse point is between the desired boundary and the provided bounds, the MLE point and the ellipse point is the point pair for bracketing.\nthe ellipse point is between the MLE point and the desired boundary, the ellipse point and the point on the provided bounds is the point pair for bracketing.\nthe ellipse point is exactly on the desired boundary (profile log-likelihood function evaluates to exactly the confidence level threshold), the ellipse point is the boundary point and the method will (wrongfully) state there's a point on the bounds. From a numerical perspective this is incredibly unlikely so this is regarded as not a big deal. \n\nFor additional information on arguments see the keyword arguments for generate_N_clustered_points in EllipseSampling.jl.\n\nRecommended for use with the EllipseApprox profile type. Will produce reasonable results for the LogLikelihood profile type when bivariate profile boundaries are convex. Otherwise, IterativeBoundaryMethod, which has an option to use a starting solution from RadialMLEMethod, is preferred as it iteratively improves the quality of the boundary and can discover regions not explored by this method. \n\nExtended help\n\nBoundary finding method\n\nUses a 1D bracketing algorithm between the MLE point and the point on the user-provided bounds in the search directions to find the boundary at the desired confidence level. Will replace one of these sides with the ellipse point depending on which side of the boundary the ellipse point is on, subject to it being between the two points. \n\nThis method is unlikely to find boundaries that do not contain the MLE point (if they exist).\n\nImpact of parameter bounds\n\nIf a parameter bound is in the way of reaching the boundary in a given search direction, the point on that bound will be returned as the boundary point. For the bracketing method to work, parameter values on the bounds need to be legal and return a non Inf value from the log-likelihood function. Interest parameter bounds that have ranges magnitudes larger than the range of the boundary may prevent the true boundary from being found. Additionally, the bracketing method will be less efficient if the interest parameter bounds are far from the true boundary. \n\nThreaded implementation\n\nThis method is partially implemented with Threads parallelisation if use_threads is set to true when calling bivariate_confidenceprofiles!. The finding point pairs step is not parallelised and the finding boundary from point pairs step is parallelised.\n\nInternal Points\n\nFinds no internal points.\n\nSupertype Hiearachy\n\nRadialMLEMethod <: AbstractBivariateVectorMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.RadialRandomMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.RadialRandomMethod","text":"RadialRandomMethod(num_radial_directions::Int, use_MLE_point::Bool=true)\n\nMethod for finding the bivariate boundary of a confidence profile by finding internal boundary points using a uniform random distribution on provided bounds and choosing num_radial_directions to explore from these points (see LikelihoodBasedProfileWiseAnalysis.findNpointpairs_radialrandom! and LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch).\n\nArguments\n\nnum_radial_directions: an integer greater than zero. \nuse_MLE_point: whether to use the MLE point as the first internal point found or not. Default is true (use).\n\nDetails\n\nRecommended for use with the LogLikelihood profile type. Radial directions are rescaled by the relative magnitude/scale of the two interest parameters. This is so directions explored and consequently the boundary found are not dominated by the parameter with the larger magnitude.\n\nThe method first uniformly samples the region specified by the bounds for the two interest parameters until a point within the boundary is found. Then it chooses num_radial_directions, spaced uniformly around a circle, and rotates these directions by a random phase shift ∈ [0.0, 2π/num_directions] radians. These directions are then distorted by the relative magnitude/scale of the two interest parameters. Then for each of these directions, until it runs out of directions or finds the desired number of points, it finds the closest point on the bounds from the internal point in this direction. If the point on the bounds is inside the boundary it will be used in place of the boundary point. If the point on the bounds is outside the boundary then it will be used as the external point in the point pair. A bracketing method is then used to find a boundary point between the point pair (the bound point and the internal point). The method continues searching for internal points and generating directions until the desired number of boundary points is found.\n\nGiven a fixed number of desired boundary points, we can decrease the cost of finding internal points by increasing the number of radial directions to explore, num_radial_directions, at each internal point. However, it is important to balance num_radial_directions with the desired number of boundary points. If num_radial_directions is large relative to the number of boundary points, then the boundary the method finds may constitute a local search around the found internal points. Resultantly, there may be regions were the boundary is not well explored. This will be less of an issue for more 'circular' boundary regions.   \n\nIterativeBoundaryMethod may be preferred over this method if evaluating the log-likelihood function is expensive or the bounds provided for the interest parameters are much larger than the boundary, as the uniform random sampling for internal points will become very expensive. \n\nExtended help\n\nBoundary finding method\n\nUses a 1D bracketing algorithm between an internal point and the point on the user-provided bounds in the search direction to find the boundary at the desired confidence level.  \n\nThis method can find multiple boundaries (if they exist).\n\nImpact of parameter bounds\n\nIf a parameter bound is in the way of reaching the boundary in a given search direction, in the same way as RadialMLEMethod, the point on that bound will be returned as the boundary point. For the bracketing method to work, parameter values on the bounds need to be legal and return a non Inf value from the log-likelihood function. Interest parameter bounds that have ranges magnitudes larger than the range of the boundary may prevent the true boundary from being found. Additionally, the bracketing method will be less efficient if the interest parameter bounds are far from the true boundary.\n\nThreaded implementation\n\nThis method is partially implemented with Threads parallelisation if use_threads is set to true when calling bivariate_confidenceprofiles!. The finding point pairs step is not parallelised and the finding boundary from point pairs step is parallelised.\n\nInternal Points\n\nFinds a minimum of div(num_points, num_radial_directions, RoundUp) - 1*use_MLE_point internal points.\n\nSupertype Hiearachy\n\nRadialRandomMethod <: AbstractBivariateVectorMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.SimultaneousMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.SimultaneousMethod","text":"SimultaneousMethod(min_proportion_unique::Real=0.5, include_MLE_point::Bool=true)\n\nMethod for finding the bivariate boundary of a confidence profile by finding internal and external boundary points using a uniform random distribution on provided bounds, pairing these points in the order they're found and bracketing between each pair (see LikelihoodBasedProfileWiseAnalysis.findNpointpairs_simultaneous! and LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_vectorsearch). Values of min_proportion_unique lower than zero may improve performance if finding either internal points or external points is difficult given the specified bounds on interest parameters.\n\nArguments\n\nmin_proportion_unique: a proportion ∈ (0.0, 1.0] for the minimum allowed proportion of unique points in one of the internal or external point vectors. One of these vectors will be fully unique. Whichever vector is not unique, will have at least min_proportion_unique unique points. Default is 0.5.\nuse_MLE_point: whether to use the MLE point as the first internal point found or not. Default is true (use).\n\nDetails\n\nRecommended for use with the LogLikelihood profile type. \n\nWhen min_proportion_unique = 1.0 the method uniformly samples the region specified by the bounds for the two interest parameters until as many internal and external boundary points as the desired number of boundary points are found. These points are paired in the order they are found. \n\nIf min_proportion_unique < 1.0 the method uniformly samples the region specified by the bounds for the two interest parameters until either the internal or external number of boundary points found is as many as the number of desired boundary points. For whichever vector is less than the number of desired boundary points, we keep searching for that kind of point until at least ceil(min_proportion_unique * num_points) points have been found. Once ceil(min_proportion_unique * num_points) points have been found these points are used to fill the remainder of the vector, in order found. Internal and external points in each vector are then paired in the order they are found.\n\nA bracketing method is then used to find a boundary point between each point pair (the external point and the internal point).\n\nRadialRandomMethod and IterativeBoundaryMethod are preferred over this method from a computational performance standpoint as they require fewer log-likelihood evalutions (when RadialRandomMethod has parameter num_radial_directions > 1). \n\nExtended help\n\nBoundary finding method\n\nUses a 1D bracketing algorithm between a valid point pair to find the boundary at the desired confidence level.  \n\nThis method can find multiple boundaries (if they exist).\n\nImpact of parameter bounds\n\nIf a parameter bound is in the way of reaching the boundary, points will not be put on that bound. Additionally, if the true boundary is very close to a parameter bound, the method will struggle to find this region of the boundary. This is because finding the boundary in this location requires generating a random point between the boundary and the parameter bound, which becomes more difficult the closer they are. \n\nInterest parameter bounds that have ranges magnitudes larger than the range of the boundary will make finding internal points difficult, requiring a lot of computational effort. Similarly, the inverse will be true if external points are hard to find. Smaller values of min_proportion_unique will improve performance in these cases. \n\nThe method will fail if the interest parameter bounds are fully contained by the boundary.\n\nThreaded implementation\n\nThis method is partially implemented with Threads parallelisation if use_threads is set to true when calling bivariate_confidenceprofiles!. The finding point pairs step is not parallelised and the finding boundary from point pairs step is parallelised.\n\nInternal Points\n\nFinds at least ceil(min_proportion_unique*num_points) - 1*use_MLE_point internal points.\n\nSupertype Hiearachy\n\nSimultaneousMethod <: AbstractBivariateVectorMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.Fix1AxisMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.Fix1AxisMethod","text":"Fix1AxisMethod()\n\nMethod for finding the bivariate boundary of a confidence profile by using uniform random distributions on provided bounds to draw a value for one interest parameter, fix it, and draw two values for the other interest parameter, using the pair to find a boundary point using a bracketing method if the pair are a valid bracket (see LikelihoodBasedProfileWiseAnalysis.findNpointpairs_fix1axis! and LikelihoodBasedProfileWiseAnalysis.bivariate_confidenceprofile_fix1axis).\n\nDetails\n\nRecommended for use with the LogLikelihood profile type. \n\nThe method finds the desired number of boundary points by fixing the first and second interest parameters for half of these points each. It first draws a value for the fixed parameter using a uniform random distribution on provided bounds (e.g. ψ_x). Then it draws two values for the unfixed parameter in the same fashion (e.g. ψ_y1 and ψ_y2]). If one of these points ([ψ_x, ψ_y1] and [ψ_x, ψ_y2]) is an internal point and the other an external point, the point pair is accepted as they are a valid bracket. A bracketing method is then used to find a boundary point between the point pair (the internal and external point). The method continues searching for valid point pairs until the desired number of boundary points is found.\n\nRadialRandomMethod and IterativeBoundaryMethod are preferred over this method from a computational performance standpoint as they require fewer log-likelihood evalutions (when RadialRandomMethod has parameter num_radial_directions > 1). SimultaneousMethod will also likely be more efficient, even though it uses four random numbers vs three, as it doesn't unneccesarily throw away points.  \n\nExtended help\n\nBoundary finding method\n\nUses a 1D bracketing algorithm between a valid point pair, which are parallel to the x or y axis, to find the boundary at the desired confidence level.  \n\nThis method can find multiple boundaries (if they exist).\n\nImpact of parameter bounds\n\nIf a parameter bound is in the way of reaching the boundary, points will not be put on that bound. Additionally, if the true boundary is very close to a parameter bound, the method will struggle to find this region of the boundary. This is because finding the boundary in this location requires generating a random point between the boundary and the parameter bound, which becomes more difficult the closer they are. Interest parameter bounds that have ranges magnitudes larger than the range of the boundary will make finding internal points very difficult, requiring a lot of computational effort. Similarly, the inverse will be true if external points are hard to find. The method will fail if the interest parameter bounds are fully contained by the boundary.\n\nThreaded implementation\n\nThis method is implemented with Threads parallelisation if use_threads is set to true when calling bivariate_confidenceprofiles!.\n\nInternal Points\n\nFinds num_points internal points.\n\nSupertype Hiearachy\n\nFix1AxisMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.AnalyticalEllipseMethod","text":"AnalyticalEllipseMethod(ellipse_start_point_shift::Float64, \n    ellipse_sqrt_distortion::Float64)\n\nMethod for sampling the desired number of boundary points on a ellipse approximation of the log-likelihood function centred at the maximum likelihood estimate point using EllipseSampling.jl.\n\nArguments\n\nellipse_start_point_shift: a number ∈ [0.0,1.0]. Default is rand() (defined on [0.0,1.0]), meaning that by default a different set of points will be found each time.\nellipse_sqrt_distortion: a number ∈ [0.0,1.0]. Default is 1.0, meaning that by default points on the ellipse approximation are equally spaced with respect to arc length. \n\nDetails\n\nUsed for the EllipseApproxAnalytical profile type only: if this method is specified, then any user provided profile type will be overriden and replaced with EllipseApproxAnalytical. This ellipse approximation ignores user provided bounds.\n\nFor additional information on arguments see the keyword arguments for generate_N_clustered_points in EllipseSampling.jl.\n\nExtended help\n\nBoundary finding method\n\nExplicitly finds the boundary using EllipseSampling.jl.\n\nInternal Points\n\nFinds no internal points.\n\nSupertype Hiearachy\n\nAnalyticalEllipseMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.ContinuationMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.ContinuationMethod","text":"ContinuationMethod(num_level_sets::Int, \n    ellipse_confidence_level::Float64, \n    ellipse_start_point_shift::Float64=rand(), \n    level_set_spacing::Symbol=:loglikelihood)\n\nKept available for completeness but not recommended for use. A previous implementation of search directions from the MLE point was moved to [RadialMLEMethod].\n\nArguments\n\nnum_level_sets: the number of level sets used to get to the desired confidence level set.\nellipse_confidence_level: a number ∈ (0.0, 1.0). the confidence level at which to construct the initial ellipse and find the initial level set boundary. Recommended to be around 0.1.\nellipse_start_point_shift: a number ∈ [0.0,1.0]. Default is rand() (defined on [0.0,1.0]), meaning that by default a different set of points will be found each time.\nellipse_sqrt_distortion: a number ∈ [0.0,1.0]. Default is 1.0, meaning that by default points on the ellipse approximation are equally spaced with respect to arc length. \nlevel_set_spacing: a Symbol ∈ [:loglikelihood, :confidence]. Whether to space level sets uniformly in confidence level space or log-likelihood space, between the first level set found and the level set of desired confidence level. Default is :loglikelihood.\n\nDetails\n\nThe method finds an initial boundary at a low confidence level close to the ellipse_confidence_level (see initial_continuation_solution!). Then it 'continues' this initial boundary sequentially to num_level_sets higher confidence level boundaries until the desired confidence level is reached. If a user defined bound is in the way of a level set point reaching the next level set, that point is frozen on the bounds for all subsequent level sets. \n\nExtended help\n\nPresently this continuation is done by finding a point inside the boundary that is as close as possible to being a point that makes the boundary a star domain and is close to the centre of the area of the boundary in the x and y axes. We refer to this point as a 'star point', or a point that can see all the points on the boundary, without being blocked by an edge. We use a heuristic to estimate this, sampling points within the boundary and using these to produce kmeans points, of which one is hopefully a star point and at the centre of the boundary. \n\nIf we find a star point we then, for every point on the current boundary, push out in the direction defined by the line segment connecting the star point and the boundary to find the next confidence level boundary. If we do not find a star point, we assign each of the boundary points to the Kmeans point they are closest to (using a Euclidean distance metric), and use the direction defined by the line segments between a boundary point and it's associated Kmeans point to find the next confidence level boundary. This direction heuristic is carried out by refine_search_directions!. \n\nA traveling salesman heuristic is used to reorder the boundary points into a new minimum perimeter polygon, LikelihoodBasedProfileWiseAnalysis.minimum_perimeter_polygon!, if the continuation of one boundary to the next causes the mapping of adjacent vertices to change (expected if a star point is not found). \n\nFor additional information on the ellipse_start_point_shift and ellipse_sqrt_distortion arguments see the keyword arguments for generate_N_clustered_points in EllipseSampling.jl.\n\nBoundary finding method\n\nUses a derivative-free 1D line search algorithm to find the boundary at subsequent confidence levels. If the derivative-free approach fails, it switches to a bracketing algorithm between a given boundary point and the point on the user-provided bounds in the search direction.\n\nThis method is unlikely to find boundaries that do not contain the MLE point (if they exist).\n\nThreaded implementation\n\nThis method is not implemented with Threads parallelisation.\n\nInternal Points\n\nFinds num_points * num_level_sets internal points at distinct level sets.\n\nSupertype Hiearachy\n\nContinuationMethod <: AbstractBivariateMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#Sampling-Internal-Points-From-Boundaries","page":"Bivariate Profiles","title":"Sampling Internal Points From Boundaries","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"In order to cheaply sample interval points within the found boundary of a bivariate profile we use sample_bivariate_internal_points!.","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"sample_bivariate_internal_points!\nbivariate_hull_methods\nAbstractBivariateHullMethod\nConvexHullMethod\nConcaveHullMethod\nMPPHullMethod","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.sample_bivariate_internal_points!","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.sample_bivariate_internal_points!","text":"sample_bivariate_internal_points!(model::LikelihoodModel,\n    num_points::Int;\n    <keyword arguments>)\n\nSamples num_points internal points in interest parameter space of existing bivariate profiles that meet the requirements of the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset (see Keyword Arguments). Modifies model in place, with sampled internal points appended to the internal points field of each BivariateConfidenceStruct.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nnum_points: number of internal points to sample from within a polygon hull approximation of a bivariate boundary and append to it's array of internal points.\n\nKeyword Arguments\n\nconfidence_levels: a vector of confidence levels. If empty, all confidence levels of bivariate profiles will be considered for finding interval points. Otherwise, only confidence levels in confidence_levels will be considered. Default is Float64[] (any confidence level).\ndofs: a vector of integer degrees of freedom. If empty, all degrees of freedom of bivariate profiles will be considered. Otherwise, only degrees of freedom in dofs are allowed. Default is Int[] (any degree of freedom).\nprofile_types: a vector of AbstractProfileType structs. If empty, all profile types of bivariate profiles are considered. Otherwise, only profiles with matching profile types will be considered. Default is AbstractProfileType[] (any profile type).\nmethods: a vector of AbstractBivariateMethod structs. If empty all methods used to find bivariate profiles are considered. Otherwise, only profiles with matching method types will be considered (struct arguments do not need to be the same). Default is AbstractBivariateMethod[] (any bivariate method).\nsample_type: either a UniformRandomSamples or LatinHypercubeSamples struct for how to sample internal points from the polygon hull. UniformRandomSamples are homogeneously sampled from the polygon and LatinHypercubeSamples use the intersection of a heuristically optimised Latin Hypercube sampling plan with the polygon. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nhullmethod: method of type AbstractBivariateHullMethod used to create a 2D polygon hull that approximates the bivariate boundary from a set of boundary points and internal points (method dependent). For available methods see bivariate_hull_methods(). Default is MPPHullMethod() (MPPHullMethod).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\nt: vector of timepoints to evaluate predictions at for each new sampled internal point from a bivariate boundary that has already had predictions evaluated. The vector must be the same vector used to produce these previous predictions, otherwise points will not be sampled from this boundary. Default is missing.\nevaluate_predictions_for_samples: boolean variable specifying whether to evaluate predictions for sampled points given predictions have been evaluated for the boundary they were sampled from. If false, then existing predictions will be forgotten by the model and overwritten the next time predictions are evaluated for each profile internal points were sampled from. Default is true.\nproportion_of_predictions_to_keep: The proportion of predictions from num_points internal points to save. Does not impact the extrema calculated from predictions. Default is 1.0.\noptimizationsettings: a OptimizationSettings struct containing the optimisation settings used to find optimal values of nuisance parameters for a given pair of interest parameter values. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θs_to_profile completed and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across combinations of interest parameters. Set this variable to false if Distributed.jl is not being used. Default is true.\nuse_threads: boolean variable specifying, if use_distributed is false, to use a parallelised for loop across Threads.nthreads() threads to evaluate the log-likelihood at each sampled point. Default is true.\n\nDetails\n\nFor each bivariate profile that meets the requirements of LikelihoodBasedProfileWiseAnalysis.desired_df_subset it creates a 2D polygon hull from it's set of boundary and internal points (method dependent) using hullmethod and samples points from the hull using sample_type until num_points are found, rejecting any that are not inside the log-likelihood threshold at that confidence_level, dof and profile_type. For LatinHypercubeSamples this will be approximately num_points, whereas for UniformRandomSamples this will be exactly num_points. Nuisance parameters of each point in bivariate interest parameter space are found by maximising the log-likelihood function given by the profile_type of the profile.\n\nIt is highly recommended to view the docstrings of each hullmethod as the rejection rate of sampled points and the representation accuracy / coverage of the true confidence boundary varies between them, which can impact both computational performance and sampling coverage. For example, given the same set of boundary and internal points, ConvexHullMethod will produce a polygon hull that contains at least as much of the true confidence boundary as the other methods, but may have a higher rejection rate than other methods leading to higher computational cost.\n\nParallel Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true then the internal samples of distinct interest parameter combinations will be computed in parallel across Distributed.nworkers() workers. If use_distributed is false and use_threads is true then the internal samples of each distinct interest parameter combination will be computed in parallel across Threads.nthreads() threads. It is highly recommended to set use_threads to true in that situation.\n\nIteration Speed Of the Progress Meter\n\nAn iteration within the progress meter is specified as the time it takes for all internal points within a bivariate boundary to be found.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.bivariate_hull_methods","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.bivariate_hull_methods","text":"bivariate_hull_methods()\n\nPrints a list of available bivariate hull methods. Available bivariate hull methods include ConvexHullMethod, ConcaveHullMethod and MPPHullMethod.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.AbstractBivariateHullMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.AbstractBivariateHullMethod","text":"AbstractBivariateMethod\n\nSupertype for bivariate boundary hull methods. Use bivariate_hull_methods() for a list of available methods (see bivariate_hull_methods).\n\nSubtypes\n\nConvexHullMethod\n\nConcaveHullMethod\n\nMPPHullMethod\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.ConvexHullMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.ConvexHullMethod","text":"ConvexHullMethod()\n\nConstructs a 2D polygon hull to sample internal points from by applying a convex hull algorithm from Meshes.jl on the collection of points given by both the boundary and saved internal points. \n\nDetails\n\nRepresentation Accuracy\n\nFor convex boundaries, this method has the ability to create a more accurate representation of the true confidence boundary than MPPHullMethod, if saved internal points contain information about the boundary that is not covered by a convex hull of boundary points alone. For concave boundaries, this method will not be an accurate representation of the true confidence boundary. However, this may be desirable if the rejection rate when sampling is not too high, because the convex nature of the hull will likely contain more of the true confidence boundary than each of the other methods, particularly for lower numbers of boundary points. \n\nRejection Rate when Sampling Internal Points\n\nThe rejection rate when sampling internal points will be low for convex boundaries. The rejection rate may become very high for concave boundaries, if the area of the convex hull of the true confidence boundary is much larger than the area of the true confidence boundary. \n\nSupertype Hiearachy\n\nConvexHullMethod <: AbstractBivariateHullMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.ConcaveHullMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.ConcaveHullMethod","text":"ConcaveHullMethod()\n\nConstructs a 2D polygon hull to sample internal points from by applying a heuristic implementation of a heuristic concave hull algorithm from ConcaveHull.jl on the collection of points given by both the boundary and saved internal points (see LikelihoodBasedProfileWiseAnalysis.bivariate_concave_hull). \n\nDetails\n\nIt applies the ConcaveHull.concave_hull algorithm twice to the union of boundary and saved internal points, with the number of neighbours for the first pass chosen using a heuristic based on the number of points in the point union. Resultantly, it may result in more accurate coverage of the true confidence boundary than MPPHullMethod, for smaller numbers of boundary points, if saved internal points are in locations not enclosed by a polygon found using only boundary points. For example, if the true confidence boundary is a square and there is an internal point in the bottom left corner, but no boundary points around that corner, the boundary polygon created by this method will likely have a vertex at that internal point. However, this is not guaranteed because it is a heuristic. Bivariate methods that struggle to find boundaries close to or on the other side of a parameter bound are an example where using information on saved internal points will prove useful.\n\nRepresentation Accuracy\n\nThis method has the ability to create a more accurate representation of the boundary than MPPHullMethod, but because of it's heuristic nature this is not guaranteed. However, it should be a more accurate representation of the boundary than ConvexHullMethod for non-convex boundaries (concave boundaries). The representation in the worst case where the heuristic does not return a good representation of the boundary despite a good initial point cloud, will be signficantly worse than the ConvexHullMethod and likely worse than the MPPHullMethod.\n\nRejection Rate when Sampling Internal Points\n\nThe rejection rate when sampling internal points will be low for convex boundaries. The rejection rate should be low for concave boundaries as well, but the nature of the heuristic used may cause concave sections to be treated as convex.   \n\nSupertype Hiearachy\n\nConcaveHullMethod <: AbstractBivariateHullMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.MPPHullMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.MPPHullMethod","text":"MPPHullMethod()\n\nConstructs a 2D polygon hull to sample internal points from by applying a minimum perimeter polygon (MPP) traveling salesman problem algorithm to the boundary points (see LikelihoodBasedProfileWiseAnalysis.minimum_perimeter_polygon!). \n\nDetails\n\nIt does not use information on the position of internal points saved while finding a boundary. This may result in less accurate coverage of the true confidence boundary for smaller numbers of boundary points. For example, if the true confidence boundary is a square and there is an internal point in the bottom left corner, but no boundary points around that corner, the boundary polygon created by this method will not enclose the area around that corner. Bivariate methods that struggle to find boundaries close to or on the other side of a parameter bound are an example where using information on saved internal points would prove useful - ConvexHullMethod or ConcaveHullMethod may be more appropriate in these cases.\n\nRepresentation Accuracy\n\nThis method will create the most accurate representation of the boundary that has been found from any of the AbstractBivariateHullMethod methods, particularly for non-convex boundaries, given a sufficient number of boundary points.\n\nRejection Rate when Sampling Internal Points\n\nThe rejection rate when sampling internal points will be low for convex and concave boundaries. \n\nSupertype Hiearachy\n\nMPPHullMethod <: AbstractBivariateHullMethod <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#Merging-Boundaries-From-Multiple-Methods","page":"Bivariate Profiles","title":"Merging Boundaries From Multiple Methods","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"To improve the performance of internal point sampling, it may be worth finding bivariate boundaries using a combination of methods, where one method has more guaranteed boundary coverage and the other gives a more random search of interest parameter space. For example, combining IterativeBoundaryMethod and SimultaneousMethod into a CombinedBivariateMethod. ","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"CombinedBivariateMethod\ncombine_bivariate_boundaries!","category":"page"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.CombinedBivariateMethod","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.CombinedBivariateMethod","text":"CombinedBivariateMethod()\n\nA method representing a BivariateConfidenceStruct that has been destructively merged from one or more boundaries found with a AbstractBivariateMethod. Does not represent a method usable by bivariate_confidenceprofiles!.\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/bivariate/#LikelihoodBasedProfileWiseAnalysis.combine_bivariate_boundaries!","page":"Bivariate Profiles","title":"LikelihoodBasedProfileWiseAnalysis.combine_bivariate_boundaries!","text":"combine_bivariate_boundaries!(model::LikelihoodModel;\n    <keyword arguments>)\n\nCombines the confidence_level bivariate boundaries at dof of profile_type found using methods into a single CombinedBivariateMethod boundary for each set of interest parameters, modifying model destructively in place. Rows of model.biv_profiles_df to combine are found using the bivariate method of LikelihoodBasedProfileWiseAnalysis.desired_df_subset. Dictionary entries and dataframe rows of boundaries that have beeen combined will be deleted and the datastructures will be rebuilt according to the new row indices of model.biv_profiles_df. \n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level of profile_type boundaries to combine. Default is 0.95 (95%).\ndof: a integer ∈ [2, model.core.numpars] for the degrees of freedom of `profiletype` boundaries to combine. Default is 2.\nprofile_type: the profile type of boundaries to combine. Default is LogLikelihood() (LogLikelihood).\nmethods: a vector of methods of type AbstractBivariateMethod for combining boundaries found using those method types. methods should not contain CombinedBivariateMethod, but the case where it is included in methods is handled: it will be removed from the vector. Default is AbstractBivariateMethod[] (boundaries found using all methods are combined).\nnot_evaluated_predictions: a boolean specifiying whether to combine only boundaries that have not had or have had predictions evaluated. If predictions are evaluated for the combined struct (if it exists) but not for the rows to combine with it, they will not be combined, and vice versa. Default is true (combine boundaries that have not had predictions evaluated).\n\ninfo: Combining predictions\nIf predictions have been evaluated: the time points at which predictions have been evaluated at must be the same for all of the boundaires that are being combined.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/bivariate/#Index","page":"Bivariate Profiles","title":"Index","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/bivariate/","page":"Bivariate Profiles","title":"Bivariate Profiles","text":"Pages = [\"bivariate.md\"]","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/#Dimensional-Samples","page":"Dimensional Samples","title":"Dimensional Samples","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"We can generate samples within the log-likelihood boundary at any dimension of model interest parameters. Nuisance parameters will be set to the values that maximise the log-likelihood function, found using an optimisation scheme. Samples are only implemented for the true log-likelihood function (the LogLikelihood profile type).","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/#Sample-Types","page":"Dimensional Samples","title":"Sample Types","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"Three methods are implemented for rejection sampling of parameter confidence sets within the supplied parameter bounds. These will be inefficient if the bounds are not well-specified (are not close to the boundary of the desired parameter confidence set), and as both the model parameter dimension increases and the interest parameter dimension of the profile increases. By default these are the bounds contained with CoreLikelihoodModel, however, seperate bounds can be supplied for the purposes of sampling. Uniform grid sampling, uniform random sampling and sampling from a random Latin Hypercube scheme (the default) are supported.","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"AbstractSampleType\nUniformGridSamples\nUniformRandomSamples\nLatinHypercubeSamples","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.AbstractSampleType","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.AbstractSampleType","text":"AbstractSampleType\n\nSupertype for sample types.\n\nSubtypes\n\nUniformGridSamples\n\nUniformRandomSamples\n\nLatinHypercubeSamples\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.UniformGridSamples","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.UniformGridSamples","text":"UniformGridSamples()\n\nEvaluate the interest parameter bounds space on a uniform grid, determining the optimised log-likelihood value at each grid point. Keep samples which are inside the confidence threshold boundary. \n\nSupertype Hiearachy\n\nUniformGridSamples <: AbstractSampleType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.UniformRandomSamples","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.UniformRandomSamples","text":"UniformRandomSamples()\n\nTake uniform random samples of interest parameter bounds space, determining the optimised log-likelihood value at each point. Keep samples which are inside the confidence threshold boundary. \n\nSupertype Hiearachy\n\nUniformRandomSamples <: AbstractSampleType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.LatinHypercubeSamples","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.LatinHypercubeSamples","text":"LatinHypercubeSamples()\n\nCreate a Latin Hypercube sampling plan in interest parameter bounds space, determining the optimised log-likelihood value at each point in the plan. Keep samples which are inside the confidence threshold boundary. Uses LatinHypercubeSampling.jl.\n\nSupertype Hiearachy\n\nLatinHypercubeSamples <: AbstractSampleType <: Any\n\n\n\n\n\n","category":"type"},{"location":"user_interface/profiles_and_samples/dimensional/#Full-Likelihood-Sampling","page":"Dimensional Samples","title":"Full Likelihood Sampling","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"To sample a confidence set for the full parameter vector, a full parameter confidence set, we use full_likelihood_sample!. The sampled confidence set will be contained within a SampledConfidenceStruct that is stored in the LikelihoodModel.","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"full_likelihood_sample!","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.full_likelihood_sample!","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.full_likelihood_sample!","text":"full_likelihood_sample!(model::LikelihoodModel,\n    num_points_to_sample::Union{Int, Vector{Int}};\n    <keyword arguments>)\n\nSamples num_points_to_sample points from full parameter space, evaluating the log-likelihood function at each, saving all points that are inside the confidence_level log-likelihood threshold. Saves this sample by modifying model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nnum_points_to_sample: integer number of points to sample (for UniformRandomSamples and LatinHypercubeSamples sample types). For the UniformGridSamples sample type, if integer it is the number of points to grid over in each parameter dimension. If it is a vector of integers each index of the vector is the number of points to grid over in the corresponding parameter dimension. For example, [1,2] would mean a single point in dimension 1 and two points in dimension 2. \n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level which . Default is 0.95 (95%).\nsample_type: the sampling method used to sample parameter space. Available sample types are UniformGridSamples, UniformRandomSamples and LatinHypercubeSamples. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nlb: optional vector of lower bounds on parameters. Use to specify parameter lower bounds to sample over that are different than those contained in model.core. Default is Float64[] (use lower bounds from model.core).\nub: optional vector of upper bounds on parameters. Use to specify parameter upper bounds to sample over that are different than those contained in model.core. Default is Float64[] (use upper bounds from model.core).\nexisting_profiles: Symbol ∈ [:ignore, :overwrite] specifying what to do if samples already exist for a given confidence_level and sample_type.  Default is :overwrite.\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θcombinations completed and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a threaded for loop or distributed for loop to evaluate the log-likelihood at each sampled point. This should be set to true if Julia instances have been started with low numbers of threads or distributed computing is being used. Default is false.\nuse_threads: boolean variable specifying, if use_distributed is false, whether to use a parallelised for loop across Threads.nthreads() threads or a non-parallel for loop to evaluate the log-likelihood at each sampled point. Default is true.\n\nDetails\n\nUsing full_likelihood_sample this function calls the sample method specified by sample_type (depending on the setting for existing_profiles and confidence_level if a full likelihood sample already exists). Updates model.dim_samples_df if the sample is successful and saves the results as a SampledConfidenceStruct in model.dim_samples_dict, where the keys for the dictionary is the row number in model.dim_samples_df of the corresponding sample.\n\nExtended help\n\nParallel Computing Implementation\n\nIf Distributed.jl is being used and use_distributed is true,the log-likelihood value of sampled points will be computed in parallel across Distributed.nworkers() workers. If use_distributed is false and use_threads is true then the log-likelihood value of sampled points will be computed in parallel across Threads.nthreads() threads.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for each point chosen under the specified sampling scheme to be evaluated as valid or not. A point is valid if the log-likelihood function value at that point is greater than the confidence log-likelihood threshold.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/dimensional/#Dimensional-Likelihood-Sampling","page":"Dimensional Samples","title":"Dimensional Likelihood Sampling","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"Similarly, to sample a confidence set for an interest subset of the parameter vector, a 'dimensional profile', we use dimensional_likelihood_samples!. The sampled confidence set(s) will be contained within a SampledConfidenceStruct that is stored in the LikelihoodModel.","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"Note: dimensional likelihood samples can be 'full' likelihood samples as well. These will be computed before any other dimensional samples.","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"dimensional_likelihood_samples!\nget_dimensional_confidence_set","category":"page"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.dimensional_likelihood_samples!","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.dimensional_likelihood_samples!","text":"dimensional_likelihood_samples!(model::LikelihoodModel,\n    θindices::Vector{Vector{Int}},\n    num_points_to_sample::Union{Int, Vector{Int}};\n    <keyword arguments>)\n\nSamples num_points_to_sample points from interest parameter space, for each interest parameter combination in θindices, determining the values of nuisance parameters that maximise log-likelihood function at each, saving all points that are inside the confidence_level log-likelihood threshold. Saves these samples by modifying model in place.\n\nArguments\n\nmodel: a LikelihoodModel containing model information, saved profiles and predictions.\nθindices: a vector of vectors of parameter indexes for the combinations of interest parameters to samples points from.\nnum_points_to_sample: integer number of points to sample (for UniformRandomSamples and LatinHypercubeSamples sample types). For the UniformGridSamples sample type, if integer it is the number of points to grid over in each parameter dimension. If it is a vector of integers each index of the vector is the number of points to grid over in the corresponding parameter dimension. For example, [1,2] would mean a single point in dimension 1 and two points in dimension 2. \n\nKeyword Arguments\n\nconfidence_level: a number ∈ (0.0, 1.0) for the confidence level to find samples within. Default is 0.95 (95%).\nsample_type: the sampling method used to sample parameter space. Available sample types are UniformGridSamples, UniformRandomSamples and LatinHypercubeSamples. Default is LatinHypercubeSamples() (LatinHypercubeSamples).\nlb: optional vector of lower bounds on interest parameters. Use to specify interest parameter lower bounds to sample over that are different than those contained in model.core (must be the same length as original bounds). Default is Float64[] (use lower bounds from model.core).\nub: optional vector of upper bounds on interest parameters. Use to specify interest parameter upper bounds to sample over that are different than those contained in model.core (must be the same length as original bounds). Default is Float64[] (use upper bounds from model.core).\nθlb_nuisance: a vector of lower bounds on nuisance parameters, require θlb_nuisance .≤ model.core.θmle. Default is model.core.θlb. \nθub_nuisance: a vector of upper bounds on nuisance parameters, require θub_nuisance .≥ model.core.θmle. Default is model.core.θub.\nexisting_profiles: Symbol ∈ [:ignore, :overwrite] specifying what to do if samples already exist for a given confidence_level and sample_type. Default is :overwrite.\noptimizationsettings: a OptimizationSettings containing the optimisation settings used to find optimal values of nuisance parameters for a given interest parameter values. Default is missing (will use model.core.optimizationsettings).\nshow_progress: boolean variable specifying whether to display progress bars on the percentage of θcombinations completed and estimated time of completion. Default is model.show_progress.\nuse_distributed: boolean variable specifying whether to use a normal for loop or a @distributed for loop across combinations of interest parameters. Set this variable to false if Distributed.jl is not being used. Default is true.\nuse_threads: boolean variable specifying, if use_distributed is false, to use a parallelised for loop across Threads.nthreads() threads to evaluate the log-likelihood at each sampled point. Default is true.\n\nDetails\n\nUsing dimensional_likelihood_sample this function calls the sample method specified by sample_type for each set of interest parameters in [θindices] (depending on the setting for existing_profiles and confidence_level if these samples already exist). Updates model.dim_samples_df for each successful sample and saves their results as a SampledConfidenceStruct in model.dim_samples_dict, where the keys for the dictionary is the row number in model.dim_samples_df of the corresponding sample. model.dim_samples_df.num_points is the number of points within the confidence boundary from those sampled.\n\nnote: Support for `dof`\nSetting the degrees of freedom of a sampled parameter confidence set to a value other than the interest parameter dimensionality is not currently supported (e.g. as supported for univariate and bivariate profiles). Support may be added in the future, with a slight change in the API of this function.\n\nExtended help\n\nParallel Computing Implementation\n\nIf Distributed.jl is being used use_distributed is true, then the dimensional samples of distinct interest parameter combinations will be computed in parallel across Distributed.nworkers() workers. If use_distributed is false and use_threads is true then the dimensional samples of each distinct interest parameter combination will be computed in parallel across Threads.nthreads() threads. It is highly recommended to set use_threads to true in that situation.\n\nIteration Speed Of the Progress Meter\n\nThe time/it value is the time it takes for each point chosen under the specified sampling scheme to be evaluated as valid or not, for each interest parameter combination. A point is valid if the log-likelihood function value at that point is greater than the confidence log-likelihood threshold.\n\n\n\n\n\ndimensional_likelihood_samples!(model::LikelihoodModel,\n    θnames::Vector{Vector{Symbol}},\n    num_points_to_sample::Union{Int, Vector{Int}};\n    <keyword arguments>)\n\nSamples just the provided θnames interest parameter sets, provided as a vector of vectors.\n\n\n\n\n\ndimensional_likelihood_samples!(model::LikelihoodModel,\n    sample_dimension::Int,\n    sample_m_random_combinations::Int,\n    num_points_to_sample::Union{Int, Vector{Int}}\n    <keyword arguments>)\n\nSamples m random combinations of sample_dimension model parameters (sampling without replacement), where 0 < m ≤ binomial(model.core.num_pars, sample_dimension).\n\n\n\n\n\ndimensional_likelihood_samples!(model::LikelihoodModel,\n    sample_dimension::Int,\n    num_points_to_sample::Union{Int, Vector{Int}};\n    <keyword arguments>)\n\nSamples all combinations of sample_dimension model parameters.\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/dimensional/#LikelihoodBasedProfileWiseAnalysis.get_dimensional_confidence_set","page":"Dimensional Samples","title":"LikelihoodBasedProfileWiseAnalysis.get_dimensional_confidence_set","text":"get_dimensional_confidence_set(model::LikelihoodModel, dim_row_number::Int)\n\nReturns the SampledConfidenceStruct corresponding to the profile in row dim_row_number of model.dim_samples_df\n\n\n\n\n\n","category":"function"},{"location":"user_interface/profiles_and_samples/dimensional/#Index","page":"Dimensional Samples","title":"Index","text":"","category":"section"},{"location":"user_interface/profiles_and_samples/dimensional/","page":"Dimensional Samples","title":"Dimensional Samples","text":"Pages = [\"dimensional.md\"]","category":"page"},{"location":"internal_library/dimensional/#Dimensional-Functions","page":"Dimensional Functions","title":"Dimensional Functions","text":"","category":"section"},{"location":"internal_library/dimensional/","page":"Dimensional Functions","title":"Dimensional Functions","text":"Pages = [\"dimensional.md\"]","category":"page"},{"location":"internal_library/dimensional/#Common-Full-and-Dimensional-Likelihood","page":"Dimensional Functions","title":"Common Full and Dimensional Likelihood","text":"","category":"section"},{"location":"internal_library/dimensional/","page":"Dimensional Functions","title":"Dimensional Functions","text":"LikelihoodBasedProfileWiseAnalysis.add_dim_samples_rows!\nLikelihoodBasedProfileWiseAnalysis.set_dim_samples_row!\nLikelihoodBasedProfileWiseAnalysis.valid_points\nLikelihoodBasedProfileWiseAnalysis.check_if_bounds_supplied\nLikelihoodBasedProfileWiseAnalysis.uniform_grid\nLikelihoodBasedProfileWiseAnalysis.uniform_random\nLikelihoodBasedProfileWiseAnalysis.LHS","category":"page"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.add_dim_samples_rows!","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.add_dim_samples_rows!","text":"add_dim_samples_rows!(model::LikelihoodModel, num_rows_to_add::Int)\n\nAdds num_rows_to_add free rows to model.dim_samples_df by vertically concatenating the existing DataFrame and free rows using LikelihoodBasedProfileWiseAnalysis.init_dim_samples_df.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.set_dim_samples_row!","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.set_dim_samples_row!","text":"set_dim_samples_row!(model::LikelihoodModel, \n    row_ind::Int,\n    θindices::Vector{Int},\n    not_evaluated_predictions::Bool,\n    confidence_level::Float64,\n    sample_type::AbstractSampleType,\n    num_points::Int)\n\nSets the relevant fields of row row_ind in model.dim_samples_df after a profile has been evaluated.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.valid_points","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.valid_points","text":"valid_points(model::LikelihoodModel, \n    grid::Matrix{Float64},\n    grid_size::Int,\n    confidence_level::Float64, \n    num_dims::Int,\n    use_threads::Bool,\n    use_distributed::Bool,\n    channel::RemoteChannel)\n\nGiven a grid of grid_size points in full parameter space with num_dims dimensions, this function evaluates the log-likelihood function at each point and returns the points that are within the confidence_level log-likelihood threshold as a num_dims * n array, alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\n\n\n\n\nvalid_points(model::LikelihoodModel, \n    p::NamedTuple, \n    grid::Matrix{Float64},\n    grid_size::Int,\n    confidence_level::Float64, \n    dof::Int,\n    num_dims::Int,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nGiven a grid of grid_size points in interest parameter space with num_dims dimensions, this function finds the values of nuisance parameters that maximise the log-likelihood function at each point and returns the points that are within the confidence_level, dof, log-likelihood threshold as a num_dims * n array, alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.check_if_bounds_supplied","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.check_if_bounds_supplied","text":"check_if_bounds_supplied(model::LikelihoodModel,\n    lb::AbstractVector{<:Real},\n    ub::AbstractVector{<:Real})\n\nReturns the model bounds on full parameter space if lb and ub are empty, and lb and ub otherwise.  \n\n\n\n\n\ncheck_if_bounds_supplied(model::LikelihoodModel,\n    lb::AbstractVector{<:Real},\n    ub::AbstractVector{<:Real})\n\nReturns the model bounds on interest parameter space if lb and ub are empty, and lb and ub for interest parameter indices otherwise.  \n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.uniform_grid","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.uniform_grid","text":"uniform_grid(model::LikelihoodModel,\n    points_per_dimension::Union{Int, Vector{Int}},\n    confidence_level::Float64,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[];\n    use_threads::Bool=true,\n    use_distributed::Bool=false,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(Inf)))\n\nCreates a uniform grid with points_per_dimension in each dimension, uniformly spaced between lb and ub if supplied or between the bounds contained in model.core.The log-likelihood function is evaluated at each grid point and all grid points within the confidence_level log-likelihood threshold are saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the UniformGridSamples sample type.\n\n\n\n\n\nuniform_grid(model::LikelihoodModel,\n    θindices::Vector{Int},\n    points_per_dimension::Union{Int, Vector{Int}},\n    confidence_level::Float64,\n    dof::Int,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[],\n    θlb_nuisance::AbstractVector{<:Real}=model.core.θlb,\n    θub_nuisance::AbstractVector{<:Real}=model.core.θub,\n    optimizationsettings::OptimizationSettings=default_OptimizationSettings();\n    use_threads=true,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(Inf)))\n\nCreates a uniform grid on interest parameter space θindices with points_per_dimension in each interest dimension, uniformly spaced between lb[θindices] and ub[θindices] if supplied or between the bounds contained in model.core. The grid is then passed to LikelihoodBasedProfileWiseAnalysis.valid_points to determine the values of nuisance parameters that maximise log-likelihood function at each grid point. All grid points within the confidence_level log-likelihood threshold are then saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the UniformGridSamples sample type.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.uniform_random","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.uniform_random","text":"uniform_random(model::LikelihoodModel,\n    num_points::Int,\n    confidence_level::Float64,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[];\n    use_threads::Bool=true,\n    use_distributed::Bool=false,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(num_points+1)))\n\nCreates a grid of num_points uniform random points sampled between lb and ub if supplied or between the bounds contained in model.core. The log-likelihood function is evaluated at each grid point and all grid points within the confidence_level log-likelihood threshold are saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the UniformRandomSamples sample type.\n\n\n\n\n\nuniform_random(model::LikelihoodModel,\n    num_points::Int,\n    confidence_level::Float64,\n    dof::Int,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[],\n    θlb_nuisance::AbstractVector{<:Real}=model.core.θlb,\n    θub_nuisance::AbstractVector{<:Real}=model.core.θub,\n    optimizationsettings::OptimizationSettings=default_OptimizationSettings();\n    use_threads::Bool=true,\n    use_distributed::Bool=false,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(num_points+1)))\n\nCreates a grid of num_points uniform random points on interest parameter space θindices sampled between lb[θindices] and ub[θindices] if supplied or between the bounds contained in model.core. The grid is then passed to LikelihoodBasedProfileWiseAnalysis.valid_points to determine the values of nuisance parameters that maximise log-likelihood function at each grid point. All grid points within the confidence_level log-likelihood threshold are then saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the UniformRandomSamples sample type.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.LHS","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.LHS","text":"LHS(model::LikelihoodModel,\n    num_points::Int,\n    confidence_level::Float64,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[];\n    use_threads::Bool=true,\n    use_distributed::Bool=false,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(num_points+1)))\n\nCreates a grid of num_points points sampled using a Latin Hypercube sampling plan between lb and ub if supplied or between the bounds contained in model.core. The log-likelihood function is evaluated at each grid point and all grid points within the confidence_level log-likelihood threshold are saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the LatinHypercubeSamples sample type.\n\n\n\n\n\nLHS(model::LikelihoodModel,\n    θindices::Vector{Int},\n    num_points::Int,\n    confidence_level::Float64,\n    dof::Int,\n    lb::AbstractVector{<:Real}=Float64[],\n    ub::AbstractVector{<:Real}=Float64[],\n    θlb_nuisance::AbstractVector{<:Real}=model.core.θlb,\n    θub_nuisance::AbstractVector{<:Real}=model.core.θub,\n    optimizationsettings::OptimizationSettings=default_OptimizationSettings();\n    use_threads::Bool=true,\n    arguments_checked::Bool=false,\n    channel::RemoteChannel=RemoteChannel(() -> Channel{Bool}(num_points+1)))\n\nCreates a grid of num_points points on interest parameter space θindices using a Latin Hypercube sampling plan between lb[θindices] and ub[θindices] if supplied or between the bounds contained in model.core. The grid is then passed to LikelihoodBasedProfileWiseAnalysis.valid_points to determine the values of nuisance parameters that maximise log-likelihood function at each grid point. All grid points within the confidence_level log-likelihood threshold are then saved as a SampledConfidenceStruct. Points are saved alongside a vector of their log-likelihood values. Log-likelihood values are standardised to 0.0 at the MLE point.\n\nFor the LatinHypercubeSamples sample type.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#Full-Likelihood-Only","page":"Dimensional Functions","title":"Full Likelihood Only","text":"","category":"section"},{"location":"internal_library/dimensional/","page":"Dimensional Functions","title":"Dimensional Functions","text":"LikelihoodBasedProfileWiseAnalysis.full_likelihood_sample","category":"page"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.full_likelihood_sample","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.full_likelihood_sample","text":"full_likelihood_sample(model::LikelihoodModel,\n    num_points::Union{Int, Vector{Int}},\n    confidence_level::Float64,\n    sample_type::AbstractSampleType,\n    lb::AbstractVector{<:Real},\n    ub::AbstractVector{<:Real},\n    use_threads::Bool,\n    use_distributed::Bool,\n    channel::RemoteChannel)\n\nCalls the desired method for sampling parameter space, sample_type, and returns a SampledConfidenceStruct containing any points that were found within the confidence_level log-likelihood threshold.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#Dimensional-Likelihood-Only","page":"Dimensional Functions","title":"Dimensional Likelihood Only","text":"","category":"section"},{"location":"internal_library/dimensional/","page":"Dimensional Functions","title":"Dimensional Functions","text":"LikelihoodBasedProfileWiseAnalysis.dimensional_optimiser!\nLikelihoodBasedProfileWiseAnalysis.dimensional_likelihood_sample","category":"page"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.dimensional_optimiser!","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.dimensional_optimiser!","text":"dimensional_optimiser!(θs_opt::Union{Vector, SubArray}, \n    q::NamedTuple, \n    options::OptimizationSettings, \n    targetll::Real)\n\nGiven a log-likelihood function (q.consistent.loglikefunction) which is bounded in parameter space, this function finds the values of the nuisance parameters ω that optimise the function for fixed values of the interest parameters ψ (which are already in θs_opt) and returns the log-likelihood value minus the confidence boundary target threshold. The returned function value will be zero at the locations of the approximate confidence boundary for ψ. Nuisance parameter values are stored in the corresponding indices of θs_opt, modifying the array in place.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/dimensional/#LikelihoodBasedProfileWiseAnalysis.dimensional_likelihood_sample","page":"Dimensional Functions","title":"LikelihoodBasedProfileWiseAnalysis.dimensional_likelihood_sample","text":"dimensional_likelihood_sample(model::LikelihoodModel,\n    θindices::Vector{Int},\n    num_points::Union{Int, Vector{Int}},\n    confidence_level::Float64,\n    dof::Int,\n    sample_type::AbstractSampleType,\n    lb::AbstractVector{<:Real},\n    ub::AbstractVector{<:Real},\n    θlb_nuisance::AbstractVector{<:Real},\n    θub_nuisance::AbstractVector{<:Real},\n    optimizationsettings::OptimizationSettings,\n    use_threads::Bool,\n    channel::RemoteChannel)\n\nCalls the desired method for sampling interest parameter space, sample_type, and returns a SampledConfidenceStruct containing any points that were found within the confidence_level, `dof, log-likelihood threshold.\n\n\n\n\n\n","category":"function"},{"location":"examples/logistic/#Logistic-Model","page":"Logistic Model","title":"Logistic Model","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"The code included in this example is compiled into a single file here.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"The logistic model with a normal data distribution [1] has the following differential equation for the population density C(t)geq0:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"fracmathrmdC(t)mathrmdt = lambda C(t) Bigg1-fracC(t)KBigg","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"where the model parameter vector is given by theta^M = (lambda K C(0)). The corresponding additive Gaussian data distribution, with a fixed standard deviation, has a density function for the observed data given by:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"y_i sim p(y_i  theta) sim mathcalN(z_i(theta^M) sigma^2)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"where z_i(theta^M)=z(t_i theta^M) is the model solution of the first Equation at t_i and sigma=10.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"The true parameter values are theta^M =(001 100 10). The corresponding lower and upper parameter bounds are a = (0 50 0) and b = (00515050). Observation times are t_1I = 01002001000. The original implementation can be found at https://github.com/ProfMJSimpson/Workflow. Example realisations, the true model trajectory and 95% population reference set under this parameterisation can be seen in the figure below:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/#Initial-Setup","page":"Logistic Model","title":"Initial Setup","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Here we add three worker processes, which matches the number of univariate and bivariate profiles. For coverage testing we recommend setting this number as discussed in Import Package and Set Up Distributed Environment. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"using Distributed\nif nprocs()==1; addprocs(3, env=[\"JULIA_NUM_THREADS\"=>\"1\"]) end\n@everywhere using Random, Distributions\n@everywhere using LikelihoodBasedProfileWiseAnalysis\nusing Combinatorics","category":"page"},{"location":"examples/logistic/#Model-and-Likelihood-Function-Definition","page":"Logistic Model","title":"Model and Likelihood Function Definition","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"@everywhere function solvedmodel(t, θ)\n    return (θ[2]*θ[3]) ./ ((θ[2]-θ[3]) .* (exp.(-θ[1] .* t)) .+ θ[3])\nend\n\n@everywhere function loglhood(θ, data)\n    y=solvedmodel(data.t, θ)\n    e=sum(loglikelihood(data.dist, data.y_obs .- y))\n    return e\nend","category":"page"},{"location":"examples/logistic/#Initial-Data-and-Parameter-Definition","page":"Logistic Model","title":"Initial Data and Parameter Definition","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"# true parameters\nλ_true=0.01; K_true=100.0; C0_true=10.0; t=0:100:1000; \n@everywhere global σ=10.0;\nθ_true=[λ_true, K_true, C0_true]\ny_true = solvedmodel(t, θ_true)\ny_obs = [19.27, 20.14, 37.23, 74.87, 88.51, 82.91, 123.88, 103.25, 78.89, 87.87, 113.0]\n\n# Named tuple of all data required within the log-likelihood function\ndata = (y_obs=y_obs, t=t, dist=Normal(0, σ))\n\n# Bounds on model parameters \nλ_min, λ_max = (0.00, 0.05)\nK_min, K_max = (50., 150.)\nC0_min, C0_max = (0.0, 50.)\nlb = [λ_min, K_min, C0_min]\nub = [λ_max, K_max, C0_max]\n\nθnames = [:λ, :K, :C0]\nθG = θ_true\npar_magnitudes = [0.005, 10, 10]","category":"page"},{"location":"examples/logistic/#LikelihoodModel-Initialisation","page":"Logistic Model","title":"LikelihoodModel Initialisation","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Here we choose to set some optimization settings, opt_settings, which are used when determining the maximum likelihood estimate hattheta. If different settings are not provided to functions for profiling, then these settings (which are now contained in the LikelihoodModel), will be used.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5,))\nmodel = initialise_LikelihoodModel(loglhood, data, θnames, θG, lb, ub, par_magnitudes, optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/#Full-Parameter-Vector-Confidence-Set-Evaluation","page":"Logistic Model","title":"Full Parameter Vector Confidence Set Evaluation","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To evaluate the full parameter vector confidence set at a 95% confidence level we use:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"full_likelihood_sample!(model, 30000, use_distributed=true)","category":"page"},{"location":"examples/logistic/#Profiling","page":"Logistic Model","title":"Profiling","text":"","category":"section"},{"location":"examples/logistic/#Univariate-Profiles","page":"Logistic Model","title":"Univariate Profiles","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To find the confidence intervals for all three parameters at a 95% confidence level (the default), we use:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"If we instead wish to find these intervals at a 99% confidence interval we use:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model, confidence_level=0.99)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Similarly, if we wish to find simultaneous 95% confidence intervals for the parameters we set the degrees of freedom to the number of model parameters (instead of 1).","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model, dof=model.core.num_pars) # model.core.num_pars=3","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To find asymptotic confidence intervals using the ellipse approximation, we change the specified profile type to EllipseApproxAnalytical or EllipseApprox. When parameter constraints are not in the way these will produce the same result for well-identified models:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model, profile_type=EllipseApproxAnalytical())\nunivariate_confidenceintervals!(model, profile_type=EllipseApprox())","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"If we want to visualise the univariate profiles across the range defined by each confidence interval then we need to evaluate points inside each interval. We can also evaluate some points to the left and right of each interval to observe the behaviour of the profile log-likelihood function outside of this range:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"get_points_in_intervals!(model, 20, additional_width=0.2)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"This can also be done within univariate_confidenceintervals! using the num_points_in_interval and additional_width keyword arguments.","category":"page"},{"location":"examples/logistic/#Initial-Guesses","page":"Logistic Model","title":"Initial Guesses","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"We can use existing confidence intervals to reduce the search bracket for other confidence intervals of interest. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"For example, we can use the confidence intervals found at a 99% confidence level with one degree of freedom to more quickly find the corresponding intervals at a 95% confidence level. We set existing_profiles=:overwrite so that we recalculate these profiles - otherwise they won't be calculated as they already exist!","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model, confidence_level=0.99)\nunivariate_confidenceintervals!(model, confidence_level=0.95, use_existing_profiles=true, \n    existing_profiles=:overwrite, num_points_in_interval=20, additional_width=0.2)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Similarly, we can use profiles of type EllipseApproxAnalytical to decrease the bracket. This is recommended for identifiable parameters.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"univariate_confidenceintervals!(model, profile_type=EllipseApproxAnalytical())\nunivariate_confidenceintervals!(model, use_ellipse_approx_analytical_start=true, \n    existing_profiles=:overwrite, num_points_in_interval=20, additional_width=0.2)","category":"page"},{"location":"examples/logistic/#Bivariate-Profiles","page":"Logistic Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To evaluate the bivariate boundaries for all three bivariate parameter combinations, here we use the IterativeBoundaryMethod, which uses a 20 point ellipse approximation of the boundary as a starting guess using RadialMLEMethod. The boundaries in this example are reasonably convex, which makes this starting guess appropriate. To speed up computation we provide stronger optimization settings.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 50, \n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 1.0, use_ellipse=true), \n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Similarly, if we wish to evaluate simultaneous 95% bivariate profiles we set the degrees of freedom parameter, dof, to the number of model parameters (instead of 2).","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\nbivariate_confidenceprofiles!(model, 50, \n    method=IterativeBoundaryMethod(20, 5, 5, 0.15, 1.0, use_ellipse=true), \n    dof=model.core.num_pars,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To evaluate the analytical ellipse boundaries using EllipseSampling we use:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"bivariate_confidenceprofiles!(model, 50, \n    profile_type=EllipseApproxAnalytical(), method=AnalyticalEllipseMethod(0.15, 1.0))","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To efficiently sample 100 points within the bivariate boundaries using a rejection sampling approach we use:","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"sample_bivariate_internal_points!(model, 100)","category":"page"},{"location":"examples/logistic/#Plots-of-Profiles","page":"Logistic Model","title":"Plots of Profiles","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To visualise plots of these profiles we load Plots alongside a plotting backend. Here we use GR.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"using Plots, Plots.PlotMeasures; gr()\nPlots.reset_defaults(); Plots.scalefontsizes(0.75)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Univariate and bivariate profiles can either be visualised individually or in comparison to profiles at the same confidence level and degrees of freedom. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Here we compare the univariate profiles formed at a 95% confidence level and 1 degree of freedom. The first two arguments scale the limits of the x and y axis, respectively, away from the found confidence interval at the specified threshold.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"plts = plot_univariate_profiles_comparison(model, 0.1, 0.1,\n    confidence_levels=[0.95], dofs=[1])\n\nplt = plot(plts..., layout=(1,3),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Similarly, here we compare the bivariate profiles formed at a 95% confidence level and 2 degrees of freedom.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"plts = plot_bivariate_profiles_comparison(model, 0.1, 0.1, confidence_levels=[0.95], dofs=[2])\n\nplt = plot(plts..., layout=(1,3),\n    legend=:outertop, title=\"\", dpi=150, size=(550,300), margin=1mm)\ndisplay(plt)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/#Predictions","page":"Logistic Model","title":"Predictions","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To make predictions for the model trajectory and the 1-delta population reference set we define the following functions, which then need to be added to our LikelihoodModel. The region variable in errorfunction should be set equal to 1-delta when generating predictions. These could also be added in initialise_LikelihoodModel.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"@everywhere function predictfunction(θ, data, t=data.t); solvedmodel(t, θ) end\n@everywhere function errorfunction(predictions, θ, region); normal_error_σ_known(predictions, θ, region, σ) end\n\nadd_prediction_function!(model, predictfunction)\nadd_error_function!(model, errorfunction)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To generate profile-wise predictions for each of the evaluated profiles we first define the desired time points for prediction and then evaluate the approximate model trajectory confidence sets and (1-delta 1-alpha) population reference tolerance sets. By default, the population reference tolerance set evaluates reference interval regions at the same level as the default confidence level (1-delta = 1-alpha = 095); however, this is not required.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"t_pred=0:5:1000\n\ngenerate_predictions_univariate!(model, t_pred)\ngenerate_predictions_bivariate!(model, t_pred)\ngenerate_predictions_dim_samples!(model, t_pred) # for the full likelihood sample","category":"page"},{"location":"examples/logistic/#Plotting-Predictions","page":"Logistic Model","title":"Plotting Predictions","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"We can plot the predictions of individual profiles or the union of all profiles at a given number of interest parameters, confidence level, degrees of freedom and reference interval region (if relevant). When plotting the union of these predictions we can compare it to the result of the full likelihood sample, which here used LatinHypercubeSamples, the default. Here we plot the results from simultaneous profiles at a 95% confidence level.","category":"page"},{"location":"examples/logistic/#Model-Trajectory","page":"Logistic Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"SCBs (approx) here refer to approximate simultaneous confidence bands for the true model trajectory.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"using Plots, Plots.PlotMeasures; gr()\nplt = plot_predictions_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), title=\"\") # univariate profiles\n\nplot!(plt, t_pred, solvedmodel(t_pred, θ_true), \n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash,\n    dpi=150, size=(450,300), rightmargin=3mm)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"plt = plot_predictions_union(model, t_pred, 2, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), title=\"\") # bivariate profiles\n\nplot!(plt, t_pred, solvedmodel(t_pred, θ_true), \n    label=\"True model trajectory\", lw=3, color=:turquoise4, linestyle=:dash,\n    dpi=150, size=(450,300), rightmargin=3mm)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/#1-\\delta-Population-Reference-Set","page":"Logistic Model","title":"1-delta Population Reference Set","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"SRTBs (approx) here refer to approximate simultaneous reference tolerance bands for the 1-delta population reference tolerance set.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"lq, uq = errorfunction(solvedmodel(t_pred, θ_true), θ_true, 0.95)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"using Plots; gr()\nplt = plot_realisations_union(model, t_pred, 1, dof=model.core.num_pars,\n    compare_to_full_sample_type=LatinHypercubeSamples(), title=\"\") # univariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3])","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"plt = plot_realisations_union(model, t_pred, 2, dof=model.core.num_pars, \n    compare_to_full_sample_type=LatinHypercubeSamples(), title=\"\") # bivariate profiles\n\nplot!(plt, t_pred, lq, fillrange=uq, fillalpha=0.3, linealpha=0,\n    label=\"95% population reference set\", color=palette(:Paired)[1])\nscatter!(plt, data.t, data.y_obs, label=\"Observations\", msw=0, ms=7, color=palette(:Paired)[3])","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"(Image: )","category":"page"},{"location":"examples/logistic/#Coverage-Testing","page":"Logistic Model","title":"Coverage Testing","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To conduct an investigation into the coverage properties of the profiles and profile-wise predictions sets we can perform a simulation study using the provided coverage functions.","category":"page"},{"location":"examples/logistic/#Data-Generation","page":"Logistic Model","title":"Data Generation","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"First we define functions and arguments which we use to simulate new training and testing data, and evaluate the true 1-delta population reference set, given the true parameter values. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"# DATA GENERATION FUNCTION AND ARGUMENTS\n@everywhere function data_generator(θ_true, generator_args::NamedTuple)\n    y_obs = generator_args.y_true .+ rand(generator_args.dist, length(generator_args.t))\n    if generator_args.is_test_set; return y_obs end\n\n    data = (y_obs=y_obs, generator_args...)\n    return data\nend\n\n@everywhere function reference_set_generator(θ_true, generator_args::NamedTuple, region::Float64)\n    lq, uq = errorfunction(generator_args.y_true, θ_true, region)\n    return (lq, uq)\nend\n\ntraining_gen_args = (y_true=y_true, t=t, dist=Normal(0, σ), is_test_set=false)\ntesting_gen_args = (y_true=solvedmodel(t_pred, θ_true), t=t_pred, dist=Normal(0, σ), is_test_set=true)","category":"page"},{"location":"examples/logistic/#Parameter-Coverage","page":"Logistic Model","title":"Parameter Coverage","text":"","category":"section"},{"location":"examples/logistic/#Parameter-Confidence-Intervals","page":"Logistic Model","title":"Parameter Confidence Intervals","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Here we check the coverage of the 95% confidence interval for each of the three parameters in this model across 1000 simulated iterations using check_univariate_parameter_coverage. For a sufficiently regular likelihood, we would expect coverage to be approximately 0.95; a (default 95%) confidence interval using HypothesisTests.jl is provided to quantify the uncertainty in the simulated estimate. This is a scenario where using a larger number of worker processes than the three we started in Initial Setup would be useful, as we can distribute each simulation iteration across these workers. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nuni_coverage_df = check_univariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, θ_true, collect(1:model.core.num_pars),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/#Bivariate-Profiles-2","page":"Logistic Model","title":"Bivariate Profiles","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Similarly, we can check the coverage of a 95% bivariate profile boundary for each combination of two parameters in this model across 1000 simulated iterations using check_bivariate_parameter_coverage. Here we evaluate the performance of a 50 point boundary generated using the IterativeBoundaryMethod. Again, we expect each bivariate profile boundary to have coverage of approximately 0.95. This means that under repeated sampling, the true value of the two interest parameters considered in a given bivariate profile is contained within the profiles boundary 95% of the time.  ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_coverage_df = check_bivariate_parameter_coverage(data_generator,\n    training_gen_args, model, 1000, 50, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method = IterativeBoundaryMethod(10, 5, 5, 0.15, 0.1, use_ellipse=true), \n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"We can also evaluate how well a given bivariate boundary is being represented for a given boundary method and number of boundary points. To do this we use check_bivariate_boundary_coverage which not only calculates each bivariate profiles boundary using method, but also uses a rejection sampling approach to find points inside each profiles boundary. Here we sample 4000 points using dimensional_likelihood_samples! inside the specified parameters bounds and retain those that are within each profiles boundary. In this case this corresponds to around 250-400 retained points. We then check the proportion of these sampled points that are inside our bivariate profile using a point in polygon algorithm. If our method is performing well, we would expect this proportion to be close to 1.0. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"Here we set coverage_estimate_quantile_level to 0.9; we are interested in the lower 0.05 quantile result of the 200 simulations as this indicates that the area coverage observed should be at least this value approximately 95% of the time. If the proportion is not close to 1.0, then we would expect the result from check_bivariate_parameter_coverage to be generally lower than the expected confidence level.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nbiv_boundary_coverage_df = check_bivariate_boundary_coverage(data_generator,\n    training_gen_args, model, 200, 50, 4000, θ_true,\n    collect(combinations(1:model.core.num_pars, 2)); \n    method=IterativeBoundaryMethod(10, 5, 5, 0.15, 0.1, use_ellipse=true), \n    coverage_estimate_quantile_level=0.9,\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/#Prediction-Coverage","page":"Logistic Model","title":"Prediction Coverage","text":"","category":"section"},{"location":"examples/logistic/#Model-Trajectory-2","page":"Logistic Model","title":"Model Trajectory","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To test the coverage of the true model trajectory we can use check_dimensional_prediction_coverage, check_univariate_prediction_coverage and check_bivariate_prediction_coverage. Again we use the default 95% confidence level here. Given a sufficient number of sampled points we expect the model trajectory coverage from the trajectory confidence set from propagating forward the full parameter vector 95% confidence set to have 95% simultaneous coverage. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"The profile-wise predictions are approximate trajectory confidence sets and are not expected to reach 95% simultaneous coverage. However, they will converge to approximately the correct coverage as higher numbers of interest parameters are considered. Additionally, the asymptotic threshold being used to define the extremities of the profiles is lower than the threshold for the full parameter confidence set; there is evidence to suggest this is also responsible for constraining their coverage performance on this model. We do generally recommend sampling some points within univariate confidence intervals for propagation forward into the prediction sets, here we use 20.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"opt_settings = create_OptimizationSettings(solve_kwargs=(maxtime=5, xtol_rel=1e-12))\n\nfull_trajectory_coverage_df = check_dimensional_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 30000, \n    θ_true, [collect(1:model.core.num_pars)])\n\nuni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 20, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    method=IterativeBoundaryMethod(10, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"We instead suggest using the profile path approach for these lower dimensional profiles, where the degrees of freedom, dof, used to calibrate the asymptotic threshold is equal to the number of model parameters (as opposed to the dimensionality of the profile). This produces simultaneous profiles; the extremities of these profiles now touch the extremities of the full parameter vector confidence set. This has been shown to be reasonable for identifiable models with low numbers of parameters (<10). Most significantly in this example, the coverage of the trajectory confidence set from the union of simultaneous bivariate profiles is approximately 0.95. ","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"uni_trajectory_coverage_df = check_univariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars,\n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_trajectory_coverage_df = check_bivariate_prediction_coverage(data_generator, \n    training_gen_args, t_pred, model, 1000, 20, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=IterativeBoundaryMethod(10, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"examples/logistic/#1-\\delta-Population-Reference-Set-and-Observations","page":"Logistic Model","title":"1-delta Population Reference Set and Observations","text":"","category":"section"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"To test the coverage of the 1-delta population reference set as well as observations we can use check_dimensional_prediction_realisations_coverage, check_univariate_prediction_realisations_coverage and check_bivariate_prediction_realisations_coverage. Here we will only look at the coverage for simultaneous profiles.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"danger: Using manual GC calls\nOn versions of Julia earlier than 1.10, we recommend setting the kwarg, manual_GC_calls, to true in each of the coverage functions. Otherwise the garbage collector may not successfully free memory every iteration leading to out of memory errors.","category":"page"},{"location":"examples/logistic/","page":"Logistic Model","title":"Logistic Model","text":"full_reference_coverage_df = check_dimensional_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, 30000, \n    θ_true, [collect(1:model.core.num_pars)])\n\nuni_reference_coverage_df = check_univariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, \n    θ_true, collect(1:model.core.num_pars), \n    dof=model.core.num_pars,\n    num_points_in_interval=20, \n    optimizationsettings=opt_settings)\n\nbiv_reference_coverage_df = check_bivariate_prediction_realisations_coverage(data_generator,\n    reference_set_generator, training_gen_args, testing_gen_args, t_pred, model, 1000, 20, θ_true, \n    collect(combinations(1:model.core.num_pars, 2)),\n    dof=model.core.num_pars,\n    method=IterativeBoundaryMethod(10, 5, 5, 0.15, 0.1, use_ellipse=true),\n    optimizationsettings=opt_settings)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = LikelihoodBasedProfileWiseAnalysis","category":"page"},{"location":"#LikelihoodBasedProfileWiseAnalysis","page":"Home","title":"LikelihoodBasedProfileWiseAnalysis","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for LikelihoodBasedProfileWiseAnalysis.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is an implementation and exploration of the likelihood-based Profile-Wise Analysis (PWA) workflow from Matthew Simpson and Oliver Maclaren [1]. It provides methods for:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Maximum Likelihood Estimation.\nCalculation of the observed Fisher information matrix (FIM) [2] and associated approximation of the log-likelihood function.\nParameter identifiability analysis.\nParameter confidence intervals.\nEvaluating univariate profiles.\nEvaluating the boundaries of bivariate profiles and sampling points within these boundaries.\nRejection sampling of full parameter vector confidence sets and profiles.\nSimultaneous prediction of model solutions/trajectories using approximate profile-wise confidence trajectory sets.\nSimultaneous prediction of population reference sets using approximate profile-wise reference tolerance sets.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, to assist with evaluating the frequentist coverage properties of intervals and sets within the PWA workflow on new models it provides methods for the coverage testing of:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Parameter confidence intervals.\nBivariate confidence profiles.\nProfile-wise confidence trajectory sets.\nProfile-wise reference tolerance sets.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To understand the background of the workflow and how it can be used see Background. For implementation examples see the examples section, such as on a Logistic Model. To better understand how to interact with the user interface and in particular the LikelihoodModel, which holds all the information on computed profiles and predictions, check out the user interface starting with Initialisation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The API is largely stable, but shouldn't be regarded as fixed until the first version is released; the package will be kept as v1.0.0-DEV until then. The main work remaining is in completing documentation. However, one of the bivariate boundary methods may be removed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A package developed to fulfil the requirements of a Masters of Engineering at The University of Auckland by Joel Trent between March 2023 and February 2024. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Supervised by Oliver Maclaren, Ruanui Nicholson and Matthew Simpson.","category":"page"},{"location":"#Getting-Started:-Installation","page":"Home","title":"Getting Started: Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install the package, use the following command inside the Julia REPL. In the future this will be able to be installed directly rather than via the url.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/JoelTrent/LikelihoodBasedProfileWiseAnalysis.jl\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"To load the package, use the command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LikelihoodBasedProfileWiseAnalysis","category":"page"},{"location":"#Alternatives-for-Likelihood-Based-Uncertainty-Quantification","page":"Home","title":"Alternatives for Likelihood-Based Uncertainty Quantification","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are solely interested in parameter identifiability analysis and computing parameter confidence intervals we recommend LikelihoodProfiler, which is generally more stable and faster than the implementation in this package on the models we've tested.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ProfileLikelihood is another excellent package which implements the PWA workflow from [1] - it has a different interface and its own set of heuristics for computing profiles. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"InformationGeometry can compute the exact confidence regions/boundaries of models using differential geometry. It would be an interesting approach which could potentially evaluate confidence set boundaries more efficiently than the heuristics implemented in this package. Resultantly, its use within the PWA workflow may be worth investigating.","category":"page"},{"location":"internal_library/coverage/#Coverage-Functions","page":"Coverage Functions","title":"Coverage Functions","text":"","category":"section"},{"location":"internal_library/coverage/","page":"Coverage Functions","title":"Coverage Functions","text":"Pages = [\"coverage.md\"]","category":"page"},{"location":"internal_library/coverage/#Predictions","page":"Coverage Functions","title":"Predictions","text":"","category":"section"},{"location":"internal_library/coverage/","page":"Coverage Functions","title":"Coverage Functions","text":"LikelihoodBasedProfileWiseAnalysis.union_of_prediction_extrema\nLikelihoodBasedProfileWiseAnalysis.union_of_prediction_realisations_extrema\nLikelihoodBasedProfileWiseAnalysis.evaluate_coverage\nLikelihoodBasedProfileWiseAnalysis.evaluate_coverage_realisations\nLikelihoodBasedProfileWiseAnalysis.evaluate_coverage_reference_sets\nLikelihoodBasedProfileWiseAnalysis.evaluate_conf_simultaneous_coverage","category":"page"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.union_of_prediction_extrema","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.union_of_prediction_extrema","text":"union_of_prediction_extrema(df::DataFrame, dict::Dict)\n\nCompute the extrema union of the extrema of the predictions of each profile in the rows of df, with extrema stored in dict.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.union_of_prediction_realisations_extrema","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.union_of_prediction_realisations_extrema","text":"union_of_prediction_realisations_extrema(df::DataFrame, dict::Dict)\n\nCompute the extrema union of the extrema of the prediction realisations of each profile in the rows of df, with extrema stored in dict.\n\n\n\n\n\n","category":"function"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.evaluate_coverage","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.evaluate_coverage","text":"evaluate_coverage(y_true::Array, \n    y_pred_extrema::Union{Array,Missing}, \n    multiple_outputs::Bool)\n\nsimultaneous coverage requires ytrue to be inside ypred for every row and column in y_true (every observed variable)\n\npointwise coverage is on a row and column basis; whether ytrue is inside ypred at each individual row and column \n\n\n\n\n\nevaluate_coverage(model::LikelihoodModel, \n    y_true::Array, \n    profile_kind::Symbol, \n    multiple_outputs::Bool, \n    len_θs::Int)\n\n\n\n\n\n","category":"function"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.evaluate_coverage_realisations","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.evaluate_coverage_realisations","text":"evaluate_coverage_realisations(model::LikelihoodModel, \n    testing_data::Array, \n    profile_kind::Symbol, \n    multiple_outputs::Bool, \n    len_θs::Int)\n\n\n\n\n\n","category":"function"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.evaluate_coverage_reference_sets","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.evaluate_coverage_reference_sets","text":"evaluate_coverage_reference_sets(y_true::Array, \n    y_pred_extrema::Union{Array,Missing}, \n    multiple_outputs::Bool)\n\nsimultaneous coverage requires ytrue to be inside ypred for every row and column in y_true (every observed variable)\n\npointwise coverage is on a row and column basis; whether ytrue is inside ypred at each individual row and column \n\n\n\n\n\nevaluate_coverage_reference_sets(model::LikelihoodModel, \n    reference_set_data::Tuple{Array,Array}, \n    profile_kind::Symbol, \n    multiple_outputs::Bool, \n    len_θs::Int, \n    not_missing_profiles::Bool)\n\n\n\n\n\n","category":"function"},{"location":"internal_library/coverage/#LikelihoodBasedProfileWiseAnalysis.evaluate_conf_simultaneous_coverage","page":"Coverage Functions","title":"LikelihoodBasedProfileWiseAnalysis.evaluate_conf_simultaneous_coverage","text":"evaluate_conf_simultaneous_coverage(pointwise::BitArray, \n    region::Float64)\n\nEvaluate whether at least region proportion of entries in pointwise are true. This is simultaneous coverage with a different goal; i.e. region proportion of points are contained simultaneously.\n\n\n\n\n\n","category":"function"}]
}
