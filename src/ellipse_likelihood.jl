"""
Analytic ellipse log-likelihood has no knowledge of lower and upper bounds on parameters. Hence profiles generated by optimising out the nuisance parameters for a given interest parameter may look different if the analytical profile enters space where a bound would be active.
Pushing forward from these confidence bounds may be infeasible - if analytical profile has entered a space where a parameter bound is active.
"""
function analytic_ellipse_loglike(θ::Vector, θIndexes::Vector{Int}, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}) where T<:Float64
    return -0.5 * (θ-mleTuple.θmle[θIndexes])' * inv(mleTuple.Γmle[θIndexes, θIndexes]) * (θ-mleTuple.θmle[θIndexes])
end

"""
    analytic_ellipse_loglike_1D_soln(θIndex::Int, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}, targetll::T)

See [Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood](https://doi.org/10.1093/bioinformatics/btp358), equation 7.

```math
L^* = -\\frac{1}{2}(θ_i - θ_i^*)^2 \\times Γ_{ii}(θ^*)^{-1}
```
```math
θ_i =  θ_i^* + \\sqrt{\\frac{-2 L^*}{Γ_{ii}(θ^*)^{-1}}} \\equiv θ_i^* + \\sqrt{-2 L^* \\times Γ_{ii}(θ^*)}
```

Note: ``C(θ^*) = 2 \\times H(θ^*)^{-1}`` and ``Γ(θ^*) = H(θ^*)^{-1}``, and ``L^* = -χ^2(α, df)``, so the equation is equivalent to equation 7 in the above reference.
"""
function analytic_ellipse_loglike_1D_soln(θIndex::Int, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}, targetll::T) where T<:Float64

    sqrt_inner = (-2 * targetll * mleTuple.Γmle[θIndex, θIndex])
    if sqrt_inner < 0.0; return nothing end

    sqrt_part = sqrt(sqrt_inner)
    return mleTuple.θmle[θIndex] - sqrt_part, mleTuple.θmle[θIndex] + sqrt_part
end

"""
    ellipse_loglike(θ::Vector, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
"""
function ellipse_loglike(θ::Vector, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
    return -0.5 * ((θ - mleTuple.θmle)' * mleTuple.Hmle * (θ - mleTuple.θmle))
end

"""
    ellipse_like(θ::Vector{T}, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
"""
function ellipse_like(θ::Vector{T}, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
    return exp(ellipse_loglike(θ, mleTuple))
end

"""
    test_hessian_identifiability(Hmle::Matrix{T}, num_pars::Int) where T<:Float64

Modified R code from Cole, (2020): https://doi.org/10.1201/9781315120003, (https://www.kent.ac.uk/smsas/personal/djc24/parameterredundancy.html) in book code, `bassicocc.R`.

The cutoff used to estimate whether a standardised eigenvalue is close enough to zero to indicate non-identifiability is 1e-12*number of parameters, which is a smaller value than used in  Viallefont et. al. (1998) (https://doi.org/10.1002/(SICI)1521-4036(199807)40:3<313::AID-BIMJ313>3.0.CO;2-2) and Cole (2020) (https://doi.org/10.1201/9781315120003) due to smaller error in the calculation of `Hmle` via automatic differentiation. This cutoff is meant as an indication of non-identifiability/singularity; the hessian may still be identifiable/non-singular even if a warning occurs.
"""
function test_hessian_identifiability(Hmle::Matrix{T}, num_pars::Int) where T<:Float64

    # Uses 1e-9 * num_pars Viallefont et. al. (1998) https://doi.org/10.1002/(SICI)1521-4036(199807)40:3<313::AID-BIMJ313>3.0.CO;2-2
    # Referenced in Cole (2020) https://doi.org/10.1201/9781315120003 
    # We use even smaller value due to smaller error in calculation of Hmle via automatic differentiation
    cutoff = 1e-12 * num_pars 

    # Cole (2020) https://doi.org/10.1201/9781315120003 
    epsilon= 0.01

    E = eigen(Hmle)
    standard_eigenvalues = abs.(E.values) ./ maximum(abs.(E.values))
    num_estimable_pars = 0

    small_eigvals = Int[]
    for i in 1:num_pars
        if standard_eigenvalues[i] >= cutoff
           num_estimable_pars += 1
        else
            push!(small_eigvals, i)
        end
    end
    identifiable_pars = Int[]
    if minimum(standard_eigenvalues) < cutoff
        for i in 1:num_pars 
            indent = 1
            for j in eachindex(small_eigvals) 
                if abs(E.vectors[i, small_eigvals[j]]) > epsilon
                    indent = 0 
                end
            end
            if indent == 1
                push!(identifiable_pars, i)
            end
        end
    end
    if minimum(standard_eigenvalues) < cutoff

        message = "the model is likely to be non-identifiable or parameter redundant as the Hessian of the log-likelihood function at the MLE point is close to singular. Using EllipseApproxAnalytical or EllipseApprox profile types may result in errors or poor results. Smallest standardised eigenvalue: "*string(minimum(standard_eigenvalues))
        
        if isempty(identifiable_pars)
            message = message*". None of the original parameters are estimable"
        else 
            message = message*". Number of estimable parameters: "*string(num_estimable_pars)
            message = message*". Estimable parameter indexes: "*string(identifiable_pars)
        end
        @warn message
    end
    return nothing
end

"""
    getMLE_hessian_and_covariance(f::Function, θmle::Vector{<:Float64})

Computes the negative hessian of function `f` at `θmle` using [ForwardDiff.jl](https://juliadiff.org/ForwardDiff.jl/stable/user/api/#ForwardDiff.hessian) and it's pseudoinverse, returning both matrices.

Hessian identifiability is tested using [`PlaceholderLikelihood.test_hessian_identifiability`](@ref).
"""
function getMLE_hessian_and_covariance(f::Function, θmle::Vector{<:Float64})

    Hmle = -ForwardDiff.hessian(f, θmle)
    test_hessian_identifiability(Hmle, length(θmle))

    # if inverse fails then may have locally non-identifiable parameter OR parameter is
    # a delta distribution given data.
    # improves precision of inverse when variables have significantly different magnitudes.
    # Γmle = convert.(Float64, inv(BigFloat.(Hmle, precision=64)))

    # Hmle is hermitian / a normal matrix, so the pseudoinverse acts as a traditional inverse of Hmle and will be the traditional inverse if it is invertible (https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)
    Γmle = pinv(Hmle)
    return Hmle, Γmle
end

"""
    getMLE_ellipse_approximation!(model::LikelihoodModel)

Creates the ellipse approximation of the model at the maximum likelihood estimate, modifying `model` in place, computing the negative hessian of the log-likelihood function and it's inverse using [`getMLE_hessian_and_covariance`](@ref). These matrices are stored as a [`EllipseMLEApprox`](@ref) struct within `model` at `model.ellipse_MLE_approx`.
"""
function getMLE_ellipse_approximation!(model::LikelihoodModel)

    function funmle(θ); return model.core.loglikefunction(θ, model.core.data) end

    Hmle, Γmle = getMLE_hessian_and_covariance(funmle, model.core.θmle)

    model.ellipse_MLE_approx = EllipseMLEApprox(Hmle, Γmle)

    return model.ellipse_MLE_approx.Hmle, model.ellipse_MLE_approx.Γmle
end

"""
    check_ellipse_approx_exists!(model::LikelihoodModel)

Checks if the ellipse approximation at the maximum likelihood estimate has been created and if not creates it using [`getMLE_ellipse_approximation!`](@ref), modifying `model` in place.
"""
function check_ellipse_approx_exists!(model::LikelihoodModel)
    if ismissing(model.ellipse_MLE_approx)
        getMLE_ellipse_approximation!(model)
    end
    return nothing
end
