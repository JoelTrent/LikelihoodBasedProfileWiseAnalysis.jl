"""
    analytic_ellipse_loglike(θ::Vector, θIndexes::Vector{Int}, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}) where T<:Float64

Computes the analytical value of the ellipse approximation of the profile log-likelihood function for the interest parameters in `θindexes`.

Will produce an equivalent result to optimising out nuisance parameters with [`LikelihoodBasedProfileWiseAnalysis.ellipse_loglike`](@ref) if there are no bounds on parameters (or not close to the values considered) and there are no issues with the invertibility of the hessian, ``\\mathcal{H}(\\hat{\\theta})``.

The analytic ellipse log-likelihood has no knowledge of lower and upper bounds on parameters. Hence profiles generated by optimising out the nuisance parameters for a given interest parameter may look different to those from the analytical profile if it enters space where a bound would be active. Pushing forward from analytical profiles may thus be infeasible if the profile has entered a space where a parameter bound should be active (e.g. a non-negativity bound).
"""
function analytic_ellipse_loglike(θ::Vector, θIndexes::Vector{Int}, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}) where T<:Float64
    return -0.5 * (θ-mleTuple.θmle[θIndexes])' * inv(mleTuple.Γmle[θIndexes, θIndexes]) * (θ-mleTuple.θmle[θIndexes])
end

"""
    analytic_ellipse_loglike_1D_soln(θIndex::Int, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}, targetll::T)

In order to find the asympotic confidence interval for an interest parameter at `θIndex`, also known as Wald intervals [pawitanall2001](@cite), we solve the following equation from Ref. [rauestructural2009; Eq. (7)](@cite).

```math
\\ell_c  = -\\frac{1}{2}(θ_i - \\hat{θ_i})^2 \\times Γ_{ii}(\\hat{θ})^{-1}
```
```math
θ_i =  \\hat{θ_i} + \\sqrt{\\frac{-2 \\ell_c}{Γ_{ii}(\\hat{θ})^{-1}}} \\equiv \\hat{θ}_i + \\sqrt{-2 \\ell_c \\times Γ_{ii}(\\hat{θ})}
```

Note: ``C(\\hat{θ}) = 2 \\times H(\\hat{θ})^{-1}`` and ``Γ(\\hat{θ}) = H(\\hat{θ})^{-1}``, and ``\\ell_c = -χ^2(α, \\texttt{dof})``, so the equation is equivalent to equation 7 in the above reference. ``χ^2(α, \\texttt{dof})`` is the ``\alpha`` quantile of the chi-squared distribution with `dof` degrees of freedom, as calculated in [`LikelihoodBasedProfileWiseAnalysis.get_target_loglikelihood`](@ref).
"""
function analytic_ellipse_loglike_1D_soln(θIndex::Int, mleTuple::@NamedTuple{θmle::Vector{T}, Γmle::Matrix{T}}, targetll::T) where T<:Float64

    sqrt_inner = (-2 * targetll * mleTuple.Γmle[θIndex, θIndex])
    if sqrt_inner < 0.0; return nothing end

    sqrt_part = sqrt(sqrt_inner)
    return mleTuple.θmle[θIndex] - sqrt_part, mleTuple.θmle[θIndex] + sqrt_part
end

"""
    ellipse_loglike(θ::Vector, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64

Returns the value of the ellipse approximation of the normalised log-likelihood function using a second-order Taylor expansion at the MLE, ``\\hat{θ}``.

Where: ``\\hat{\\ell}(θ) \\approx \\hat{\\ell}^\\mathcal{E} (\\theta) = -\\frac{1}{2} (\\theta-\\hat{\\theta})' \\mathcal{H}(\\hat{\\theta}) (\\theta-\\hat{\\theta}) [pawitanall2001](@cite).

``\\mathcal{H}`` is the hessian of the log-likelihood function at the MLE, as evaluated using [`LikelihoodBasedProfileWiseAnalysis.getMLE_hessian_and_covariance`](@ref).
"""
function ellipse_loglike(θ::Vector, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
    return -0.5 * ((θ - mleTuple.θmle)' * mleTuple.Hmle * (θ - mleTuple.θmle))
end

"""
    ellipse_like(θ::Vector{T}, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64

The approximate likelihood function (i.e. the natural base exponential of the log-likelihood function), [`LikelihoodBasedProfileWiseAnalysis.ellipse_loglike`](@ref).
"""
function ellipse_like(θ::Vector{T}, mleTuple::@NamedTuple{θmle::Vector{T}, Hmle::Matrix{T}}) where T<:Float64
    return exp(ellipse_loglike(θ, mleTuple))
end

"""
    test_hessian_identifiability(Hmle::Matrix{T}, num_pars::Int) where T<:Float64

Modified R code from [coleparameter2020](@cite) in ['Code for Book'](https://www.kent.ac.uk/smsas/personal/djc24/parameterredundancy.html), `bassicocc.R`.

The cutoff used to estimate whether a standardised eigenvalue is close enough to zero to indicate non-identifiability is 1e-12*number of parameters, which is a smaller value than used in [coleparameter2020](@cite) and [viallefontparameter1998](@cite) due to smaller error in the calculation of `Hmle` via automatic differentiation. This cutoff is meant as an indication of non-identifiability/singularity; the hessian may still be identifiable/non-singular even if a warning occurs.
"""
function test_hessian_identifiability(Hmle::Matrix{T}, num_pars::Int) where T<:Float64

    # Uses 1e-9 * num_pars Viallefont et. al. (1998) https://doi.org/10.1002/(SICI)1521-4036(199807)40:3<313::AID-BIMJ313>3.0.CO;2-2
    # Referenced in Cole (2020) https://doi.org/10.1201/9781315120003 
    # We use even smaller value due to smaller error in calculation of Hmle via automatic differentiation
    cutoff = 1e-12 * num_pars 

    # Cole (2020) https://doi.org/10.1201/9781315120003 
    epsilon= 0.01

    E = eigen(Hmle)
    standard_eigenvalues = abs.(E.values) ./ maximum(abs.(E.values))
    num_estimable_pars = 0

    small_eigvals = Int[]
    for i in 1:num_pars
        if standard_eigenvalues[i] >= cutoff
           num_estimable_pars += 1
        else
            push!(small_eigvals, i)
        end
    end
    identifiable_pars = Int[]
    if minimum(standard_eigenvalues) < cutoff
        for i in 1:num_pars 
            indent = 1
            for j in eachindex(small_eigvals) 
                if abs(E.vectors[i, small_eigvals[j]]) > epsilon
                    indent = 0 
                end
            end
            if indent == 1
                push!(identifiable_pars, i)
            end
        end
    end
    if minimum(standard_eigenvalues) < cutoff

        message = "the model is likely to be non-identifiable or parameter redundant as the Hessian of the log-likelihood function at the MLE point is close to singular. Using EllipseApproxAnalytical or EllipseApprox profile types may result in errors or poor results. Smallest standardised eigenvalue: "*string(minimum(standard_eigenvalues))
        
        if isempty(identifiable_pars)
            message = message*". None of the original parameters are estimable"
        else 
            message = message*". Number of estimable parameters: "*string(num_estimable_pars)
            message = message*". Estimable parameter indexes: "*string(identifiable_pars)
        end
        @warn message
    end
    return nothing
end

"""
    getMLE_hessian_and_covariance(f::Function, θmle::Vector{<:Float64})

Computes the negative hessian of function `f` at `θmle` using [ForwardDiff.jl](https://juliadiff.org/ForwardDiff.jl/stable/user/api/#ForwardDiff.hessian) and it's pseudoinverse, returning both matrices.

Hessian identifiability is tested using [`LikelihoodBasedProfileWiseAnalysis.test_hessian_identifiability`](@ref).
"""
function getMLE_hessian_and_covariance(f::Function, θmle::Vector{<:Float64})

    Hmle = -ForwardDiff.hessian(f, θmle)
    test_hessian_identifiability(Hmle, length(θmle))

    # if inverse fails then may have locally non-identifiable parameter OR parameter is
    # a delta distribution given data.
    # improves precision of inverse when variables have significantly different magnitudes.
    # Γmle = convert.(Float64, inv(BigFloat.(Hmle, precision=64)))

    # Hmle is hermitian / a normal matrix, so the pseudoinverse acts as a traditional inverse of Hmle and will be the traditional inverse if it is invertible (https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse)
    Γmle = pinv(Hmle)
    return Hmle, Γmle
end

"""
    getMLE_ellipse_approximation!(model::LikelihoodModel)

Creates the ellipse approximation of the log-likelihood function at the maximum likelihood estimate, modifying `model` in place, computing the negative hessian of the log-likelihood function and it's pseudoinverse using [`getMLE_hessian_and_covariance`](@ref). These matrices are stored as a [`EllipseMLEApprox`](@ref) struct within `model` at `model.ellipse_MLE_approx`.
"""
function getMLE_ellipse_approximation!(model::LikelihoodModel)

    function funmle(θ); return model.core.loglikefunction(θ, model.core.data) end

    Hmle, Γmle = getMLE_hessian_and_covariance(funmle, model.core.θmle)

    model.ellipse_MLE_approx = EllipseMLEApprox(Hmle, Γmle)

    return model.ellipse_MLE_approx.Hmle, model.ellipse_MLE_approx.Γmle
end

"""
    check_ellipse_approx_exists!(model::LikelihoodModel)

Checks if the ellipse approximation at the maximum likelihood estimate has been created and if not creates it using [`getMLE_ellipse_approximation!`](@ref), modifying `model` in place.
"""
function check_ellipse_approx_exists!(model::LikelihoodModel)
    if ismissing(model.ellipse_MLE_approx)
        getMLE_ellipse_approximation!(model)
    end
    return nothing
end
